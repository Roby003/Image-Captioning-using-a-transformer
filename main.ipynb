{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":214432,"sourceType":"datasetVersion","datasetId":92290},{"sourceId":1706129,"sourceType":"datasetVersion","datasetId":1011404}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data loading and feature extraction","metadata":{}},{"cell_type":"markdown","source":"#### Load Karpathy split and organize the COCO data according to it","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nimport os\n\n\nkarpathy_file = '/kaggle/input/karpathy-splits/dataset_coco.json'\n\nif not os.path.exists(karpathy_file):\n    raise FileNotFoundError(f\"Karpathy split not found at: {karpathy_file}\")\n\nwith open(karpathy_file, 'r') as f:\n    karpathy_data = json.load(f)\n\n\ndef organize_by_split(karpathy_data):\n    splits = {'train': [], 'val': [], 'test': []}\n    \n    for img_data in karpathy_data['images']:\n        split = img_data['split']\n        \n        # handle 'restval' - we add them to the training set \n        if split == 'restval':\n            split = 'train'\n        \n        if split in ['train', 'val', 'test']:\n          \n            image_info = {\n                'image_id': img_data['cocoid'],\n                'file_name': img_data['filename'],  \n                'captions': [sent['raw'] for sent in img_data['sentences']]\n            }\n            splits[split].append(image_info)\n    \n    return splits\n\nsplits_data = organize_by_split(karpathy_data)\n\n# convert to DataFrames\ntrain_df = pd.DataFrame(splits_data['train'])\nval_df = pd.DataFrame(splits_data['val'])\ntest_df = pd.DataFrame(splits_data['test'])\nprint(train_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the pretrained feature extractor models","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"device: {device}\")\n\n# VGG16 - fc7 features (4096-dim) - matching the paper\n\nvgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nvgg16.classifier = vgg16.classifier[:-1]  # remove last layer to get fc7\nvgg16 = vgg16.to(device)\nvgg16.eval()\n\n# ResNet101\n\nresnet101 = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\nresnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])  # remove FC layer\nresnet101 = resnet101.to(device)\nresnet101.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature extraction","metadata":{}},{"cell_type":"code","source":"# image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\n\ndef extract_features(image_path, model):\n    try:\n        img = Image.open(image_path).convert('RGB')\n        img = transform(img).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            features = model(img)\n            if len(features.shape) > 2:\n                features = features.squeeze()\n        \n        return features.cpu().numpy()\n    except Exception as e:\n        return None\n\n\ndef get_image_path(filename, base_paths):   \n    # determine which folder based on filename\n    if 'train2014' in filename:\n        folder = 'train2014'\n    elif 'val2014' in filename:\n        folder = 'val2014'\n    else:\n        return None\n    \n    img_path = f\"{base_paths[folder]}/{filename}\"\n    \n    if os.path.exists(img_path):\n        return img_path\n    else:\n        return None\n\n\n\ndef extract_and_save_split_features(df, split_name, base_paths, models_dict):\n   \n    features_by_model = {model_name: {} for model_name in models_dict.keys()}\n    missing_images = []\n    processed = 0\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{split_name}\"):\n        img_id = row['image_id']\n        img_filename = row['file_name']\n        \n        img_path = get_image_path(img_filename, base_paths)\n        \n        if img_path is None:\n            missing_images.append(img_filename)\n            continue\n        \n        # extract features with each model\n        for model_name, model in models_dict.items():\n            features = extract_features(img_path, model)\n            if features is not None:\n                features_by_model[model_name][img_id] = features\n        \n        processed += 1\n    \n\nBASE_PATHS = {\n    'train2014': '/kaggle/input/coco2014/train2014/train2014',\n    'val2014': '/kaggle/input/coco2014/val2014/val2014'\n}\n\n\nmodels_dict = {\n    'vgg16': vgg16,\n    'resnet101': resnet101,\n}\n\n#start feature extraction\ntrain_features = extract_and_save_split_features(train_df, 'train', BASE_PATHS, models_dict)\nval_features = extract_and_save_split_features(val_df, 'val', BASE_PATHS, models_dict)\ntest_features = extract_and_save_split_features(test_df, 'test', BASE_PATHS, models_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Save the caption metadata","metadata":{}},{"cell_type":"code","source":"train_df.to_pickle('train_captions.pkl')\nval_df.to_pickle('val_captions.pkl')\ntest_df.to_pickle('test_captions.pkl')\n\ntrain_df.to_csv('train_captions.csv', index=False)\nval_df.to_csv('val_captions.csv', index=False)\ntest_df.to_csv('test_captions.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vocabulary\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport re\nfrom collections import Counter\nfrom tqdm import tqdm\n\n\n# vocabulary size  ~9,221 words + special tokens ( matches the paper's approach)\n\nclass Vocabulary:\n    \n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.word_counts = Counter()\n        \n        # special tokens\n        self.PAD_TOKEN = '<PAD>'\n        self.START_TOKEN = '<START>'\n        self.END_TOKEN = '<END>'\n        self.UNK_TOKEN = '<UNK>'\n        \n        # initialize with special tokens\n        self.word2idx = {\n            self.PAD_TOKEN: 0,\n            self.START_TOKEN: 1,\n            self.END_TOKEN: 2,\n            self.UNK_TOKEN: 3\n        }\n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n        self.idx = 4  # next available index\n    \n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n    \n    def __len__(self):\n        return len(self.word2idx)\n    \n    def __call__(self, word):\n        return self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])\n\n\ndef tokenize_caption(caption):\n    \"\"\"\n    - Convert to lowercase\n    - Remove punctuation (except hyphens in words)\n    - Split into words\n\n    \"\"\"\n    caption = caption.lower()\n    \n    # keeps alphanumeric, apostrophes, and hyphens\n    caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n    \n    # split and remove extra whitespace\n    tokens = caption.split()\n    \n    # remove empty strings\n    tokens = [t for t in tokens if t]\n    \n    return tokens\n\n\ndef build_vocabulary(train_captions_df, vocab_size=9221, min_word_freq=5):\n    \n    vocab = Vocabulary()\n    \n    # count all words in training captions\n    all_tokens = []\n    \n    for idx, row in tqdm(train_captions_df.iterrows(), \n                         total=len(train_captions_df),\n                         desc=\"Processing\"):\n        captions = row['captions']\n\n        #process captions\n        for caption in captions:\n            tokens = tokenize_caption(caption)\n            all_tokens.extend(tokens)\n            vocab.word_counts.update(tokens)\n    \n\n    # filter by minimum frequency\n    filtered_words = {word: count for word, count in vocab.word_counts.items() \n                      if count >= min_word_freq}\n    \n\n    most_common = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)\n    \n    # vocab_size - 4 (to account for special tokens)\n    top_words = most_common[:vocab_size - 4]\n    \n    for word, count in tqdm(top_words, desc=\"Adding words\"):\n        vocab.add_word(word)\n    \n    return vocab\n\n\ndef save_vocabulary(vocab, filepath='vocabulary.pkl'):\n    with open(filepath, 'wb') as f:\n        pickle.dump(vocab, f)\n\n\ndef load_vocabulary(filepath='vocabulary.pkl'):\n    with open(filepath, 'rb') as f:\n        vocab = pickle.load(f)\n\n    return vocab\n\n\n\n\n# load training captions\ntrain_df = pd.read_pickle('train_captions.pkl')\n\nvocab = build_vocabulary(\n    train_df, \n    vocab_size=9221,  \n    min_word_freq=5  \n)\n\nsave_vocabulary(vocab, 'vocabulary.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T11:22:38.142858Z","iopub.execute_input":"2026-01-13T11:22:38.143246Z","iopub.status.idle":"2026-01-13T11:22:45.769482Z","shell.execute_reply.started":"2026-01-13T11:22:38.143198Z","shell.execute_reply":"2026-01-13T11:22:45.768857Z"}},"outputs":[{"name":"stderr","text":"Processing: 100%|██████████| 113287/113287 [00:07<00:00, 15719.59it/s]\nAdding words: 100%|██████████| 9217/9217 [00:00<00:00, 1943244.19it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Dataset and DataLoaders","metadata":{}},{"cell_type":"markdown","source":"Dataset Objects","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport random\nfrom typing import Tuple, List\n\n\nclass CaptionDataset(Dataset):\n    \"\"\"\n    dataset for image captioning that loads pre-extracted features and captions.\n    \n    Args:\n        captions_df: DataFrame with columns ['image_id', 'file_name', 'captions']\n        features_dict: Dictionary mapping image_id -> feature vector \n        vocabulary: Vocabulary object\n        max_length: Maximum caption length (default: 15, matching paper)\n        training: If True, randomly sample one caption per image per epoch\n    \"\"\"\n    \n    def __init__(self, \n                 captions_df: pd.DataFrame,\n                 features_dict: dict,\n                 vocabulary,\n                 max_length: int = 15,\n                 training: bool = True):\n        \n        self.captions_df = captions_df.reset_index(drop=True)\n        self.features_dict = features_dict\n        self.vocab = vocabulary\n        self.max_length = max_length\n        self.training = training\n        self.valid_indices = []\n        for idx, row in self.captions_df.iterrows():\n            if row['image_id'] in self.features_dict:\n                self.valid_indices.append(idx)\n\n        \n    \n    def __len__(self):\n        return len(self.valid_indices)\n    \n    def tokenize_caption(self, caption: str) -> List[str]:\n        \n        import re\n        caption = caption.lower()\n        caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n        tokens = caption.split()\n        tokens = [t for t in tokens if t]\n        return tokens\n    \n    def caption_to_sequence(self, caption: str) -> Tuple[torch.Tensor, int]:\n        \"\"\"\n        Convert caption to sequence of word indices with <START> and <END>. Pad to max length using <PAD>\n        \"\"\"\n        \n        words = self.tokenize_caption(caption)\n        \n        # leave room for START and END\n        if len(words) > self.max_length:\n            words = words[:self.max_length]\n        \n        # convert to indices and add START + END\n        tokens = [self.vocab.word2idx[self.vocab.START_TOKEN]]\n        tokens.extend([self.vocab(word) for word in words])\n        tokens.append(self.vocab.word2idx[self.vocab.END_TOKEN])\n        \n        actual_length = len(tokens)\n        \n        # pad to max_length + 2 (for START and END)\n        max_seq_len = self.max_length + 2\n        if len(tokens) < max_seq_len:\n            tokens.extend([self.vocab.word2idx[self.vocab.PAD_TOKEN]] * (max_seq_len - len(tokens)))\n        \n        return torch.tensor(tokens, dtype=torch.long), actual_length\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        returns:\n            image_features: Tensor of shape (feature_dim,) - e.g., (4096,) for VGG16\n            caption: Tensor of shape (max_length + 2,) - padded caption sequence\n            caption_length: int - actual caption length including START/END\n        \"\"\"\n        df_idx = self.valid_indices[idx]\n        row = self.captions_df.iloc[df_idx]\n        \n        # Get image features\n        image_id = row['image_id']\n        image_features = self.features_dict[image_id]\n        image_features = torch.from_numpy(image_features).float()\n        \n        # get caption\n        captions = row['captions']\n        if self.training:\n            caption = random.choice(captions)\n        else:\n            caption = captions[0]\n        \n        # Convert caption to sequence\n        caption_seq, caption_length = self.caption_to_sequence(caption)\n        \n        return image_features, caption_seq, caption_length\n\n\nclass CaptionDatasetAllCaptions(Dataset):\n    \"\"\"\n    dataset that returns all captions for each image.\n    \n    Args:\n        captions_df: DataFrame with columns ['image_id', 'file_name', 'captions']\n        features_dict: Dictionary mapping image_id -> feature vector\n        vocabulary: Vocabulary object\n        max_length: Maximum caption length\n    \"\"\"\n    \n    def __init__(self, \n                 captions_df: pd.DataFrame,\n                 features_dict: dict,\n                 vocabulary,\n                 max_length: int = 15):\n        \n        self.captions_df = captions_df.reset_index(drop=True)\n        self.features_dict = features_dict\n        self.vocab = vocabulary\n        self.max_length = max_length\n        \n        # create expanded dataset with one entry per caption\n        self.data = []\n        for idx, row in self.captions_df.iterrows():\n            if row['image_id'] in self.features_dict:\n                for caption in row['captions']:\n                    self.data.append({\n                        'image_id': row['image_id'],\n                        'caption': caption,\n                        'all_captions': row['captions']\n                    })\n        \n\n    \n    def tokenize_caption(self, caption: str) -> List[str]:\n        import re\n        caption = caption.lower()\n        caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n        tokens = caption.split()\n        tokens = [t for t in tokens if t]\n        return tokens\n    \n    def caption_to_sequence(self, caption: str) -> Tuple[torch.Tensor, int]:\n        words = self.tokenize_caption(caption)\n        if len(words) > self.max_length:\n            words = words[:self.max_length]\n        \n        tokens = [self.vocab.word2idx[self.vocab.START_TOKEN]]\n        tokens.extend([self.vocab(word) for word in words])\n        tokens.append(self.vocab.word2idx[self.vocab.END_TOKEN])\n        \n        actual_length = len(tokens)\n        max_seq_len = self.max_length + 2\n        \n        if len(tokens) < max_seq_len:\n            tokens.extend([self.vocab.word2idx[self.vocab.PAD_TOKEN]] * (max_seq_len - len(tokens)))\n        \n        return torch.tensor(tokens, dtype=torch.long), actual_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        image_features = self.features_dict[item['image_id']]\n        image_features = torch.from_numpy(image_features).float()\n        \n        # convert caption to sequence\n        caption_seq, caption_length = self.caption_to_sequence(item['caption'])\n        \n        return image_features, caption_seq, caption_length, item['all_captions']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T11:22:45.770765Z","iopub.execute_input":"2026-01-13T11:22:45.770972Z","iopub.status.idle":"2026-01-13T11:22:47.391854Z","shell.execute_reply.started":"2026-01-13T11:22:45.770952Z","shell.execute_reply":"2026-01-13T11:22:47.391012Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Helper functions and main dataloader entrypoint","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Custom collate function for batching.\n    \n    Args:\n        batch: List of (image_features, caption, length) tuples\n    \n    Returns:\n        images: Tensor of shape (batch_size, feature_dim)\n        captions: Tensor of shape (batch_size, max_length + 2)\n        lengths: Tensor of shape (batch_size,)\n    \"\"\"\n    # Separate the components\n    images, captions, lengths = zip(*batch)\n    \n    # Stack into tensors\n    images = torch.stack(images, dim=0)\n    captions = torch.stack(captions, dim=0)\n    lengths = torch.tensor(lengths, dtype=torch.long)\n    \n    return images, captions, lengths\n\n\ndef collate_fn_eval(batch):\n    \"\"\"\n    Custom collate function for evaluation (includes all reference captions).\n    \n    Returns:\n        images: Tensor of shape (batch_size, feature_dim)\n        captions: Tensor of shape (batch_size, max_length + 2)\n        lengths: Tensor of shape (batch_size,)\n        all_captions: List of lists of reference captions\n    \"\"\"\n    images, captions, lengths, all_captions = zip(*batch)\n    \n    images = torch.stack(images, dim=0)\n    captions = torch.stack(captions, dim=0)\n    lengths = torch.tensor(lengths, dtype=torch.long)\n    \n    return images, captions, lengths, list(all_captions)\n\n\ndef create_dataloaders(\n    train_df: pd.DataFrame,\n    val_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_features: dict,\n    val_features: dict,\n    test_features: dict,\n    vocabulary,\n    batch_size: int = 64,\n    max_length: int = 15,\n    num_workers: int = 4,\n    shuffle_train: bool = True\n):\n    \"\"\"\n    Create train, validation, and test dataloaders.\n    \n    Args:\n        train_df, val_df, test_df: DataFrames with captions\n        train_features, val_features, test_features: Feature dictionaries\n        vocabulary: Vocabulary object\n        batch_size: Batch size for training\n        max_length: Maximum caption length\n        num_workers: Number of worker processes for data loading\n        shuffle_train: Whether to shuffle training data\n    \n    Returns:\n        train_loader, val_loader, test_loader\n    \"\"\"\n  \n    # create datasets\n    train_dataset = CaptionDataset(\n        train_df, \n        train_features, \n        vocabulary, \n        max_length=max_length,\n        training=True\n    )\n    \n    val_dataset = CaptionDataset(\n        val_df, \n        val_features, \n        vocabulary, \n        max_length=max_length,\n        training=False\n    )\n    \n    test_dataset = CaptionDataset(\n        test_df, \n        test_features, \n        vocabulary, \n        max_length=max_length,\n        training=False\n    )\n    \n    # create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle= shuffle_train,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    print(f\"\\nDataLoader Configuration:\")\n    print(f\"  Batch size: {batch_size}\")\n    print(f\"  Num workers: {num_workers}\")\n    print(f\"  Max caption length: {max_length}\")\n    \n    print(f\"\\nDataLoader Sizes:\")\n    print(f\"  Train batches: {len(train_loader)}\")\n    print(f\"  Val batches: {len(val_loader)}\")\n    print(f\"  Test batches: {len(test_loader)}\")\n    \n    return train_loader, val_loader, test_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T11:22:47.392800Z","iopub.execute_input":"2026-01-13T11:22:47.393202Z","iopub.status.idle":"2026-01-13T11:22:47.402551Z","shell.execute_reply.started":"2026-01-13T11:22:47.393177Z","shell.execute_reply":"2026-01-13T11:22:47.402018Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Decoder-Transformer Implementation","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\n\nclass ImageCaptioningTransformer(nn.Module):\n \n    def __init__(self, vocab_size, embed_dim=512, num_heads=8, \n                 num_layers=4, image_feat_dim=4096, max_len=17,dropout=0.1):\n        super().__init__()\n        \n        self.embed_dim = embed_dim\n        self.max_len = max_len\n        self.vocab_size = vocab_size\n        \n        self.image_embed = nn.Sequential(\n            nn.Linear(image_feat_dim, embed_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        self.word_embed = nn.Embedding(vocab_size, embed_dim)\n        self.positional_embed = nn.Embedding(max_len+1, embed_dim) # +1 for the image\n\n        self.embed_dropout = nn.Dropout(dropout)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=2048,\n            dropout=dropout,\n            activation='relu',\n            batch_first=True, \n            norm_first=False  \n        )\n\n        self.transformer = nn.TransformerEncoder(\n            encoder_layer=encoder_layer,\n            num_layers = num_layers\n        )\n        \n        self.output_project= nn.Linear(embed_dim, vocab_size)\n        \n\n    def generate_causal_mask(self, seq_len, device):\n        \"\"\"\n        Generate causal mask: upper triangular matrix of -inf.\n        \n        \"\"\"\n        mask = torch.triu(\n            torch.ones(seq_len, seq_len, device=device) * float('-inf'),\n            diagonal=1\n        )\n        return mask\n    \n    def generate_padding_mask(self,captions,pad_idx=0):\n        return (captions == pad_idx)\n        \n    def forward(self,images,captions):\n        \"\"\"\n        forward pass\n        \"\"\"\n        if images.dim()==3:\n            images = images.squeeze(1)\n        device = captions.device\n        batch_size, seq_len = captions.shape\n\n        img_embed = self.image_embed(images).unsqueeze(1) # -> (batch, 1,512)\n\n        caption_embed = self.word_embed(captions)\n\n        sequence = torch.cat([img_embed,caption_embed], dim = 1)\n\n        positional_encoding = torch.arange(seq_len + 1,device=device).unsqueeze(0)\n        positional_encoding= self.positional_embed(positional_encoding)\n\n        sequence = sequence + positional_encoding\n        sequence = self.embed_dropout(sequence)\n        \n        causal_mask = self.generate_causal_mask(seq_len +1, device)\n        padding_mask = self.generate_padding_mask(captions)\n\n        img_padding_mask = torch.zeros(batch_size,1, dtype=bool, device=device)\n\n        padding_mask = torch.cat([img_padding_mask, padding_mask], dim = 1)\n\n        output = self.transformer(\n            sequence,\n            mask=causal_mask,          \n            src_key_padding_mask=padding_mask\n        )\n\n        logits = self.output_project(output)\n        \n        return logits\n\n\n    def generate(self,images, start_token_idx=1, end_token_idx =2):\n        \"\"\"\n        Generate captions autoregressively\n        \n        Start with [START]\n        Loop:\n          1. Get predictions for current sequence\n          2. Take last prediction\n          3. Sample/argmax next token\n          4. Append to sequence\n          5. Stop if END token or max_len reached\n        \"\"\"\n        self.eval()\n        batch_size = images.shape[0]\n        device = images.device\n        generated = torch.full((batch_size,1), start_token_idx,dtype= torch.long, device=device)\n\n        # track what sequences have finished\n        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n        with torch.no_grad():\n            for _ in range(self.max_len):\n                logits = self.forward(images, generated)\n\n                next_logits = logits[:,-1,:]\n                next_token = next_logits.argmax(dim=1,keepdim = True)\n\n                generated = torch.cat([generated, next_token], dim = 1)\n\n                finished = finished | (next_token.squeeze(-1) == end_token_idx)\n\n                # stop if all sequences have finished\n                if finished.all():\n                    break\n\n        return generated\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T11:27:20.571375Z","iopub.execute_input":"2026-01-13T11:27:20.572095Z","iopub.status.idle":"2026-01-13T11:27:20.585696Z","shell.execute_reply.started":"2026-01-13T11:27:20.572058Z","shell.execute_reply":"2026-01-13T11:27:20.585058Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Loss function and padding mask","metadata":{}},{"cell_type":"code","source":"class MaskedCrossEntropyLoss(nn.Module):\n    def __init__(self,pad_idx=0):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.criterion = nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, logits, targets):\n        \"\"\"\n        logits (batch,seq_len,vocab_size)\n        targets (batch, seq_len, 1)\n        \"\"\"\n        batch_size,seq_len, vocab_size = logits.shape\n        device = logits.device\n        \n        logits_flat = logits.reshape(-1,vocab_size) # ( batch * seq_len, vocab_size)\n        targets_flat = targets.reshape(-1)  # (batch * seq_len)\n\n        \n        loss = self.criterion(logits_flat, targets_flat)\n\n        mask = (targets_flat != self.pad_idx).float()\n\n        loss_masked = loss * mask\n\n        return torch.sum(loss_masked) / torch.sum(mask)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T11:27:20.587138Z","iopub.execute_input":"2026-01-13T11:27:20.587422Z","iopub.status.idle":"2026-01-13T11:27:20.606548Z","shell.execute_reply.started":"2026-01-13T11:27:20.587393Z","shell.execute_reply":"2026-01-13T11:27:20.606035Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Train step and validation","metadata":{}},{"cell_type":"code","source":"\ndef train_step(model, images, captions, lengths, criterion, optimizer, device):\n    \"\"\"\n    One training step with teacher forcing\n    \n    Remember:\n      - Input to model: captions\n      - Model output: logits at each position\n      - Predictions: logits[:, :-1, :] (drop last)\n      - Targets: captions (original)\n    \"\"\"\n\n    model.train()\n    # Move to gpu\n    images = images.to(device)\n    captions = captions.to(device)\n    lengths = lengths.to(device)\n    logits = model(images,captions) # we pass ground truth for \"teacher forcing\"\n    \n    logits = logits[:,:-1,:]\n\n    loss = criterion(logits, captions)\n\n    \n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    \n    # Gradient clipping (prevent exploding gradients)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    \n    optimizer.step()\n    \n    return loss.item()\n\n\ndef validate(model, val_loader, criterion, device):\n    \"\"\"\n    Validation loop.\n    \n    Args:\n        model: ImageCaptioningTransformer model\n        val_loader: validation DataLoader\n        criterion: MaskedCrossEntropyLoss\n        device: device to run on\n    \n    Returns:\n        avg_loss: average validation loss\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for images, captions, lengths in val_loader:\n            images = images.to(device)\n            captions = captions.to(device)\n            lengths = lengths.to(device)\n            \n            # forward pass\n            logits = model(images, captions)\n            \n            # compute loss \n            predictions = logits[:, :-1, :]\n            targets = captions\n            \n            loss = criterion(predictions, targets)\n            \n            total_loss += loss.item()\n            num_batches += 1\n    \n    return total_loss / num_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T11:32:31.572020Z","iopub.execute_input":"2026-01-13T11:32:31.572726Z","iopub.status.idle":"2026-01-13T11:32:31.580390Z","shell.execute_reply.started":"2026-01-13T11:32:31.572690Z","shell.execute_reply":"2026-01-13T11:32:31.579654Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, criterion, train_loader,optimizer,config,epoch):\n    model.train()\n\n    total_loss = 0\n    num_batches =0\n\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.num_epochs}')\n    for batch_idx, (images,captions,lengths) in enumerate(pbar):\n       \n        loss = train_step(model, images, captions, lengths, \n                         criterion, optimizer, config.device)\n        \n        total_loss+=loss\n        num_batches+=1\n\n        if (batch_idx + 1) % config.print_every == 0:\n            print(f'\\n[Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}] '\n                  f'Loss: {loss:.4f}, Avg Loss: {total_loss/num_batches:.4f}')\n    return total_loss/ num_batches\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T11:32:31.581779Z","iopub.execute_input":"2026-01-13T11:32:31.582001Z","iopub.status.idle":"2026-01-13T11:32:31.600509Z","shell.execute_reply.started":"2026-01-13T11:32:31.581981Z","shell.execute_reply":"2026-01-13T11:32:31.599940Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class Config:\n    \"\"\"\n    training configuration\n    \"\"\"\n   # model\n    vocab_size = 9221\n    embed_dim = 512\n    num_heads = 8\n    num_layers = 4\n    image_feat_dim = 4096\n    max_len = 17\n    dropout = 0.1\n    \n    # training\n    batch_size = 128\n    num_epochs = 30\n    learning_rate = 5e-5\n    weight_decay = 1e-4\n    base_dir =\"/kaggle/working\"\n    save_dir=\"checkpoints\"\n    save_every= 10 \n    print_every=100\n    \n    \n    # device\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n\n\nimport time\nimport os\ndef train(config):\n    \"\"\"\n    -. we need to call the train_step function for n epochs for all batches\n    - \n    \n    \"\"\"\n    train_df = pd.read_pickle('train_captions.pkl')\n    val_df = pd.read_pickle('val_captions.pkl')\n\n    train_features = np.load('coco_train_vgg16_features.npy', allow_pickle=True).item()\n    val_features = np.load('coco_val_vgg16_features.npy',allow_pickle = True).item()\n    \n    vocab = build_vocabulary(train_df,config.vocab_size)\n    \n    train_loader, val_loader, _ = create_dataloaders(# test_features and test_df are not used during training, so i used val_df as a dummy\n        train_df,val_df,val_df,train_features,val_features,val_features,vocab,config.batch_size\n    )\n\n    model = ImageCaptioningTransformer(config.vocab_size, \n                                       config.embed_dim,\n                                       config.num_heads,\n                                       config.num_layers, \n                                       config.image_feat_dim,\n                                       config.max_len,\n                                       config.dropout\n                                      ).to(config.device)\n    \n\n    optimizer = torch.optim.RMSprop(model.parameters(), \n                                    lr=config.learning_rate)\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=config.num_epochs\n    )\n\n    criterion = MaskedCrossEntropyLoss()\n    best_val_loss = float('inf')\n    train_losses = []\n    val_losses = []\n\n    os.makedirs(os.path.join(config.base_dir, config.save_dir),exist_ok=True)\n    \n    for epoch in range(config.num_epochs):\n        epoch_start= time.time()\n        \n        train_loss = train_epoch(model, criterion, train_loader,optimizer,config,epoch)\n        train_losses.append(train_loss)\n\n        val_loss = validate(model,val_loader,criterion,config.device)\n        val_losses.append(val_loss)\n        \n        scheduler.step()\n\n        epoch_time = time.time() - epoch_start\n\n        if (epoch +1) % config.save_every ==0:\n            checkpoint_path = os.path.join(\n                config.base_dir,\n                config.save_dir,\n                f'checkpoint_epoch_{epoch+1}.pt'\n            )\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'config': config\n            }, checkpoint_path)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            checkpoint_path = os.path.join(\n                config.base_dir,\n                config.save_dir,\n                f'best_model.pt'\n            )\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'config': config\n            }, checkpoint_path)\n        \n    history = {\n        'train_losses': train_losses,\n        'val_losses': val_losses\n    }\n    \n    history_path = os.path.join(config.base_dir,config.save_dir, 'training_history.pkl')\n    with open(history_path, 'wb') as f:\n        pickle.dump(history, f)\n        \n    print(\"Training history saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T11:32:31.601307Z","iopub.execute_input":"2026-01-13T11:32:31.601581Z","iopub.status.idle":"2026-01-13T11:32:31.623615Z","shell.execute_reply.started":"2026-01-13T11:32:31.601550Z","shell.execute_reply":"2026-01-13T11:32:31.623067Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"config=Config()\ntrain(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T11:32:31.624557Z","iopub.execute_input":"2026-01-13T11:32:31.625029Z","iopub.status.idle":"2026-01-13T12:01:23.695346Z","shell.execute_reply.started":"2026-01-13T11:32:31.624995Z","shell.execute_reply":"2026-01-13T12:01:23.694564Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"\n\n\nProcessing:   0%|          | 0/113287 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   1%|          | 1366/113287 [00:00<00:08, 13653.27it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   3%|▎         | 2964/113287 [00:00<00:07, 15017.79it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   4%|▍         | 4539/113287 [00:00<00:07, 15351.84it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   5%|▌         | 6108/113287 [00:00<00:06, 15482.08it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   7%|▋         | 7657/113287 [00:00<00:06, 15130.79it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   8%|▊         | 9175/113287 [00:00<00:06, 15144.87it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   9%|▉         | 10691/113287 [00:00<00:06, 14968.97it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  11%|█         | 12195/113287 [00:00<00:06, 14989.35it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  12%|█▏        | 13695/113287 [00:00<00:06, 14912.93it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  13%|█▎        | 15187/113287 [00:01<00:06, 14914.11it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  15%|█▍        | 16751/113287 [00:01<00:06, 15134.13it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  16%|█▌        | 18265/113287 [00:01<00:06, 15039.38it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  17%|█▋        | 19770/113287 [00:01<00:06, 14952.63it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  19%|█▉        | 21319/113287 [00:01<00:06, 15111.80it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  20%|██        | 22831/113287 [00:01<00:06, 14728.90it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  21%|██▏       | 24307/113287 [00:01<00:06, 14559.70it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  23%|██▎       | 25824/113287 [00:01<00:05, 14736.92it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  24%|██▍       | 27300/113287 [00:01<00:05, 14522.80it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  25%|██▌       | 28812/113287 [00:01<00:05, 14696.44it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  27%|██▋       | 30342/113287 [00:02<00:05, 14872.28it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  28%|██▊       | 31831/113287 [00:02<00:05, 14480.71it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  29%|██▉       | 33340/113287 [00:02<00:05, 14658.49it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  31%|███       | 34809/113287 [00:02<00:05, 14315.19it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  32%|███▏      | 36265/113287 [00:02<00:05, 14385.37it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  33%|███▎      | 37748/113287 [00:02<00:05, 14513.77it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  35%|███▍      | 39202/113287 [00:02<00:05, 14173.84it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  36%|███▌      | 40701/113287 [00:02<00:05, 14410.26it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  37%|███▋      | 42219/113287 [00:02<00:04, 14635.95it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  39%|███▊      | 43685/113287 [00:02<00:04, 14195.53it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  40%|███▉      | 45212/113287 [00:03<00:04, 14506.36it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  41%|████▏     | 46744/113287 [00:03<00:04, 14744.73it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  43%|████▎     | 48277/113287 [00:03<00:04, 14917.15it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  44%|████▍     | 49802/113287 [00:03<00:04, 15015.09it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  45%|████▌     | 51306/113287 [00:03<00:04, 14985.40it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  47%|████▋     | 52806/113287 [00:03<00:04, 14917.82it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  48%|████▊     | 54327/113287 [00:03<00:03, 15003.91it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  49%|████▉     | 55883/113287 [00:03<00:03, 15167.14it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  51%|█████     | 57426/113287 [00:03<00:03, 15243.20it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  52%|█████▏    | 58970/113287 [00:03<00:03, 15300.94it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  53%|█████▎    | 60501/113287 [00:04<00:03, 15283.02it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  55%|█████▍    | 62030/113287 [00:04<00:03, 14297.43it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  56%|█████▌    | 63494/113287 [00:04<00:03, 14391.82it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  57%|█████▋    | 64943/113287 [00:04<00:03, 12983.34it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  59%|█████▊    | 66436/113287 [00:04<00:03, 13511.43it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  60%|█████▉    | 67969/113287 [00:04<00:03, 14018.54it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  61%|██████▏   | 69500/113287 [00:04<00:03, 14385.97it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  63%|██████▎   | 71032/113287 [00:04<00:02, 14654.23it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  64%|██████▍   | 72580/113287 [00:04<00:02, 14894.42it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  65%|██████▌   | 74109/113287 [00:05<00:02, 15008.66it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  67%|██████▋   | 75639/113287 [00:05<00:02, 15093.33it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  68%|██████▊   | 77162/113287 [00:05<00:02, 15132.65it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  69%|██████▉   | 78704/113287 [00:05<00:02, 15217.76it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  71%|███████   | 80229/113287 [00:05<00:02, 15217.91it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  72%|███████▏  | 81753/113287 [00:05<00:02, 15109.40it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  74%|███████▎  | 83266/113287 [00:05<00:01, 15074.25it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  75%|███████▍  | 84791/113287 [00:05<00:01, 15123.94it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  76%|███████▌  | 86309/113287 [00:05<00:01, 15137.85it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  78%|███████▊  | 87841/113287 [00:05<00:01, 15191.20it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  79%|███████▉  | 89366/113287 [00:06<00:01, 15208.13it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  80%|████████  | 90888/113287 [00:06<00:01, 15182.41it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  82%|████████▏ | 92407/113287 [00:06<00:01, 15178.41it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  83%|████████▎ | 93925/113287 [00:06<00:01, 15173.72it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  84%|████████▍ | 95446/113287 [00:06<00:01, 15183.31it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  86%|████████▌ | 96965/113287 [00:06<00:01, 15089.88it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  87%|████████▋ | 98482/113287 [00:06<00:00, 15112.11it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  88%|████████▊ | 100003/113287 [00:06<00:00, 15140.76it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  90%|████████▉ | 101518/113287 [00:06<00:00, 15120.17it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  91%|█████████ | 103041/113287 [00:06<00:00, 15150.81it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  92%|█████████▏| 104557/113287 [00:07<00:00, 15080.90it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  94%|█████████▎| 106066/113287 [00:07<00:00, 15078.86it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  95%|█████████▍| 107574/113287 [00:07<00:00, 15001.38it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  96%|█████████▋| 109075/113287 [00:07<00:00, 14961.47it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  98%|█████████▊| 110572/113287 [00:07<00:00, 14927.53it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing: 100%|██████████| 113287/113287 [00:07<00:00, 14840.08it/s]\u001b[A\u001b[A\u001b[A\n\n\n\nAdding words: 100%|██████████| 9217/9217 [00:00<00:00, 1648848.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nDataLoader Configuration:\n  Batch size: 64\n  Num workers: 4\n  Max caption length: 15\n\nDataLoader Sizes:\n  Train batches: 1771\n  Val batches: 79\n  Test batches: 79\n","output_type":"stream"},{"name":"stderr","text":"\n\n\nEpoch 1/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 1, Batch 50/1771] Loss: 4.7236, Avg Loss: 5.3725\n\n[Epoch 1, Batch 100/1771] Loss: 4.2177, Avg Loss: 4.9437\n\n[Epoch 1, Batch 150/1771] Loss: 4.1572, Avg Loss: 4.7206\n\n[Epoch 1, Batch 200/1771] Loss: 4.0510, Avg Loss: 4.5713\n\n[Epoch 1, Batch 250/1771] Loss: 3.9284, Avg Loss: 4.4548\n\n[Epoch 1, Batch 300/1771] Loss: 3.7850, Avg Loss: 4.3632\n\n[Epoch 1, Batch 350/1771] Loss: 3.8697, Avg Loss: 4.2832\n\n[Epoch 1, Batch 400/1771] Loss: 3.8575, Avg Loss: 4.2137\n\n[Epoch 1, Batch 450/1771] Loss: 3.7071, Avg Loss: 4.1557\n\n[Epoch 1, Batch 500/1771] Loss: 3.6258, Avg Loss: 4.1021\n\n[Epoch 1, Batch 550/1771] Loss: 3.6565, Avg Loss: 4.0521\n\n[Epoch 1, Batch 600/1771] Loss: 3.3626, Avg Loss: 4.0062\n\n[Epoch 1, Batch 650/1771] Loss: 3.3727, Avg Loss: 3.9656\n\n[Epoch 1, Batch 700/1771] Loss: 3.3961, Avg Loss: 3.9268\n\n[Epoch 1, Batch 750/1771] Loss: 3.1984, Avg Loss: 3.8918\n\n[Epoch 1, Batch 800/1771] Loss: 3.2946, Avg Loss: 3.8601\n\n[Epoch 1, Batch 850/1771] Loss: 3.5771, Avg Loss: 3.8293\n\n[Epoch 1, Batch 900/1771] Loss: 3.3872, Avg Loss: 3.8001\n\n[Epoch 1, Batch 950/1771] Loss: 3.3307, Avg Loss: 3.7714\n\n[Epoch 1, Batch 1000/1771] Loss: 3.2541, Avg Loss: 3.7457\n\n[Epoch 1, Batch 1050/1771] Loss: 3.4257, Avg Loss: 3.7222\n\n[Epoch 1, Batch 1100/1771] Loss: 3.2466, Avg Loss: 3.6993\n\n[Epoch 1, Batch 1150/1771] Loss: 3.2860, Avg Loss: 3.6773\n\n[Epoch 1, Batch 1200/1771] Loss: 3.1596, Avg Loss: 3.6567\n\n[Epoch 1, Batch 1250/1771] Loss: 2.8295, Avg Loss: 3.6370\n\n[Epoch 1, Batch 1300/1771] Loss: 3.3430, Avg Loss: 3.6183\n\n[Epoch 1, Batch 1350/1771] Loss: 3.1165, Avg Loss: 3.6011\n\n[Epoch 1, Batch 1400/1771] Loss: 3.1077, Avg Loss: 3.5840\n\n[Epoch 1, Batch 1450/1771] Loss: 2.9547, Avg Loss: 3.5670\n\n[Epoch 1, Batch 1500/1771] Loss: 2.9387, Avg Loss: 3.5513\n\n[Epoch 1, Batch 1550/1771] Loss: 3.1565, Avg Loss: 3.5357\n\n[Epoch 1, Batch 1600/1771] Loss: 3.1757, Avg Loss: 3.5207\n\n[Epoch 1, Batch 1650/1771] Loss: 2.9510, Avg Loss: 3.5070\n\n[Epoch 1, Batch 1700/1771] Loss: 2.8158, Avg Loss: 3.4931\n\n[Epoch 1, Batch 1750/1771] Loss: 2.9234, Avg Loss: 3.4802\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 2/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 2, Batch 50/1771] Loss: 2.9012, Avg Loss: 2.9990\n\n[Epoch 2, Batch 100/1771] Loss: 2.8934, Avg Loss: 2.9876\n\n[Epoch 2, Batch 150/1771] Loss: 3.0050, Avg Loss: 2.9824\n\n[Epoch 2, Batch 200/1771] Loss: 3.1482, Avg Loss: 2.9795\n\n[Epoch 2, Batch 250/1771] Loss: 3.0134, Avg Loss: 2.9813\n\n[Epoch 2, Batch 300/1771] Loss: 2.9707, Avg Loss: 2.9782\n\n[Epoch 2, Batch 350/1771] Loss: 2.8600, Avg Loss: 2.9729\n\n[Epoch 2, Batch 400/1771] Loss: 2.9823, Avg Loss: 2.9661\n\n[Epoch 2, Batch 450/1771] Loss: 3.0770, Avg Loss: 2.9637\n\n[Epoch 2, Batch 500/1771] Loss: 2.7954, Avg Loss: 2.9578\n\n[Epoch 2, Batch 550/1771] Loss: 2.9355, Avg Loss: 2.9525\n\n[Epoch 2, Batch 600/1771] Loss: 3.0156, Avg Loss: 2.9487\n\n[Epoch 2, Batch 650/1771] Loss: 3.0338, Avg Loss: 2.9447\n\n[Epoch 2, Batch 700/1771] Loss: 2.9316, Avg Loss: 2.9394\n\n[Epoch 2, Batch 750/1771] Loss: 2.6117, Avg Loss: 2.9329\n\n[Epoch 2, Batch 800/1771] Loss: 2.9958, Avg Loss: 2.9307\n\n[Epoch 2, Batch 850/1771] Loss: 2.9032, Avg Loss: 2.9293\n\n[Epoch 2, Batch 900/1771] Loss: 2.5811, Avg Loss: 2.9250\n\n[Epoch 2, Batch 950/1771] Loss: 2.9243, Avg Loss: 2.9217\n\n[Epoch 2, Batch 1000/1771] Loss: 2.8165, Avg Loss: 2.9184\n\n[Epoch 2, Batch 1050/1771] Loss: 2.8307, Avg Loss: 2.9159\n\n[Epoch 2, Batch 1100/1771] Loss: 2.8710, Avg Loss: 2.9138\n\n[Epoch 2, Batch 1150/1771] Loss: 2.9530, Avg Loss: 2.9110\n\n[Epoch 2, Batch 1200/1771] Loss: 2.8227, Avg Loss: 2.9071\n\n[Epoch 2, Batch 1250/1771] Loss: 2.9683, Avg Loss: 2.9037\n\n[Epoch 2, Batch 1300/1771] Loss: 2.6145, Avg Loss: 2.9012\n\n[Epoch 2, Batch 1350/1771] Loss: 2.6423, Avg Loss: 2.8986\n\n[Epoch 2, Batch 1400/1771] Loss: 2.7700, Avg Loss: 2.8957\n\n[Epoch 2, Batch 1450/1771] Loss: 2.8828, Avg Loss: 2.8934\n\n[Epoch 2, Batch 1500/1771] Loss: 2.7256, Avg Loss: 2.8903\n\n[Epoch 2, Batch 1550/1771] Loss: 2.6466, Avg Loss: 2.8861\n\n[Epoch 2, Batch 1600/1771] Loss: 2.7678, Avg Loss: 2.8834\n\n[Epoch 2, Batch 1650/1771] Loss: 2.7238, Avg Loss: 2.8815\n\n[Epoch 2, Batch 1700/1771] Loss: 2.6450, Avg Loss: 2.8792\n\n[Epoch 2, Batch 1750/1771] Loss: 2.9142, Avg Loss: 2.8766\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 3/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 3, Batch 50/1771] Loss: 2.9744, Avg Loss: 2.7697\n\n[Epoch 3, Batch 100/1771] Loss: 2.7629, Avg Loss: 2.7628\n\n[Epoch 3, Batch 150/1771] Loss: 2.7866, Avg Loss: 2.7619\n\n[Epoch 3, Batch 200/1771] Loss: 2.6280, Avg Loss: 2.7650\n\n[Epoch 3, Batch 250/1771] Loss: 2.6957, Avg Loss: 2.7584\n\n[Epoch 3, Batch 300/1771] Loss: 2.7844, Avg Loss: 2.7579\n\n[Epoch 3, Batch 350/1771] Loss: 2.8643, Avg Loss: 2.7514\n\n[Epoch 3, Batch 400/1771] Loss: 2.7218, Avg Loss: 2.7485\n\n[Epoch 3, Batch 450/1771] Loss: 2.9351, Avg Loss: 2.7476\n\n[Epoch 3, Batch 500/1771] Loss: 2.5824, Avg Loss: 2.7451\n\n[Epoch 3, Batch 550/1771] Loss: 2.6333, Avg Loss: 2.7462\n\n[Epoch 3, Batch 600/1771] Loss: 2.8392, Avg Loss: 2.7435\n\n[Epoch 3, Batch 650/1771] Loss: 2.5568, Avg Loss: 2.7414\n\n[Epoch 3, Batch 700/1771] Loss: 2.6538, Avg Loss: 2.7380\n\n[Epoch 3, Batch 750/1771] Loss: 2.7833, Avg Loss: 2.7363\n\n[Epoch 3, Batch 800/1771] Loss: 2.7078, Avg Loss: 2.7340\n\n[Epoch 3, Batch 850/1771] Loss: 2.5995, Avg Loss: 2.7327\n\n[Epoch 3, Batch 900/1771] Loss: 2.7587, Avg Loss: 2.7318\n\n[Epoch 3, Batch 950/1771] Loss: 2.5911, Avg Loss: 2.7284\n\n[Epoch 3, Batch 1000/1771] Loss: 2.7630, Avg Loss: 2.7266\n\n[Epoch 3, Batch 1050/1771] Loss: 2.7942, Avg Loss: 2.7239\n\n[Epoch 3, Batch 1100/1771] Loss: 2.7371, Avg Loss: 2.7222\n\n[Epoch 3, Batch 1150/1771] Loss: 2.7076, Avg Loss: 2.7211\n\n[Epoch 3, Batch 1200/1771] Loss: 2.7743, Avg Loss: 2.7201\n\n[Epoch 3, Batch 1250/1771] Loss: 2.7662, Avg Loss: 2.7193\n\n[Epoch 3, Batch 1300/1771] Loss: 2.7468, Avg Loss: 2.7182\n\n[Epoch 3, Batch 1350/1771] Loss: 2.7364, Avg Loss: 2.7167\n\n[Epoch 3, Batch 1400/1771] Loss: 2.6353, Avg Loss: 2.7152\n\n[Epoch 3, Batch 1450/1771] Loss: 2.8255, Avg Loss: 2.7136\n\n[Epoch 3, Batch 1500/1771] Loss: 2.4918, Avg Loss: 2.7107\n\n[Epoch 3, Batch 1550/1771] Loss: 2.4461, Avg Loss: 2.7085\n\n[Epoch 3, Batch 1600/1771] Loss: 2.6375, Avg Loss: 2.7067\n\n[Epoch 3, Batch 1650/1771] Loss: 2.9490, Avg Loss: 2.7053\n\n[Epoch 3, Batch 1700/1771] Loss: 2.8789, Avg Loss: 2.7031\n\n[Epoch 3, Batch 1750/1771] Loss: 2.6867, Avg Loss: 2.7016\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 4/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 4, Batch 50/1771] Loss: 2.6359, Avg Loss: 2.6674\n\n[Epoch 4, Batch 100/1771] Loss: 2.7942, Avg Loss: 2.6478\n\n[Epoch 4, Batch 150/1771] Loss: 2.7716, Avg Loss: 2.6407\n\n[Epoch 4, Batch 200/1771] Loss: 2.7730, Avg Loss: 2.6452\n\n[Epoch 4, Batch 250/1771] Loss: 2.5882, Avg Loss: 2.6412\n\n[Epoch 4, Batch 300/1771] Loss: 2.7949, Avg Loss: 2.6409\n\n[Epoch 4, Batch 350/1771] Loss: 2.6415, Avg Loss: 2.6325\n\n[Epoch 4, Batch 400/1771] Loss: 2.4575, Avg Loss: 2.6336\n\n[Epoch 4, Batch 450/1771] Loss: 2.6528, Avg Loss: 2.6322\n\n[Epoch 4, Batch 500/1771] Loss: 2.3388, Avg Loss: 2.6297\n\n[Epoch 4, Batch 550/1771] Loss: 2.3367, Avg Loss: 2.6271\n\n[Epoch 4, Batch 600/1771] Loss: 2.4610, Avg Loss: 2.6243\n\n[Epoch 4, Batch 650/1771] Loss: 2.5608, Avg Loss: 2.6228\n\n[Epoch 4, Batch 700/1771] Loss: 2.6160, Avg Loss: 2.6222\n\n[Epoch 4, Batch 750/1771] Loss: 2.6900, Avg Loss: 2.6222\n\n[Epoch 4, Batch 800/1771] Loss: 2.6640, Avg Loss: 2.6213\n\n[Epoch 4, Batch 850/1771] Loss: 2.7051, Avg Loss: 2.6216\n\n[Epoch 4, Batch 900/1771] Loss: 2.5110, Avg Loss: 2.6199\n\n[Epoch 4, Batch 950/1771] Loss: 2.5040, Avg Loss: 2.6178\n\n[Epoch 4, Batch 1000/1771] Loss: 2.5342, Avg Loss: 2.6174\n\n[Epoch 4, Batch 1050/1771] Loss: 2.6483, Avg Loss: 2.6182\n\n[Epoch 4, Batch 1100/1771] Loss: 2.5158, Avg Loss: 2.6175\n\n[Epoch 4, Batch 1150/1771] Loss: 2.7129, Avg Loss: 2.6161\n\n[Epoch 4, Batch 1200/1771] Loss: 2.7791, Avg Loss: 2.6162\n\n[Epoch 4, Batch 1250/1771] Loss: 2.3984, Avg Loss: 2.6151\n\n[Epoch 4, Batch 1300/1771] Loss: 2.6638, Avg Loss: 2.6149\n\n[Epoch 4, Batch 1350/1771] Loss: 2.7070, Avg Loss: 2.6136\n\n[Epoch 4, Batch 1400/1771] Loss: 2.4461, Avg Loss: 2.6114\n\n[Epoch 4, Batch 1450/1771] Loss: 2.6666, Avg Loss: 2.6107\n\n[Epoch 4, Batch 1500/1771] Loss: 2.7408, Avg Loss: 2.6093\n\n[Epoch 4, Batch 1550/1771] Loss: 2.5397, Avg Loss: 2.6073\n\n[Epoch 4, Batch 1600/1771] Loss: 2.8222, Avg Loss: 2.6058\n\n[Epoch 4, Batch 1650/1771] Loss: 2.4913, Avg Loss: 2.6061\n\n[Epoch 4, Batch 1700/1771] Loss: 2.5544, Avg Loss: 2.6057\n\n[Epoch 4, Batch 1750/1771] Loss: 2.4657, Avg Loss: 2.6043\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 5/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 5, Batch 50/1771] Loss: 2.4859, Avg Loss: 2.5794\n\n[Epoch 5, Batch 100/1771] Loss: 2.6279, Avg Loss: 2.5708\n\n[Epoch 5, Batch 150/1771] Loss: 2.7766, Avg Loss: 2.5562\n\n[Epoch 5, Batch 200/1771] Loss: 2.6087, Avg Loss: 2.5613\n\n[Epoch 5, Batch 250/1771] Loss: 2.6899, Avg Loss: 2.5591\n\n[Epoch 5, Batch 300/1771] Loss: 2.7071, Avg Loss: 2.5572\n\n[Epoch 5, Batch 350/1771] Loss: 2.6221, Avg Loss: 2.5605\n\n[Epoch 5, Batch 400/1771] Loss: 2.5225, Avg Loss: 2.5588\n\n[Epoch 5, Batch 450/1771] Loss: 2.3819, Avg Loss: 2.5574\n\n[Epoch 5, Batch 500/1771] Loss: 2.5070, Avg Loss: 2.5584\n\n[Epoch 5, Batch 550/1771] Loss: 2.4461, Avg Loss: 2.5571\n\n[Epoch 5, Batch 600/1771] Loss: 2.5547, Avg Loss: 2.5560\n\n[Epoch 5, Batch 650/1771] Loss: 2.6599, Avg Loss: 2.5547\n\n[Epoch 5, Batch 700/1771] Loss: 2.5471, Avg Loss: 2.5537\n\n[Epoch 5, Batch 750/1771] Loss: 2.7237, Avg Loss: 2.5522\n\n[Epoch 5, Batch 800/1771] Loss: 2.4671, Avg Loss: 2.5519\n\n[Epoch 5, Batch 850/1771] Loss: 2.6620, Avg Loss: 2.5499\n\n[Epoch 5, Batch 900/1771] Loss: 2.6058, Avg Loss: 2.5493\n\n[Epoch 5, Batch 950/1771] Loss: 2.4431, Avg Loss: 2.5478\n\n[Epoch 5, Batch 1000/1771] Loss: 2.6847, Avg Loss: 2.5465\n\n[Epoch 5, Batch 1050/1771] Loss: 2.4192, Avg Loss: 2.5466\n\n[Epoch 5, Batch 1100/1771] Loss: 2.6091, Avg Loss: 2.5464\n\n[Epoch 5, Batch 1150/1771] Loss: 2.5446, Avg Loss: 2.5459\n\n[Epoch 5, Batch 1200/1771] Loss: 2.5955, Avg Loss: 2.5471\n\n[Epoch 5, Batch 1250/1771] Loss: 2.3338, Avg Loss: 2.5468\n\n[Epoch 5, Batch 1300/1771] Loss: 2.3154, Avg Loss: 2.5457\n\n[Epoch 5, Batch 1350/1771] Loss: 2.6525, Avg Loss: 2.5451\n\n[Epoch 5, Batch 1400/1771] Loss: 2.5920, Avg Loss: 2.5453\n\n[Epoch 5, Batch 1450/1771] Loss: 2.4881, Avg Loss: 2.5446\n\n[Epoch 5, Batch 1500/1771] Loss: 2.4119, Avg Loss: 2.5452\n\n[Epoch 5, Batch 1550/1771] Loss: 2.2725, Avg Loss: 2.5454\n\n[Epoch 5, Batch 1600/1771] Loss: 2.5031, Avg Loss: 2.5443\n\n[Epoch 5, Batch 1650/1771] Loss: 2.4022, Avg Loss: 2.5442\n\n[Epoch 5, Batch 1700/1771] Loss: 2.3827, Avg Loss: 2.5428\n\n[Epoch 5, Batch 1750/1771] Loss: 2.5166, Avg Loss: 2.5420\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 6/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 6, Batch 50/1771] Loss: 2.4767, Avg Loss: 2.4537\n\n[Epoch 6, Batch 100/1771] Loss: 2.6974, Avg Loss: 2.4822\n\n[Epoch 6, Batch 150/1771] Loss: 2.3154, Avg Loss: 2.4899\n\n[Epoch 6, Batch 200/1771] Loss: 2.5951, Avg Loss: 2.4919\n\n[Epoch 6, Batch 250/1771] Loss: 2.5937, Avg Loss: 2.4925\n\n[Epoch 6, Batch 300/1771] Loss: 2.2983, Avg Loss: 2.4943\n\n[Epoch 6, Batch 350/1771] Loss: 2.5343, Avg Loss: 2.4976\n\n[Epoch 6, Batch 400/1771] Loss: 2.4852, Avg Loss: 2.4994\n\n[Epoch 6, Batch 450/1771] Loss: 2.5924, Avg Loss: 2.4983\n\n[Epoch 6, Batch 500/1771] Loss: 2.4710, Avg Loss: 2.5013\n\n[Epoch 6, Batch 550/1771] Loss: 2.8367, Avg Loss: 2.5027\n\n[Epoch 6, Batch 600/1771] Loss: 2.4244, Avg Loss: 2.5028\n\n[Epoch 6, Batch 650/1771] Loss: 2.5815, Avg Loss: 2.5016\n\n[Epoch 6, Batch 700/1771] Loss: 2.6141, Avg Loss: 2.4990\n\n[Epoch 6, Batch 750/1771] Loss: 2.3119, Avg Loss: 2.4979\n\n[Epoch 6, Batch 800/1771] Loss: 2.5451, Avg Loss: 2.4968\n\n[Epoch 6, Batch 850/1771] Loss: 2.4486, Avg Loss: 2.4955\n\n[Epoch 6, Batch 900/1771] Loss: 2.3460, Avg Loss: 2.4937\n\n[Epoch 6, Batch 950/1771] Loss: 2.3727, Avg Loss: 2.4933\n\n[Epoch 6, Batch 1000/1771] Loss: 2.6039, Avg Loss: 2.4925\n\n[Epoch 6, Batch 1050/1771] Loss: 2.3583, Avg Loss: 2.4921\n\n[Epoch 6, Batch 1100/1771] Loss: 2.5346, Avg Loss: 2.4924\n\n[Epoch 6, Batch 1150/1771] Loss: 2.3245, Avg Loss: 2.4931\n\n[Epoch 6, Batch 1200/1771] Loss: 2.6444, Avg Loss: 2.4929\n\n[Epoch 6, Batch 1250/1771] Loss: 2.4952, Avg Loss: 2.4923\n\n[Epoch 6, Batch 1300/1771] Loss: 2.5794, Avg Loss: 2.4917\n\n[Epoch 6, Batch 1350/1771] Loss: 2.6206, Avg Loss: 2.4915\n\n[Epoch 6, Batch 1400/1771] Loss: 2.3677, Avg Loss: 2.4907\n\n[Epoch 6, Batch 1450/1771] Loss: 2.4735, Avg Loss: 2.4908\n\n[Epoch 6, Batch 1500/1771] Loss: 2.3606, Avg Loss: 2.4908\n\n[Epoch 6, Batch 1550/1771] Loss: 2.6314, Avg Loss: 2.4909\n\n[Epoch 6, Batch 1600/1771] Loss: 2.5014, Avg Loss: 2.4915\n\n[Epoch 6, Batch 1650/1771] Loss: 2.3625, Avg Loss: 2.4923\n\n[Epoch 6, Batch 1700/1771] Loss: 2.4023, Avg Loss: 2.4914\n\n[Epoch 6, Batch 1750/1771] Loss: 2.5719, Avg Loss: 2.4905\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 7/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 7, Batch 50/1771] Loss: 2.4873, Avg Loss: 2.4373\n\n[Epoch 7, Batch 100/1771] Loss: 2.5358, Avg Loss: 2.4440\n\n[Epoch 7, Batch 150/1771] Loss: 2.3549, Avg Loss: 2.4551\n\n[Epoch 7, Batch 200/1771] Loss: 2.6710, Avg Loss: 2.4614\n\n[Epoch 7, Batch 250/1771] Loss: 2.4013, Avg Loss: 2.4669\n\n[Epoch 7, Batch 300/1771] Loss: 2.4142, Avg Loss: 2.4699\n\n[Epoch 7, Batch 350/1771] Loss: 2.2666, Avg Loss: 2.4640\n\n[Epoch 7, Batch 400/1771] Loss: 2.4812, Avg Loss: 2.4622\n\n[Epoch 7, Batch 450/1771] Loss: 2.2281, Avg Loss: 2.4589\n\n[Epoch 7, Batch 500/1771] Loss: 2.3347, Avg Loss: 2.4628\n\n[Epoch 7, Batch 550/1771] Loss: 2.5953, Avg Loss: 2.4611\n\n[Epoch 7, Batch 600/1771] Loss: 2.3766, Avg Loss: 2.4588\n\n[Epoch 7, Batch 650/1771] Loss: 2.4566, Avg Loss: 2.4571\n\n[Epoch 7, Batch 700/1771] Loss: 2.4622, Avg Loss: 2.4568\n\n[Epoch 7, Batch 750/1771] Loss: 2.6176, Avg Loss: 2.4564\n\n[Epoch 7, Batch 800/1771] Loss: 2.5669, Avg Loss: 2.4571\n\n[Epoch 7, Batch 850/1771] Loss: 2.4784, Avg Loss: 2.4579\n\n[Epoch 7, Batch 900/1771] Loss: 2.4801, Avg Loss: 2.4582\n\n[Epoch 7, Batch 950/1771] Loss: 2.4134, Avg Loss: 2.4574\n\n[Epoch 7, Batch 1000/1771] Loss: 2.5306, Avg Loss: 2.4569\n\n[Epoch 7, Batch 1050/1771] Loss: 2.5042, Avg Loss: 2.4564\n\n[Epoch 7, Batch 1100/1771] Loss: 2.4601, Avg Loss: 2.4565\n\n[Epoch 7, Batch 1150/1771] Loss: 2.3914, Avg Loss: 2.4560\n\n[Epoch 7, Batch 1200/1771] Loss: 2.5962, Avg Loss: 2.4548\n\n[Epoch 7, Batch 1250/1771] Loss: 2.6076, Avg Loss: 2.4554\n\n[Epoch 7, Batch 1300/1771] Loss: 2.3623, Avg Loss: 2.4559\n\n[Epoch 7, Batch 1350/1771] Loss: 2.2677, Avg Loss: 2.4552\n\n[Epoch 7, Batch 1400/1771] Loss: 2.3062, Avg Loss: 2.4554\n\n[Epoch 7, Batch 1450/1771] Loss: 2.3367, Avg Loss: 2.4546\n\n[Epoch 7, Batch 1500/1771] Loss: 2.6453, Avg Loss: 2.4544\n\n[Epoch 7, Batch 1550/1771] Loss: 2.2048, Avg Loss: 2.4528\n\n[Epoch 7, Batch 1600/1771] Loss: 2.5108, Avg Loss: 2.4530\n\n[Epoch 7, Batch 1650/1771] Loss: 2.6233, Avg Loss: 2.4525\n\n[Epoch 7, Batch 1700/1771] Loss: 2.3998, Avg Loss: 2.4510\n\n[Epoch 7, Batch 1750/1771] Loss: 2.6066, Avg Loss: 2.4501\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 8/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 8, Batch 50/1771] Loss: 2.4010, Avg Loss: 2.4205\n\n[Epoch 8, Batch 100/1771] Loss: 2.4848, Avg Loss: 2.4295\n\n[Epoch 8, Batch 150/1771] Loss: 2.4269, Avg Loss: 2.4297\n\n[Epoch 8, Batch 200/1771] Loss: 2.5145, Avg Loss: 2.4330\n\n[Epoch 8, Batch 250/1771] Loss: 2.4033, Avg Loss: 2.4329\n\n[Epoch 8, Batch 300/1771] Loss: 2.5372, Avg Loss: 2.4314\n\n[Epoch 8, Batch 350/1771] Loss: 2.4082, Avg Loss: 2.4271\n\n[Epoch 8, Batch 400/1771] Loss: 2.4552, Avg Loss: 2.4244\n\n[Epoch 8, Batch 450/1771] Loss: 2.5642, Avg Loss: 2.4243\n\n[Epoch 8, Batch 500/1771] Loss: 2.4710, Avg Loss: 2.4261\n\n[Epoch 8, Batch 550/1771] Loss: 2.2055, Avg Loss: 2.4227\n\n[Epoch 8, Batch 600/1771] Loss: 2.2503, Avg Loss: 2.4225\n\n[Epoch 8, Batch 650/1771] Loss: 2.4742, Avg Loss: 2.4223\n\n[Epoch 8, Batch 700/1771] Loss: 2.2480, Avg Loss: 2.4219\n\n[Epoch 8, Batch 750/1771] Loss: 2.4745, Avg Loss: 2.4214\n\n[Epoch 8, Batch 800/1771] Loss: 2.4982, Avg Loss: 2.4211\n\n[Epoch 8, Batch 850/1771] Loss: 2.4166, Avg Loss: 2.4203\n\n[Epoch 8, Batch 900/1771] Loss: 2.4374, Avg Loss: 2.4190\n\n[Epoch 8, Batch 950/1771] Loss: 2.4542, Avg Loss: 2.4189\n\n[Epoch 8, Batch 1000/1771] Loss: 2.2097, Avg Loss: 2.4180\n\n[Epoch 8, Batch 1050/1771] Loss: 2.4972, Avg Loss: 2.4189\n\n[Epoch 8, Batch 1100/1771] Loss: 2.6704, Avg Loss: 2.4196\n\n[Epoch 8, Batch 1150/1771] Loss: 2.5319, Avg Loss: 2.4199\n\n[Epoch 8, Batch 1200/1771] Loss: 2.3163, Avg Loss: 2.4200\n\n[Epoch 8, Batch 1250/1771] Loss: 2.3040, Avg Loss: 2.4200\n\n[Epoch 8, Batch 1300/1771] Loss: 2.4462, Avg Loss: 2.4196\n\n[Epoch 8, Batch 1350/1771] Loss: 2.3347, Avg Loss: 2.4195\n\n[Epoch 8, Batch 1400/1771] Loss: 2.2549, Avg Loss: 2.4183\n\n[Epoch 8, Batch 1450/1771] Loss: 2.3547, Avg Loss: 2.4185\n\n[Epoch 8, Batch 1500/1771] Loss: 2.3392, Avg Loss: 2.4170\n\n[Epoch 8, Batch 1550/1771] Loss: 2.2583, Avg Loss: 2.4163\n\n[Epoch 8, Batch 1600/1771] Loss: 2.6174, Avg Loss: 2.4160\n\n[Epoch 8, Batch 1650/1771] Loss: 2.4825, Avg Loss: 2.4162\n\n[Epoch 8, Batch 1700/1771] Loss: 2.4517, Avg Loss: 2.4158\n\n[Epoch 8, Batch 1750/1771] Loss: 2.5687, Avg Loss: 2.4158\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 9/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 9, Batch 50/1771] Loss: 2.3778, Avg Loss: 2.3797\n\n[Epoch 9, Batch 100/1771] Loss: 2.4537, Avg Loss: 2.3858\n\n[Epoch 9, Batch 150/1771] Loss: 2.2882, Avg Loss: 2.3937\n\n[Epoch 9, Batch 200/1771] Loss: 2.3451, Avg Loss: 2.3944\n\n[Epoch 9, Batch 250/1771] Loss: 2.4659, Avg Loss: 2.3884\n\n[Epoch 9, Batch 300/1771] Loss: 2.3705, Avg Loss: 2.3892\n\n[Epoch 9, Batch 350/1771] Loss: 2.2898, Avg Loss: 2.3862\n\n[Epoch 9, Batch 400/1771] Loss: 2.2679, Avg Loss: 2.3879\n\n[Epoch 9, Batch 450/1771] Loss: 2.4147, Avg Loss: 2.3923\n\n[Epoch 9, Batch 500/1771] Loss: 2.4622, Avg Loss: 2.3944\n\n[Epoch 9, Batch 550/1771] Loss: 2.1256, Avg Loss: 2.3937\n\n[Epoch 9, Batch 600/1771] Loss: 2.3332, Avg Loss: 2.3930\n\n[Epoch 9, Batch 650/1771] Loss: 2.4218, Avg Loss: 2.3923\n\n[Epoch 9, Batch 700/1771] Loss: 2.2270, Avg Loss: 2.3912\n\n[Epoch 9, Batch 750/1771] Loss: 2.6186, Avg Loss: 2.3919\n\n[Epoch 9, Batch 800/1771] Loss: 2.4398, Avg Loss: 2.3909\n\n[Epoch 9, Batch 850/1771] Loss: 2.2120, Avg Loss: 2.3903\n\n[Epoch 9, Batch 900/1771] Loss: 2.1965, Avg Loss: 2.3897\n\n[Epoch 9, Batch 950/1771] Loss: 2.3219, Avg Loss: 2.3896\n\n[Epoch 9, Batch 1000/1771] Loss: 2.2394, Avg Loss: 2.3904\n\n[Epoch 9, Batch 1050/1771] Loss: 2.4088, Avg Loss: 2.3895\n\n[Epoch 9, Batch 1100/1771] Loss: 2.5751, Avg Loss: 2.3902\n\n[Epoch 9, Batch 1150/1771] Loss: 2.3846, Avg Loss: 2.3909\n\n[Epoch 9, Batch 1200/1771] Loss: 2.3049, Avg Loss: 2.3902\n\n[Epoch 9, Batch 1250/1771] Loss: 2.4937, Avg Loss: 2.3890\n\n[Epoch 9, Batch 1300/1771] Loss: 2.4379, Avg Loss: 2.3890\n\n[Epoch 9, Batch 1350/1771] Loss: 2.3967, Avg Loss: 2.3890\n\n[Epoch 9, Batch 1400/1771] Loss: 2.3024, Avg Loss: 2.3882\n\n[Epoch 9, Batch 1450/1771] Loss: 2.4562, Avg Loss: 2.3875\n\n[Epoch 9, Batch 1500/1771] Loss: 2.4115, Avg Loss: 2.3876\n\n[Epoch 9, Batch 1550/1771] Loss: 2.4769, Avg Loss: 2.3866\n\n[Epoch 9, Batch 1600/1771] Loss: 2.3248, Avg Loss: 2.3856\n\n[Epoch 9, Batch 1650/1771] Loss: 2.3979, Avg Loss: 2.3854\n\n[Epoch 9, Batch 1700/1771] Loss: 2.2116, Avg Loss: 2.3854\n\n[Epoch 9, Batch 1750/1771] Loss: 2.4435, Avg Loss: 2.3851\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 10/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 10, Batch 50/1771] Loss: 2.4220, Avg Loss: 2.3655\n\n[Epoch 10, Batch 100/1771] Loss: 2.3943, Avg Loss: 2.3606\n\n[Epoch 10, Batch 150/1771] Loss: 2.3991, Avg Loss: 2.3656\n\n[Epoch 10, Batch 200/1771] Loss: 2.4193, Avg Loss: 2.3730\n\n[Epoch 10, Batch 250/1771] Loss: 2.2467, Avg Loss: 2.3704\n\n[Epoch 10, Batch 300/1771] Loss: 2.3159, Avg Loss: 2.3702\n\n[Epoch 10, Batch 350/1771] Loss: 2.1315, Avg Loss: 2.3659\n\n[Epoch 10, Batch 400/1771] Loss: 2.3937, Avg Loss: 2.3692\n\n[Epoch 10, Batch 450/1771] Loss: 2.4436, Avg Loss: 2.3665\n\n[Epoch 10, Batch 500/1771] Loss: 2.3457, Avg Loss: 2.3663\n\n[Epoch 10, Batch 550/1771] Loss: 2.4308, Avg Loss: 2.3668\n\n[Epoch 10, Batch 600/1771] Loss: 2.3735, Avg Loss: 2.3665\n\n[Epoch 10, Batch 650/1771] Loss: 2.6051, Avg Loss: 2.3673\n\n[Epoch 10, Batch 700/1771] Loss: 2.4102, Avg Loss: 2.3667\n\n[Epoch 10, Batch 750/1771] Loss: 2.2874, Avg Loss: 2.3668\n\n[Epoch 10, Batch 800/1771] Loss: 2.4220, Avg Loss: 2.3676\n\n[Epoch 10, Batch 850/1771] Loss: 2.4431, Avg Loss: 2.3697\n\n[Epoch 10, Batch 900/1771] Loss: 2.2691, Avg Loss: 2.3667\n\n[Epoch 10, Batch 950/1771] Loss: 2.4742, Avg Loss: 2.3663\n\n[Epoch 10, Batch 1000/1771] Loss: 2.5677, Avg Loss: 2.3653\n\n[Epoch 10, Batch 1050/1771] Loss: 2.3095, Avg Loss: 2.3657\n\n[Epoch 10, Batch 1100/1771] Loss: 2.5522, Avg Loss: 2.3657\n\n[Epoch 10, Batch 1150/1771] Loss: 2.5493, Avg Loss: 2.3659\n\n[Epoch 10, Batch 1200/1771] Loss: 2.3791, Avg Loss: 2.3652\n\n[Epoch 10, Batch 1250/1771] Loss: 2.7478, Avg Loss: 2.3658\n\n[Epoch 10, Batch 1300/1771] Loss: 2.3753, Avg Loss: 2.3665\n\n[Epoch 10, Batch 1350/1771] Loss: 2.0688, Avg Loss: 2.3659\n\n[Epoch 10, Batch 1400/1771] Loss: 2.2482, Avg Loss: 2.3643\n\n[Epoch 10, Batch 1450/1771] Loss: 2.3607, Avg Loss: 2.3648\n\n[Epoch 10, Batch 1500/1771] Loss: 2.4017, Avg Loss: 2.3646\n\n[Epoch 10, Batch 1550/1771] Loss: 2.3978, Avg Loss: 2.3643\n\n[Epoch 10, Batch 1600/1771] Loss: 2.2998, Avg Loss: 2.3640\n\n[Epoch 10, Batch 1650/1771] Loss: 2.3303, Avg Loss: 2.3638\n\n[Epoch 10, Batch 1700/1771] Loss: 2.1798, Avg Loss: 2.3640\n\n[Epoch 10, Batch 1750/1771] Loss: 2.5446, Avg Loss: 2.3639\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 11/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 11, Batch 50/1771] Loss: 2.2737, Avg Loss: 2.3246\n\n[Epoch 11, Batch 100/1771] Loss: 2.3195, Avg Loss: 2.3385\n\n[Epoch 11, Batch 150/1771] Loss: 2.0831, Avg Loss: 2.3477\n\n[Epoch 11, Batch 200/1771] Loss: 2.5983, Avg Loss: 2.3540\n\n[Epoch 11, Batch 250/1771] Loss: 2.3413, Avg Loss: 2.3528\n\n[Epoch 11, Batch 300/1771] Loss: 2.4179, Avg Loss: 2.3471\n\n[Epoch 11, Batch 350/1771] Loss: 2.4245, Avg Loss: 2.3507\n\n[Epoch 11, Batch 400/1771] Loss: 2.3345, Avg Loss: 2.3504\n\n[Epoch 11, Batch 450/1771] Loss: 2.3221, Avg Loss: 2.3511\n\n[Epoch 11, Batch 500/1771] Loss: 2.3666, Avg Loss: 2.3484\n\n[Epoch 11, Batch 550/1771] Loss: 2.3213, Avg Loss: 2.3490\n\n[Epoch 11, Batch 600/1771] Loss: 2.1217, Avg Loss: 2.3479\n\n[Epoch 11, Batch 650/1771] Loss: 2.5571, Avg Loss: 2.3483\n\n[Epoch 11, Batch 700/1771] Loss: 2.3383, Avg Loss: 2.3465\n\n[Epoch 11, Batch 750/1771] Loss: 2.4732, Avg Loss: 2.3462\n\n[Epoch 11, Batch 800/1771] Loss: 2.4939, Avg Loss: 2.3462\n\n[Epoch 11, Batch 850/1771] Loss: 2.5490, Avg Loss: 2.3458\n\n[Epoch 11, Batch 900/1771] Loss: 2.4094, Avg Loss: 2.3467\n\n[Epoch 11, Batch 950/1771] Loss: 2.4082, Avg Loss: 2.3465\n\n[Epoch 11, Batch 1000/1771] Loss: 2.4880, Avg Loss: 2.3460\n\n[Epoch 11, Batch 1050/1771] Loss: 2.4026, Avg Loss: 2.3463\n\n[Epoch 11, Batch 1100/1771] Loss: 2.3187, Avg Loss: 2.3475\n\n[Epoch 11, Batch 1150/1771] Loss: 2.1708, Avg Loss: 2.3474\n\n[Epoch 11, Batch 1200/1771] Loss: 1.9995, Avg Loss: 2.3466\n\n[Epoch 11, Batch 1250/1771] Loss: 2.3985, Avg Loss: 2.3449\n\n[Epoch 11, Batch 1300/1771] Loss: 2.3721, Avg Loss: 2.3448\n\n[Epoch 11, Batch 1350/1771] Loss: 1.9496, Avg Loss: 2.3442\n\n[Epoch 11, Batch 1400/1771] Loss: 2.1803, Avg Loss: 2.3440\n\n[Epoch 11, Batch 1450/1771] Loss: 2.3897, Avg Loss: 2.3444\n\n[Epoch 11, Batch 1500/1771] Loss: 2.2242, Avg Loss: 2.3438\n\n[Epoch 11, Batch 1550/1771] Loss: 2.1168, Avg Loss: 2.3437\n\n[Epoch 11, Batch 1600/1771] Loss: 2.4090, Avg Loss: 2.3442\n\n[Epoch 11, Batch 1650/1771] Loss: 2.2185, Avg Loss: 2.3451\n\n[Epoch 11, Batch 1700/1771] Loss: 2.1913, Avg Loss: 2.3448\n\n[Epoch 11, Batch 1750/1771] Loss: 2.1408, Avg Loss: 2.3455\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 12/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 12, Batch 50/1771] Loss: 2.3454, Avg Loss: 2.3196\n\n[Epoch 12, Batch 100/1771] Loss: 2.0817, Avg Loss: 2.3107\n\n[Epoch 12, Batch 150/1771] Loss: 2.2494, Avg Loss: 2.3109\n\n[Epoch 12, Batch 200/1771] Loss: 2.3517, Avg Loss: 2.3099\n\n[Epoch 12, Batch 250/1771] Loss: 2.5228, Avg Loss: 2.3132\n\n[Epoch 12, Batch 300/1771] Loss: 2.2052, Avg Loss: 2.3157\n\n[Epoch 12, Batch 350/1771] Loss: 2.6132, Avg Loss: 2.3198\n\n[Epoch 12, Batch 400/1771] Loss: 2.2975, Avg Loss: 2.3217\n\n[Epoch 12, Batch 450/1771] Loss: 2.2306, Avg Loss: 2.3232\n\n[Epoch 12, Batch 500/1771] Loss: 2.1525, Avg Loss: 2.3253\n\n[Epoch 12, Batch 550/1771] Loss: 2.5584, Avg Loss: 2.3275\n\n[Epoch 12, Batch 600/1771] Loss: 2.2591, Avg Loss: 2.3263\n\n[Epoch 12, Batch 650/1771] Loss: 2.1194, Avg Loss: 2.3250\n\n[Epoch 12, Batch 700/1771] Loss: 2.2865, Avg Loss: 2.3238\n\n[Epoch 12, Batch 750/1771] Loss: 2.2730, Avg Loss: 2.3247\n\n[Epoch 12, Batch 800/1771] Loss: 2.2039, Avg Loss: 2.3250\n\n[Epoch 12, Batch 850/1771] Loss: 2.4397, Avg Loss: 2.3249\n\n[Epoch 12, Batch 900/1771] Loss: 2.2936, Avg Loss: 2.3243\n\n[Epoch 12, Batch 950/1771] Loss: 2.1946, Avg Loss: 2.3233\n\n[Epoch 12, Batch 1000/1771] Loss: 2.1920, Avg Loss: 2.3229\n\n[Epoch 12, Batch 1050/1771] Loss: 2.5622, Avg Loss: 2.3231\n\n[Epoch 12, Batch 1100/1771] Loss: 2.3687, Avg Loss: 2.3238\n\n[Epoch 12, Batch 1150/1771] Loss: 2.2870, Avg Loss: 2.3239\n\n[Epoch 12, Batch 1200/1771] Loss: 2.2480, Avg Loss: 2.3236\n\n[Epoch 12, Batch 1250/1771] Loss: 2.2668, Avg Loss: 2.3229\n\n[Epoch 12, Batch 1300/1771] Loss: 2.1806, Avg Loss: 2.3236\n\n[Epoch 12, Batch 1350/1771] Loss: 2.3831, Avg Loss: 2.3233\n\n[Epoch 12, Batch 1400/1771] Loss: 2.3892, Avg Loss: 2.3236\n\n[Epoch 12, Batch 1450/1771] Loss: 2.4504, Avg Loss: 2.3243\n\n[Epoch 12, Batch 1500/1771] Loss: 2.2177, Avg Loss: 2.3245\n\n[Epoch 12, Batch 1550/1771] Loss: 2.3985, Avg Loss: 2.3244\n\n[Epoch 12, Batch 1600/1771] Loss: 2.4307, Avg Loss: 2.3248\n\n[Epoch 12, Batch 1650/1771] Loss: 2.3142, Avg Loss: 2.3240\n\n[Epoch 12, Batch 1700/1771] Loss: 2.3339, Avg Loss: 2.3242\n\n[Epoch 12, Batch 1750/1771] Loss: 2.2678, Avg Loss: 2.3234\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 13/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 13, Batch 50/1771] Loss: 2.3936, Avg Loss: 2.3348\n\n[Epoch 13, Batch 100/1771] Loss: 2.4195, Avg Loss: 2.3318\n\n[Epoch 13, Batch 150/1771] Loss: 2.2794, Avg Loss: 2.3391\n\n[Epoch 13, Batch 200/1771] Loss: 2.1689, Avg Loss: 2.3336\n\n[Epoch 13, Batch 250/1771] Loss: 2.1893, Avg Loss: 2.3333\n\n[Epoch 13, Batch 300/1771] Loss: 2.2591, Avg Loss: 2.3222\n\n[Epoch 13, Batch 350/1771] Loss: 2.3818, Avg Loss: 2.3195\n\n[Epoch 13, Batch 400/1771] Loss: 2.4291, Avg Loss: 2.3212\n\n[Epoch 13, Batch 450/1771] Loss: 2.4139, Avg Loss: 2.3188\n\n[Epoch 13, Batch 500/1771] Loss: 2.2497, Avg Loss: 2.3186\n\n[Epoch 13, Batch 550/1771] Loss: 2.3262, Avg Loss: 2.3175\n\n[Epoch 13, Batch 600/1771] Loss: 2.3369, Avg Loss: 2.3161\n\n[Epoch 13, Batch 650/1771] Loss: 2.3231, Avg Loss: 2.3158\n\n[Epoch 13, Batch 700/1771] Loss: 2.3296, Avg Loss: 2.3152\n\n[Epoch 13, Batch 750/1771] Loss: 2.2100, Avg Loss: 2.3144\n\n[Epoch 13, Batch 800/1771] Loss: 2.2546, Avg Loss: 2.3126\n\n[Epoch 13, Batch 850/1771] Loss: 2.4575, Avg Loss: 2.3114\n\n[Epoch 13, Batch 900/1771] Loss: 2.2328, Avg Loss: 2.3109\n\n[Epoch 13, Batch 950/1771] Loss: 2.4490, Avg Loss: 2.3115\n\n[Epoch 13, Batch 1000/1771] Loss: 2.2468, Avg Loss: 2.3113\n\n[Epoch 13, Batch 1050/1771] Loss: 2.3809, Avg Loss: 2.3112\n\n[Epoch 13, Batch 1100/1771] Loss: 2.2782, Avg Loss: 2.3109\n\n[Epoch 13, Batch 1150/1771] Loss: 2.2663, Avg Loss: 2.3106\n\n[Epoch 13, Batch 1200/1771] Loss: 2.6758, Avg Loss: 2.3112\n\n[Epoch 13, Batch 1250/1771] Loss: 2.4060, Avg Loss: 2.3121\n\n[Epoch 13, Batch 1300/1771] Loss: 2.1295, Avg Loss: 2.3113\n\n[Epoch 13, Batch 1350/1771] Loss: 2.3529, Avg Loss: 2.3113\n\n[Epoch 13, Batch 1400/1771] Loss: 2.1986, Avg Loss: 2.3106\n\n[Epoch 13, Batch 1450/1771] Loss: 2.3248, Avg Loss: 2.3106\n\n[Epoch 13, Batch 1500/1771] Loss: 2.5997, Avg Loss: 2.3109\n\n[Epoch 13, Batch 1550/1771] Loss: 2.2982, Avg Loss: 2.3117\n\n[Epoch 13, Batch 1600/1771] Loss: 2.4191, Avg Loss: 2.3110\n\n[Epoch 13, Batch 1650/1771] Loss: 2.3279, Avg Loss: 2.3104\n\n[Epoch 13, Batch 1700/1771] Loss: 2.2226, Avg Loss: 2.3103\n\n[Epoch 13, Batch 1750/1771] Loss: 2.3523, Avg Loss: 2.3103\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 14/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 14, Batch 50/1771] Loss: 2.3887, Avg Loss: 2.3094\n\n[Epoch 14, Batch 100/1771] Loss: 2.2910, Avg Loss: 2.3027\n\n[Epoch 14, Batch 150/1771] Loss: 2.2943, Avg Loss: 2.2995\n\n[Epoch 14, Batch 200/1771] Loss: 2.1163, Avg Loss: 2.2975\n\n[Epoch 14, Batch 250/1771] Loss: 2.4433, Avg Loss: 2.2997\n\n[Epoch 14, Batch 300/1771] Loss: 2.1489, Avg Loss: 2.3010\n\n[Epoch 14, Batch 350/1771] Loss: 2.3214, Avg Loss: 2.2991\n\n[Epoch 14, Batch 400/1771] Loss: 2.3610, Avg Loss: 2.2998\n\n[Epoch 14, Batch 450/1771] Loss: 2.1790, Avg Loss: 2.3027\n\n[Epoch 14, Batch 500/1771] Loss: 2.2348, Avg Loss: 2.3030\n\n[Epoch 14, Batch 550/1771] Loss: 2.5175, Avg Loss: 2.3014\n\n[Epoch 14, Batch 600/1771] Loss: 2.5167, Avg Loss: 2.3023\n\n[Epoch 14, Batch 650/1771] Loss: 2.1665, Avg Loss: 2.3041\n\n[Epoch 14, Batch 700/1771] Loss: 2.4354, Avg Loss: 2.3016\n\n[Epoch 14, Batch 750/1771] Loss: 2.3206, Avg Loss: 2.3013\n\n[Epoch 14, Batch 800/1771] Loss: 2.3176, Avg Loss: 2.3006\n\n[Epoch 14, Batch 850/1771] Loss: 2.2998, Avg Loss: 2.2999\n\n[Epoch 14, Batch 900/1771] Loss: 2.1837, Avg Loss: 2.2994\n\n[Epoch 14, Batch 950/1771] Loss: 2.1490, Avg Loss: 2.2993\n\n[Epoch 14, Batch 1000/1771] Loss: 2.2736, Avg Loss: 2.2996\n\n[Epoch 14, Batch 1050/1771] Loss: 2.3158, Avg Loss: 2.2985\n\n[Epoch 14, Batch 1100/1771] Loss: 1.9868, Avg Loss: 2.2990\n\n[Epoch 14, Batch 1150/1771] Loss: 2.2534, Avg Loss: 2.2989\n\n[Epoch 14, Batch 1200/1771] Loss: 2.5192, Avg Loss: 2.2989\n\n[Epoch 14, Batch 1250/1771] Loss: 2.1467, Avg Loss: 2.2988\n\n[Epoch 14, Batch 1300/1771] Loss: 2.3842, Avg Loss: 2.2978\n\n[Epoch 14, Batch 1350/1771] Loss: 2.4499, Avg Loss: 2.2975\n\n[Epoch 14, Batch 1400/1771] Loss: 2.2283, Avg Loss: 2.2965\n\n[Epoch 14, Batch 1450/1771] Loss: 2.3063, Avg Loss: 2.2969\n\n[Epoch 14, Batch 1500/1771] Loss: 2.2631, Avg Loss: 2.2965\n\n[Epoch 14, Batch 1550/1771] Loss: 2.2196, Avg Loss: 2.2968\n\n[Epoch 14, Batch 1600/1771] Loss: 2.2667, Avg Loss: 2.2975\n\n[Epoch 14, Batch 1650/1771] Loss: 2.3123, Avg Loss: 2.2974\n\n[Epoch 14, Batch 1700/1771] Loss: 2.3482, Avg Loss: 2.2981\n\n[Epoch 14, Batch 1750/1771] Loss: 2.3570, Avg Loss: 2.2979\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 15/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 15, Batch 50/1771] Loss: 2.4858, Avg Loss: 2.2773\n\n[Epoch 15, Batch 100/1771] Loss: 2.3037, Avg Loss: 2.2772\n\n[Epoch 15, Batch 150/1771] Loss: 2.2506, Avg Loss: 2.2792\n\n[Epoch 15, Batch 200/1771] Loss: 2.2713, Avg Loss: 2.2777\n\n[Epoch 15, Batch 250/1771] Loss: 2.2212, Avg Loss: 2.2860\n\n[Epoch 15, Batch 300/1771] Loss: 2.1943, Avg Loss: 2.2848\n\n[Epoch 15, Batch 350/1771] Loss: 2.1915, Avg Loss: 2.2792\n\n[Epoch 15, Batch 400/1771] Loss: 2.3507, Avg Loss: 2.2789\n\n[Epoch 15, Batch 450/1771] Loss: 2.4260, Avg Loss: 2.2793\n\n[Epoch 15, Batch 500/1771] Loss: 2.2689, Avg Loss: 2.2781\n\n[Epoch 15, Batch 550/1771] Loss: 2.2336, Avg Loss: 2.2800\n\n[Epoch 15, Batch 600/1771] Loss: 2.3788, Avg Loss: 2.2803\n\n[Epoch 15, Batch 650/1771] Loss: 2.1933, Avg Loss: 2.2813\n\n[Epoch 15, Batch 700/1771] Loss: 2.1901, Avg Loss: 2.2813\n\n[Epoch 15, Batch 750/1771] Loss: 2.2751, Avg Loss: 2.2814\n\n[Epoch 15, Batch 800/1771] Loss: 2.1871, Avg Loss: 2.2816\n\n[Epoch 15, Batch 850/1771] Loss: 2.2386, Avg Loss: 2.2802\n\n[Epoch 15, Batch 900/1771] Loss: 2.3304, Avg Loss: 2.2814\n\n[Epoch 15, Batch 950/1771] Loss: 2.4864, Avg Loss: 2.2815\n\n[Epoch 15, Batch 1000/1771] Loss: 2.0722, Avg Loss: 2.2809\n\n[Epoch 15, Batch 1050/1771] Loss: 2.2716, Avg Loss: 2.2813\n\n[Epoch 15, Batch 1100/1771] Loss: 2.2384, Avg Loss: 2.2823\n\n[Epoch 15, Batch 1150/1771] Loss: 2.3267, Avg Loss: 2.2823\n\n[Epoch 15, Batch 1200/1771] Loss: 2.2732, Avg Loss: 2.2817\n\n[Epoch 15, Batch 1250/1771] Loss: 2.3409, Avg Loss: 2.2820\n\n[Epoch 15, Batch 1300/1771] Loss: 2.0195, Avg Loss: 2.2826\n\n[Epoch 15, Batch 1350/1771] Loss: 2.2889, Avg Loss: 2.2828\n\n[Epoch 15, Batch 1400/1771] Loss: 2.2002, Avg Loss: 2.2828\n\n[Epoch 15, Batch 1450/1771] Loss: 2.4287, Avg Loss: 2.2826\n\n[Epoch 15, Batch 1500/1771] Loss: 2.3201, Avg Loss: 2.2824\n\n[Epoch 15, Batch 1550/1771] Loss: 2.2962, Avg Loss: 2.2828\n\n[Epoch 15, Batch 1600/1771] Loss: 2.3648, Avg Loss: 2.2828\n\n[Epoch 15, Batch 1650/1771] Loss: 2.0471, Avg Loss: 2.2833\n\n[Epoch 15, Batch 1700/1771] Loss: 2.3226, Avg Loss: 2.2838\n\n[Epoch 15, Batch 1750/1771] Loss: 2.1879, Avg Loss: 2.2841\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 16/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 16, Batch 50/1771] Loss: 2.2626, Avg Loss: 2.3032\n\n[Epoch 16, Batch 100/1771] Loss: 2.1141, Avg Loss: 2.2761\n\n[Epoch 16, Batch 150/1771] Loss: 2.3541, Avg Loss: 2.2741\n\n[Epoch 16, Batch 200/1771] Loss: 2.4546, Avg Loss: 2.2720\n\n[Epoch 16, Batch 250/1771] Loss: 2.6009, Avg Loss: 2.2764\n\n[Epoch 16, Batch 300/1771] Loss: 2.1928, Avg Loss: 2.2792\n\n[Epoch 16, Batch 350/1771] Loss: 2.0682, Avg Loss: 2.2770\n\n[Epoch 16, Batch 400/1771] Loss: 2.2753, Avg Loss: 2.2756\n\n[Epoch 16, Batch 450/1771] Loss: 2.2690, Avg Loss: 2.2718\n\n[Epoch 16, Batch 500/1771] Loss: 2.3570, Avg Loss: 2.2715\n\n[Epoch 16, Batch 550/1771] Loss: 2.3027, Avg Loss: 2.2694\n\n[Epoch 16, Batch 600/1771] Loss: 2.4519, Avg Loss: 2.2695\n\n[Epoch 16, Batch 650/1771] Loss: 2.2429, Avg Loss: 2.2676\n\n[Epoch 16, Batch 700/1771] Loss: 2.3094, Avg Loss: 2.2691\n\n[Epoch 16, Batch 750/1771] Loss: 2.4335, Avg Loss: 2.2692\n\n[Epoch 16, Batch 800/1771] Loss: 2.3802, Avg Loss: 2.2701\n\n[Epoch 16, Batch 850/1771] Loss: 2.3156, Avg Loss: 2.2705\n\n[Epoch 16, Batch 900/1771] Loss: 2.1898, Avg Loss: 2.2687\n\n[Epoch 16, Batch 950/1771] Loss: 2.2916, Avg Loss: 2.2699\n\n[Epoch 16, Batch 1000/1771] Loss: 2.1049, Avg Loss: 2.2694\n\n[Epoch 16, Batch 1050/1771] Loss: 2.4292, Avg Loss: 2.2705\n\n[Epoch 16, Batch 1100/1771] Loss: 2.3485, Avg Loss: 2.2723\n\n[Epoch 16, Batch 1150/1771] Loss: 2.2206, Avg Loss: 2.2718\n\n[Epoch 16, Batch 1200/1771] Loss: 2.1490, Avg Loss: 2.2711\n\n[Epoch 16, Batch 1250/1771] Loss: 2.3553, Avg Loss: 2.2699\n\n[Epoch 16, Batch 1300/1771] Loss: 2.4092, Avg Loss: 2.2702\n\n[Epoch 16, Batch 1350/1771] Loss: 2.1107, Avg Loss: 2.2700\n\n[Epoch 16, Batch 1400/1771] Loss: 2.3282, Avg Loss: 2.2696\n\n[Epoch 16, Batch 1450/1771] Loss: 2.4194, Avg Loss: 2.2682\n\n[Epoch 16, Batch 1500/1771] Loss: 2.0626, Avg Loss: 2.2683\n\n[Epoch 16, Batch 1550/1771] Loss: 2.2840, Avg Loss: 2.2682\n\n[Epoch 16, Batch 1600/1771] Loss: 2.3022, Avg Loss: 2.2680\n\n[Epoch 16, Batch 1650/1771] Loss: 2.2079, Avg Loss: 2.2676\n\n[Epoch 16, Batch 1700/1771] Loss: 2.3809, Avg Loss: 2.2675\n\n[Epoch 16, Batch 1750/1771] Loss: 2.2928, Avg Loss: 2.2672\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 17/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 17, Batch 50/1771] Loss: 2.3066, Avg Loss: 2.2456\n\n[Epoch 17, Batch 100/1771] Loss: 2.2147, Avg Loss: 2.2438\n\n[Epoch 17, Batch 150/1771] Loss: 2.3120, Avg Loss: 2.2500\n\n[Epoch 17, Batch 200/1771] Loss: 2.0291, Avg Loss: 2.2486\n\n[Epoch 17, Batch 250/1771] Loss: 2.2093, Avg Loss: 2.2496\n\n[Epoch 17, Batch 300/1771] Loss: 2.1697, Avg Loss: 2.2458\n\n[Epoch 17, Batch 350/1771] Loss: 2.3185, Avg Loss: 2.2479\n\n[Epoch 17, Batch 400/1771] Loss: 2.3030, Avg Loss: 2.2519\n\n[Epoch 17, Batch 450/1771] Loss: 2.2970, Avg Loss: 2.2527\n\n[Epoch 17, Batch 500/1771] Loss: 2.3704, Avg Loss: 2.2573\n\n[Epoch 17, Batch 550/1771] Loss: 2.4236, Avg Loss: 2.2597\n\n[Epoch 17, Batch 600/1771] Loss: 2.0919, Avg Loss: 2.2595\n\n[Epoch 17, Batch 650/1771] Loss: 2.2593, Avg Loss: 2.2611\n\n[Epoch 17, Batch 700/1771] Loss: 2.2046, Avg Loss: 2.2604\n\n[Epoch 17, Batch 750/1771] Loss: 2.3928, Avg Loss: 2.2612\n\n[Epoch 17, Batch 800/1771] Loss: 2.3238, Avg Loss: 2.2615\n\n[Epoch 17, Batch 850/1771] Loss: 2.2204, Avg Loss: 2.2612\n\n[Epoch 17, Batch 900/1771] Loss: 2.3047, Avg Loss: 2.2617\n\n[Epoch 17, Batch 950/1771] Loss: 2.3385, Avg Loss: 2.2616\n\n[Epoch 17, Batch 1000/1771] Loss: 2.1797, Avg Loss: 2.2595\n\n[Epoch 17, Batch 1050/1771] Loss: 2.1768, Avg Loss: 2.2593\n\n[Epoch 17, Batch 1100/1771] Loss: 2.2814, Avg Loss: 2.2593\n\n[Epoch 17, Batch 1150/1771] Loss: 2.2255, Avg Loss: 2.2596\n\n[Epoch 17, Batch 1200/1771] Loss: 2.1834, Avg Loss: 2.2582\n\n[Epoch 17, Batch 1250/1771] Loss: 2.1286, Avg Loss: 2.2568\n\n[Epoch 17, Batch 1300/1771] Loss: 2.4730, Avg Loss: 2.2572\n\n[Epoch 17, Batch 1350/1771] Loss: 2.1528, Avg Loss: 2.2576\n\n[Epoch 17, Batch 1400/1771] Loss: 2.0901, Avg Loss: 2.2584\n\n[Epoch 17, Batch 1450/1771] Loss: 2.1356, Avg Loss: 2.2586\n\n[Epoch 17, Batch 1500/1771] Loss: 2.2078, Avg Loss: 2.2579\n\n[Epoch 17, Batch 1550/1771] Loss: 2.0350, Avg Loss: 2.2573\n\n[Epoch 17, Batch 1600/1771] Loss: 2.0979, Avg Loss: 2.2572\n\n[Epoch 17, Batch 1650/1771] Loss: 2.0741, Avg Loss: 2.2567\n\n[Epoch 17, Batch 1700/1771] Loss: 2.0999, Avg Loss: 2.2563\n\n[Epoch 17, Batch 1750/1771] Loss: 2.2055, Avg Loss: 2.2553\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 18/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 18, Batch 50/1771] Loss: 2.2987, Avg Loss: 2.2571\n\n[Epoch 18, Batch 100/1771] Loss: 2.2432, Avg Loss: 2.2498\n\n[Epoch 18, Batch 150/1771] Loss: 2.1955, Avg Loss: 2.2530\n\n[Epoch 18, Batch 200/1771] Loss: 1.9804, Avg Loss: 2.2478\n\n[Epoch 18, Batch 250/1771] Loss: 2.1048, Avg Loss: 2.2479\n\n[Epoch 18, Batch 300/1771] Loss: 2.0937, Avg Loss: 2.2508\n\n[Epoch 18, Batch 350/1771] Loss: 2.1957, Avg Loss: 2.2497\n\n[Epoch 18, Batch 400/1771] Loss: 2.3405, Avg Loss: 2.2536\n\n[Epoch 18, Batch 450/1771] Loss: 2.3609, Avg Loss: 2.2494\n\n[Epoch 18, Batch 500/1771] Loss: 2.3201, Avg Loss: 2.2469\n\n[Epoch 18, Batch 550/1771] Loss: 2.4236, Avg Loss: 2.2449\n\n[Epoch 18, Batch 600/1771] Loss: 2.4742, Avg Loss: 2.2460\n\n[Epoch 18, Batch 650/1771] Loss: 2.1872, Avg Loss: 2.2451\n\n[Epoch 18, Batch 700/1771] Loss: 2.2477, Avg Loss: 2.2461\n\n[Epoch 18, Batch 750/1771] Loss: 2.0798, Avg Loss: 2.2452\n\n[Epoch 18, Batch 800/1771] Loss: 2.2444, Avg Loss: 2.2461\n\n[Epoch 18, Batch 850/1771] Loss: 2.1383, Avg Loss: 2.2458\n\n[Epoch 18, Batch 900/1771] Loss: 2.1523, Avg Loss: 2.2456\n\n[Epoch 18, Batch 950/1771] Loss: 2.3471, Avg Loss: 2.2461\n\n[Epoch 18, Batch 1000/1771] Loss: 2.2376, Avg Loss: 2.2465\n\n[Epoch 18, Batch 1050/1771] Loss: 2.2854, Avg Loss: 2.2472\n\n[Epoch 18, Batch 1100/1771] Loss: 2.3164, Avg Loss: 2.2475\n\n[Epoch 18, Batch 1150/1771] Loss: 2.2173, Avg Loss: 2.2464\n\n[Epoch 18, Batch 1200/1771] Loss: 2.2486, Avg Loss: 2.2462\n\n[Epoch 18, Batch 1250/1771] Loss: 2.0955, Avg Loss: 2.2451\n\n[Epoch 18, Batch 1300/1771] Loss: 2.0747, Avg Loss: 2.2451\n\n[Epoch 18, Batch 1350/1771] Loss: 2.2839, Avg Loss: 2.2442\n\n[Epoch 18, Batch 1400/1771] Loss: 2.1084, Avg Loss: 2.2445\n\n[Epoch 18, Batch 1450/1771] Loss: 2.3364, Avg Loss: 2.2439\n\n[Epoch 18, Batch 1500/1771] Loss: 2.3513, Avg Loss: 2.2442\n\n[Epoch 18, Batch 1550/1771] Loss: 2.0886, Avg Loss: 2.2438\n\n[Epoch 18, Batch 1600/1771] Loss: 2.4508, Avg Loss: 2.2450\n\n[Epoch 18, Batch 1650/1771] Loss: 2.3572, Avg Loss: 2.2446\n\n[Epoch 18, Batch 1700/1771] Loss: 2.2032, Avg Loss: 2.2445\n\n[Epoch 18, Batch 1750/1771] Loss: 2.0283, Avg Loss: 2.2451\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 19/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 19, Batch 50/1771] Loss: 2.2459, Avg Loss: 2.2557\n\n[Epoch 19, Batch 100/1771] Loss: 2.0568, Avg Loss: 2.2489\n\n[Epoch 19, Batch 150/1771] Loss: 2.1107, Avg Loss: 2.2424\n\n[Epoch 19, Batch 200/1771] Loss: 2.1970, Avg Loss: 2.2446\n\n[Epoch 19, Batch 250/1771] Loss: 2.2022, Avg Loss: 2.2404\n\n[Epoch 19, Batch 300/1771] Loss: 2.1798, Avg Loss: 2.2391\n\n[Epoch 19, Batch 350/1771] Loss: 2.3785, Avg Loss: 2.2377\n\n[Epoch 19, Batch 400/1771] Loss: 2.3732, Avg Loss: 2.2373\n\n[Epoch 19, Batch 450/1771] Loss: 2.3735, Avg Loss: 2.2359\n\n[Epoch 19, Batch 500/1771] Loss: 2.3216, Avg Loss: 2.2359\n\n[Epoch 19, Batch 550/1771] Loss: 2.2121, Avg Loss: 2.2352\n\n[Epoch 19, Batch 600/1771] Loss: 2.3939, Avg Loss: 2.2339\n\n[Epoch 19, Batch 650/1771] Loss: 1.9395, Avg Loss: 2.2331\n\n[Epoch 19, Batch 700/1771] Loss: 2.4352, Avg Loss: 2.2324\n\n[Epoch 19, Batch 750/1771] Loss: 2.1220, Avg Loss: 2.2300\n\n[Epoch 19, Batch 800/1771] Loss: 2.2945, Avg Loss: 2.2315\n\n[Epoch 19, Batch 850/1771] Loss: 2.0436, Avg Loss: 2.2326\n\n[Epoch 19, Batch 900/1771] Loss: 2.1738, Avg Loss: 2.2326\n\n[Epoch 19, Batch 950/1771] Loss: 2.1222, Avg Loss: 2.2333\n\n[Epoch 19, Batch 1000/1771] Loss: 2.0976, Avg Loss: 2.2338\n\n[Epoch 19, Batch 1050/1771] Loss: 2.1960, Avg Loss: 2.2338\n\n[Epoch 19, Batch 1100/1771] Loss: 2.1624, Avg Loss: 2.2348\n\n[Epoch 19, Batch 1150/1771] Loss: 2.1992, Avg Loss: 2.2358\n\n[Epoch 19, Batch 1200/1771] Loss: 2.1143, Avg Loss: 2.2354\n\n[Epoch 19, Batch 1250/1771] Loss: 2.2068, Avg Loss: 2.2358\n\n[Epoch 19, Batch 1300/1771] Loss: 2.1597, Avg Loss: 2.2355\n\n[Epoch 19, Batch 1350/1771] Loss: 2.2932, Avg Loss: 2.2369\n\n[Epoch 19, Batch 1400/1771] Loss: 1.9595, Avg Loss: 2.2365\n\n[Epoch 19, Batch 1450/1771] Loss: 2.4111, Avg Loss: 2.2368\n\n[Epoch 19, Batch 1500/1771] Loss: 2.3713, Avg Loss: 2.2367\n\n[Epoch 19, Batch 1550/1771] Loss: 2.4508, Avg Loss: 2.2365\n\n[Epoch 19, Batch 1600/1771] Loss: 2.2270, Avg Loss: 2.2377\n\n[Epoch 19, Batch 1650/1771] Loss: 2.2987, Avg Loss: 2.2375\n\n[Epoch 19, Batch 1700/1771] Loss: 2.3190, Avg Loss: 2.2380\n\n[Epoch 19, Batch 1750/1771] Loss: 2.2464, Avg Loss: 2.2368\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 20/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 20, Batch 50/1771] Loss: 2.4386, Avg Loss: 2.2326\n\n[Epoch 20, Batch 100/1771] Loss: 2.4213, Avg Loss: 2.2394\n\n[Epoch 20, Batch 150/1771] Loss: 2.3698, Avg Loss: 2.2387\n\n[Epoch 20, Batch 200/1771] Loss: 2.3341, Avg Loss: 2.2409\n\n[Epoch 20, Batch 250/1771] Loss: 2.0126, Avg Loss: 2.2430\n\n[Epoch 20, Batch 300/1771] Loss: 2.1382, Avg Loss: 2.2391\n\n[Epoch 20, Batch 350/1771] Loss: 2.2646, Avg Loss: 2.2411\n\n[Epoch 20, Batch 400/1771] Loss: 2.1966, Avg Loss: 2.2364\n\n[Epoch 20, Batch 450/1771] Loss: 2.1864, Avg Loss: 2.2325\n\n[Epoch 20, Batch 500/1771] Loss: 2.4007, Avg Loss: 2.2294\n\n[Epoch 20, Batch 550/1771] Loss: 2.1404, Avg Loss: 2.2308\n\n[Epoch 20, Batch 600/1771] Loss: 2.3288, Avg Loss: 2.2304\n\n[Epoch 20, Batch 650/1771] Loss: 2.3069, Avg Loss: 2.2289\n\n[Epoch 20, Batch 700/1771] Loss: 2.1570, Avg Loss: 2.2285\n\n[Epoch 20, Batch 750/1771] Loss: 2.2777, Avg Loss: 2.2282\n\n[Epoch 20, Batch 800/1771] Loss: 2.1993, Avg Loss: 2.2276\n\n[Epoch 20, Batch 850/1771] Loss: 2.2349, Avg Loss: 2.2273\n\n[Epoch 20, Batch 900/1771] Loss: 2.3099, Avg Loss: 2.2273\n\n[Epoch 20, Batch 950/1771] Loss: 2.2072, Avg Loss: 2.2282\n\n[Epoch 20, Batch 1000/1771] Loss: 2.3756, Avg Loss: 2.2289\n\n[Epoch 20, Batch 1050/1771] Loss: 2.1198, Avg Loss: 2.2284\n\n[Epoch 20, Batch 1100/1771] Loss: 2.2039, Avg Loss: 2.2295\n\n[Epoch 20, Batch 1150/1771] Loss: 2.2572, Avg Loss: 2.2294\n\n[Epoch 20, Batch 1200/1771] Loss: 2.0316, Avg Loss: 2.2299\n\n[Epoch 20, Batch 1250/1771] Loss: 2.1266, Avg Loss: 2.2301\n\n[Epoch 20, Batch 1300/1771] Loss: 2.1007, Avg Loss: 2.2307\n\n[Epoch 20, Batch 1350/1771] Loss: 2.1637, Avg Loss: 2.2312\n\n[Epoch 20, Batch 1400/1771] Loss: 2.1504, Avg Loss: 2.2303\n\n[Epoch 20, Batch 1450/1771] Loss: 2.2441, Avg Loss: 2.2297\n\n[Epoch 20, Batch 1500/1771] Loss: 2.1703, Avg Loss: 2.2296\n\n[Epoch 20, Batch 1550/1771] Loss: 2.1825, Avg Loss: 2.2298\n\n[Epoch 20, Batch 1600/1771] Loss: 2.0609, Avg Loss: 2.2298\n\n[Epoch 20, Batch 1650/1771] Loss: 2.2043, Avg Loss: 2.2297\n\n[Epoch 20, Batch 1700/1771] Loss: 2.1589, Avg Loss: 2.2300\n\n[Epoch 20, Batch 1750/1771] Loss: 2.0575, Avg Loss: 2.2303\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 21/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 21, Batch 50/1771] Loss: 2.1483, Avg Loss: 2.2447\n\n[Epoch 21, Batch 100/1771] Loss: 2.3169, Avg Loss: 2.2350\n\n[Epoch 21, Batch 150/1771] Loss: 2.2553, Avg Loss: 2.2365\n\n[Epoch 21, Batch 200/1771] Loss: 1.9893, Avg Loss: 2.2373\n\n[Epoch 21, Batch 250/1771] Loss: 2.2044, Avg Loss: 2.2341\n\n[Epoch 21, Batch 300/1771] Loss: 2.3283, Avg Loss: 2.2299\n\n[Epoch 21, Batch 350/1771] Loss: 2.1494, Avg Loss: 2.2297\n\n[Epoch 21, Batch 400/1771] Loss: 2.4839, Avg Loss: 2.2343\n\n[Epoch 21, Batch 450/1771] Loss: 2.2500, Avg Loss: 2.2327\n\n[Epoch 21, Batch 500/1771] Loss: 2.0954, Avg Loss: 2.2345\n\n[Epoch 21, Batch 550/1771] Loss: 2.2295, Avg Loss: 2.2334\n\n[Epoch 21, Batch 600/1771] Loss: 1.9832, Avg Loss: 2.2317\n\n[Epoch 21, Batch 650/1771] Loss: 2.1471, Avg Loss: 2.2299\n\n[Epoch 21, Batch 700/1771] Loss: 2.3039, Avg Loss: 2.2283\n\n[Epoch 21, Batch 750/1771] Loss: 2.3130, Avg Loss: 2.2280\n\n[Epoch 21, Batch 800/1771] Loss: 2.4995, Avg Loss: 2.2283\n\n[Epoch 21, Batch 850/1771] Loss: 2.2443, Avg Loss: 2.2303\n\n[Epoch 21, Batch 900/1771] Loss: 2.1820, Avg Loss: 2.2292\n\n[Epoch 21, Batch 950/1771] Loss: 1.9894, Avg Loss: 2.2290\n\n[Epoch 21, Batch 1000/1771] Loss: 2.2807, Avg Loss: 2.2285\n\n[Epoch 21, Batch 1050/1771] Loss: 2.0266, Avg Loss: 2.2286\n\n[Epoch 21, Batch 1100/1771] Loss: 2.4232, Avg Loss: 2.2276\n\n[Epoch 21, Batch 1150/1771] Loss: 2.2623, Avg Loss: 2.2277\n\n[Epoch 21, Batch 1200/1771] Loss: 2.0055, Avg Loss: 2.2274\n\n[Epoch 21, Batch 1250/1771] Loss: 2.3800, Avg Loss: 2.2274\n\n[Epoch 21, Batch 1300/1771] Loss: 2.2897, Avg Loss: 2.2278\n\n[Epoch 21, Batch 1350/1771] Loss: 2.3579, Avg Loss: 2.2287\n\n[Epoch 21, Batch 1400/1771] Loss: 2.3993, Avg Loss: 2.2284\n\n[Epoch 21, Batch 1450/1771] Loss: 2.2008, Avg Loss: 2.2279\n\n[Epoch 21, Batch 1500/1771] Loss: 2.2653, Avg Loss: 2.2292\n\n[Epoch 21, Batch 1550/1771] Loss: 2.2936, Avg Loss: 2.2296\n\n[Epoch 21, Batch 1600/1771] Loss: 2.1623, Avg Loss: 2.2291\n\n[Epoch 21, Batch 1650/1771] Loss: 2.1540, Avg Loss: 2.2287\n\n[Epoch 21, Batch 1700/1771] Loss: 2.2870, Avg Loss: 2.2290\n\n[Epoch 21, Batch 1750/1771] Loss: 2.1587, Avg Loss: 2.2283\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 22/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 22, Batch 50/1771] Loss: 2.3383, Avg Loss: 2.1988\n\n[Epoch 22, Batch 100/1771] Loss: 2.3009, Avg Loss: 2.2068\n\n[Epoch 22, Batch 150/1771] Loss: 2.1983, Avg Loss: 2.2099\n\n[Epoch 22, Batch 200/1771] Loss: 2.2795, Avg Loss: 2.2064\n\n[Epoch 22, Batch 250/1771] Loss: 2.1655, Avg Loss: 2.2126\n\n[Epoch 22, Batch 300/1771] Loss: 2.2374, Avg Loss: 2.2150\n\n[Epoch 22, Batch 350/1771] Loss: 2.2273, Avg Loss: 2.2147\n\n[Epoch 22, Batch 400/1771] Loss: 2.1338, Avg Loss: 2.2128\n\n[Epoch 22, Batch 450/1771] Loss: 2.1415, Avg Loss: 2.2121\n\n[Epoch 22, Batch 500/1771] Loss: 2.1998, Avg Loss: 2.2109\n\n[Epoch 22, Batch 550/1771] Loss: 2.1570, Avg Loss: 2.2106\n\n[Epoch 22, Batch 600/1771] Loss: 2.0281, Avg Loss: 2.2108\n\n[Epoch 22, Batch 650/1771] Loss: 2.2672, Avg Loss: 2.2116\n\n[Epoch 22, Batch 700/1771] Loss: 2.3106, Avg Loss: 2.2125\n\n[Epoch 22, Batch 750/1771] Loss: 2.2275, Avg Loss: 2.2136\n\n[Epoch 22, Batch 800/1771] Loss: 2.2791, Avg Loss: 2.2158\n\n[Epoch 22, Batch 850/1771] Loss: 2.2240, Avg Loss: 2.2161\n\n[Epoch 22, Batch 900/1771] Loss: 2.1002, Avg Loss: 2.2163\n\n[Epoch 22, Batch 950/1771] Loss: 2.0859, Avg Loss: 2.2168\n\n[Epoch 22, Batch 1000/1771] Loss: 2.4131, Avg Loss: 2.2182\n\n[Epoch 22, Batch 1050/1771] Loss: 2.2154, Avg Loss: 2.2206\n\n[Epoch 22, Batch 1100/1771] Loss: 2.0682, Avg Loss: 2.2205\n\n[Epoch 22, Batch 1150/1771] Loss: 2.2652, Avg Loss: 2.2202\n\n[Epoch 22, Batch 1200/1771] Loss: 2.1084, Avg Loss: 2.2189\n\n[Epoch 22, Batch 1250/1771] Loss: 2.3424, Avg Loss: 2.2189\n\n[Epoch 22, Batch 1300/1771] Loss: 2.0035, Avg Loss: 2.2185\n\n[Epoch 22, Batch 1350/1771] Loss: 2.0862, Avg Loss: 2.2183\n\n[Epoch 22, Batch 1400/1771] Loss: 2.3111, Avg Loss: 2.2195\n\n[Epoch 22, Batch 1450/1771] Loss: 2.5288, Avg Loss: 2.2199\n\n[Epoch 22, Batch 1500/1771] Loss: 2.0477, Avg Loss: 2.2197\n\n[Epoch 22, Batch 1550/1771] Loss: 2.0977, Avg Loss: 2.2181\n\n[Epoch 22, Batch 1600/1771] Loss: 2.0308, Avg Loss: 2.2178\n\n[Epoch 22, Batch 1650/1771] Loss: 2.2409, Avg Loss: 2.2178\n\n[Epoch 22, Batch 1700/1771] Loss: 2.2962, Avg Loss: 2.2185\n\n[Epoch 22, Batch 1750/1771] Loss: 2.3428, Avg Loss: 2.2189\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 23/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 23, Batch 50/1771] Loss: 2.0700, Avg Loss: 2.1955\n\n[Epoch 23, Batch 100/1771] Loss: 2.3704, Avg Loss: 2.2103\n\n[Epoch 23, Batch 150/1771] Loss: 2.4058, Avg Loss: 2.2064\n\n[Epoch 23, Batch 200/1771] Loss: 2.0665, Avg Loss: 2.2019\n\n[Epoch 23, Batch 250/1771] Loss: 2.2772, Avg Loss: 2.2031\n\n[Epoch 23, Batch 300/1771] Loss: 2.2272, Avg Loss: 2.2095\n\n[Epoch 23, Batch 350/1771] Loss: 2.2672, Avg Loss: 2.2105\n\n[Epoch 23, Batch 400/1771] Loss: 2.2161, Avg Loss: 2.2140\n\n[Epoch 23, Batch 450/1771] Loss: 2.2200, Avg Loss: 2.2130\n\n[Epoch 23, Batch 500/1771] Loss: 2.0227, Avg Loss: 2.2130\n\n[Epoch 23, Batch 550/1771] Loss: 2.3945, Avg Loss: 2.2139\n\n[Epoch 23, Batch 600/1771] Loss: 2.4308, Avg Loss: 2.2138\n\n[Epoch 23, Batch 650/1771] Loss: 2.1818, Avg Loss: 2.2136\n\n[Epoch 23, Batch 700/1771] Loss: 2.2749, Avg Loss: 2.2116\n\n[Epoch 23, Batch 750/1771] Loss: 2.1454, Avg Loss: 2.2126\n\n[Epoch 23, Batch 800/1771] Loss: 2.1269, Avg Loss: 2.2140\n\n[Epoch 23, Batch 850/1771] Loss: 2.1839, Avg Loss: 2.2123\n\n[Epoch 23, Batch 900/1771] Loss: 2.0584, Avg Loss: 2.2131\n\n[Epoch 23, Batch 950/1771] Loss: 2.2919, Avg Loss: 2.2129\n\n[Epoch 23, Batch 1000/1771] Loss: 2.2039, Avg Loss: 2.2131\n\n[Epoch 23, Batch 1050/1771] Loss: 2.3041, Avg Loss: 2.2136\n\n[Epoch 23, Batch 1100/1771] Loss: 2.0341, Avg Loss: 2.2133\n\n[Epoch 23, Batch 1150/1771] Loss: 2.0992, Avg Loss: 2.2121\n\n[Epoch 23, Batch 1200/1771] Loss: 2.3481, Avg Loss: 2.2132\n\n[Epoch 23, Batch 1250/1771] Loss: 2.2000, Avg Loss: 2.2127\n\n[Epoch 23, Batch 1300/1771] Loss: 2.3200, Avg Loss: 2.2134\n\n[Epoch 23, Batch 1350/1771] Loss: 2.0616, Avg Loss: 2.2139\n\n[Epoch 23, Batch 1400/1771] Loss: 2.1403, Avg Loss: 2.2144\n\n[Epoch 23, Batch 1450/1771] Loss: 2.4430, Avg Loss: 2.2147\n\n[Epoch 23, Batch 1500/1771] Loss: 2.1918, Avg Loss: 2.2144\n\n[Epoch 23, Batch 1550/1771] Loss: 2.3710, Avg Loss: 2.2147\n\n[Epoch 23, Batch 1600/1771] Loss: 2.1148, Avg Loss: 2.2137\n\n[Epoch 23, Batch 1650/1771] Loss: 2.3196, Avg Loss: 2.2137\n\n[Epoch 23, Batch 1700/1771] Loss: 1.9893, Avg Loss: 2.2134\n\n[Epoch 23, Batch 1750/1771] Loss: 2.2154, Avg Loss: 2.2134\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 24/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 24, Batch 50/1771] Loss: 2.1628, Avg Loss: 2.1947\n\n[Epoch 24, Batch 100/1771] Loss: 2.1978, Avg Loss: 2.1825\n\n[Epoch 24, Batch 150/1771] Loss: 2.3507, Avg Loss: 2.1946\n\n[Epoch 24, Batch 200/1771] Loss: 2.2716, Avg Loss: 2.1984\n\n[Epoch 24, Batch 250/1771] Loss: 2.1988, Avg Loss: 2.2034\n\n[Epoch 24, Batch 300/1771] Loss: 2.2827, Avg Loss: 2.2057\n\n[Epoch 24, Batch 350/1771] Loss: 2.0702, Avg Loss: 2.2042\n\n[Epoch 24, Batch 400/1771] Loss: 2.3365, Avg Loss: 2.2057\n\n[Epoch 24, Batch 450/1771] Loss: 2.1327, Avg Loss: 2.2049\n\n[Epoch 24, Batch 500/1771] Loss: 2.1842, Avg Loss: 2.2061\n\n[Epoch 24, Batch 550/1771] Loss: 2.2300, Avg Loss: 2.2054\n\n[Epoch 24, Batch 600/1771] Loss: 2.0871, Avg Loss: 2.2064\n\n[Epoch 24, Batch 650/1771] Loss: 2.5161, Avg Loss: 2.2046\n\n[Epoch 24, Batch 700/1771] Loss: 1.9792, Avg Loss: 2.2045\n\n[Epoch 24, Batch 750/1771] Loss: 2.3551, Avg Loss: 2.2039\n\n[Epoch 24, Batch 800/1771] Loss: 2.2051, Avg Loss: 2.2053\n\n[Epoch 24, Batch 850/1771] Loss: 2.0663, Avg Loss: 2.2055\n\n[Epoch 24, Batch 900/1771] Loss: 2.4248, Avg Loss: 2.2060\n\n[Epoch 24, Batch 950/1771] Loss: 2.3858, Avg Loss: 2.2075\n\n[Epoch 24, Batch 1000/1771] Loss: 2.2467, Avg Loss: 2.2073\n\n[Epoch 24, Batch 1050/1771] Loss: 2.0759, Avg Loss: 2.2076\n\n[Epoch 24, Batch 1100/1771] Loss: 2.0603, Avg Loss: 2.2076\n\n[Epoch 24, Batch 1150/1771] Loss: 2.3825, Avg Loss: 2.2078\n\n[Epoch 24, Batch 1200/1771] Loss: 2.2676, Avg Loss: 2.2088\n\n[Epoch 24, Batch 1250/1771] Loss: 2.1391, Avg Loss: 2.2083\n\n[Epoch 24, Batch 1300/1771] Loss: 2.3097, Avg Loss: 2.2092\n\n[Epoch 24, Batch 1350/1771] Loss: 2.1307, Avg Loss: 2.2100\n\n[Epoch 24, Batch 1400/1771] Loss: 2.1961, Avg Loss: 2.2107\n\n[Epoch 24, Batch 1450/1771] Loss: 2.0750, Avg Loss: 2.2103\n\n[Epoch 24, Batch 1500/1771] Loss: 2.4188, Avg Loss: 2.2103\n\n[Epoch 24, Batch 1550/1771] Loss: 2.0024, Avg Loss: 2.2107\n\n[Epoch 24, Batch 1600/1771] Loss: 2.5329, Avg Loss: 2.2106\n\n[Epoch 24, Batch 1650/1771] Loss: 2.2173, Avg Loss: 2.2107\n\n[Epoch 24, Batch 1700/1771] Loss: 1.8839, Avg Loss: 2.2111\n\n[Epoch 24, Batch 1750/1771] Loss: 2.2600, Avg Loss: 2.2111\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 25/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 25, Batch 50/1771] Loss: 2.1293, Avg Loss: 2.2099\n\n[Epoch 25, Batch 100/1771] Loss: 2.2535, Avg Loss: 2.2036\n\n[Epoch 25, Batch 150/1771] Loss: 2.2617, Avg Loss: 2.2076\n\n[Epoch 25, Batch 200/1771] Loss: 2.2543, Avg Loss: 2.2080\n\n[Epoch 25, Batch 250/1771] Loss: 2.2151, Avg Loss: 2.2079\n\n[Epoch 25, Batch 300/1771] Loss: 2.0802, Avg Loss: 2.2087\n\n[Epoch 25, Batch 350/1771] Loss: 2.0270, Avg Loss: 2.2049\n\n[Epoch 25, Batch 400/1771] Loss: 2.2389, Avg Loss: 2.2084\n\n[Epoch 25, Batch 450/1771] Loss: 2.2996, Avg Loss: 2.2096\n\n[Epoch 25, Batch 500/1771] Loss: 2.1100, Avg Loss: 2.2064\n\n[Epoch 25, Batch 550/1771] Loss: 2.2677, Avg Loss: 2.2067\n\n[Epoch 25, Batch 600/1771] Loss: 2.3651, Avg Loss: 2.2055\n\n[Epoch 25, Batch 650/1771] Loss: 2.2274, Avg Loss: 2.2051\n\n[Epoch 25, Batch 700/1771] Loss: 2.2970, Avg Loss: 2.2044\n\n[Epoch 25, Batch 750/1771] Loss: 2.0629, Avg Loss: 2.2048\n\n[Epoch 25, Batch 800/1771] Loss: 2.2239, Avg Loss: 2.2051\n\n[Epoch 25, Batch 850/1771] Loss: 2.2790, Avg Loss: 2.2070\n\n[Epoch 25, Batch 900/1771] Loss: 2.3776, Avg Loss: 2.2078\n\n[Epoch 25, Batch 950/1771] Loss: 2.2612, Avg Loss: 2.2086\n\n[Epoch 25, Batch 1000/1771] Loss: 2.1092, Avg Loss: 2.2072\n\n[Epoch 25, Batch 1050/1771] Loss: 1.9544, Avg Loss: 2.2068\n\n[Epoch 25, Batch 1100/1771] Loss: 2.1139, Avg Loss: 2.2062\n\n[Epoch 25, Batch 1150/1771] Loss: 2.0681, Avg Loss: 2.2061\n\n[Epoch 25, Batch 1200/1771] Loss: 2.1842, Avg Loss: 2.2066\n\n[Epoch 25, Batch 1250/1771] Loss: 2.2939, Avg Loss: 2.2066\n\n[Epoch 25, Batch 1300/1771] Loss: 2.1655, Avg Loss: 2.2072\n\n[Epoch 25, Batch 1350/1771] Loss: 2.0669, Avg Loss: 2.2063\n\n[Epoch 25, Batch 1400/1771] Loss: 2.3743, Avg Loss: 2.2059\n\n[Epoch 25, Batch 1450/1771] Loss: 2.1647, Avg Loss: 2.2060\n\n[Epoch 25, Batch 1500/1771] Loss: 2.1579, Avg Loss: 2.2063\n\n[Epoch 25, Batch 1550/1771] Loss: 2.2444, Avg Loss: 2.2067\n\n[Epoch 25, Batch 1600/1771] Loss: 2.3151, Avg Loss: 2.2070\n\n[Epoch 25, Batch 1650/1771] Loss: 2.3446, Avg Loss: 2.2056\n\n[Epoch 25, Batch 1700/1771] Loss: 2.1480, Avg Loss: 2.2058\n\n[Epoch 25, Batch 1750/1771] Loss: 2.1013, Avg Loss: 2.2052\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 26/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 26, Batch 50/1771] Loss: 2.1508, Avg Loss: 2.1824\n\n[Epoch 26, Batch 100/1771] Loss: 2.2714, Avg Loss: 2.1924\n\n[Epoch 26, Batch 150/1771] Loss: 2.2416, Avg Loss: 2.1915\n\n[Epoch 26, Batch 200/1771] Loss: 2.1424, Avg Loss: 2.1883\n\n[Epoch 26, Batch 250/1771] Loss: 2.4590, Avg Loss: 2.1935\n\n[Epoch 26, Batch 300/1771] Loss: 2.0928, Avg Loss: 2.1889\n\n[Epoch 26, Batch 350/1771] Loss: 2.0725, Avg Loss: 2.1909\n\n[Epoch 26, Batch 400/1771] Loss: 2.3279, Avg Loss: 2.1925\n\n[Epoch 26, Batch 450/1771] Loss: 2.2121, Avg Loss: 2.1933\n\n[Epoch 26, Batch 500/1771] Loss: 2.1343, Avg Loss: 2.1909\n\n[Epoch 26, Batch 550/1771] Loss: 2.0507, Avg Loss: 2.1922\n\n[Epoch 26, Batch 600/1771] Loss: 2.3887, Avg Loss: 2.1939\n\n[Epoch 26, Batch 650/1771] Loss: 2.2588, Avg Loss: 2.1933\n\n[Epoch 26, Batch 700/1771] Loss: 2.0720, Avg Loss: 2.1961\n\n[Epoch 26, Batch 750/1771] Loss: 2.2305, Avg Loss: 2.1962\n\n[Epoch 26, Batch 800/1771] Loss: 2.1699, Avg Loss: 2.1974\n\n[Epoch 26, Batch 850/1771] Loss: 2.0328, Avg Loss: 2.1974\n\n[Epoch 26, Batch 900/1771] Loss: 2.4589, Avg Loss: 2.2003\n\n[Epoch 26, Batch 950/1771] Loss: 2.5587, Avg Loss: 2.2002\n\n[Epoch 26, Batch 1000/1771] Loss: 2.3184, Avg Loss: 2.2014\n\n[Epoch 26, Batch 1050/1771] Loss: 2.2046, Avg Loss: 2.2008\n\n[Epoch 26, Batch 1100/1771] Loss: 2.1264, Avg Loss: 2.2009\n\n[Epoch 26, Batch 1150/1771] Loss: 2.1577, Avg Loss: 2.2007\n\n[Epoch 26, Batch 1200/1771] Loss: 2.2399, Avg Loss: 2.2013\n\n[Epoch 26, Batch 1250/1771] Loss: 2.0732, Avg Loss: 2.2016\n\n[Epoch 26, Batch 1300/1771] Loss: 2.0736, Avg Loss: 2.2027\n\n[Epoch 26, Batch 1350/1771] Loss: 2.0912, Avg Loss: 2.2029\n\n[Epoch 26, Batch 1400/1771] Loss: 2.3933, Avg Loss: 2.2033\n\n[Epoch 26, Batch 1450/1771] Loss: 2.1120, Avg Loss: 2.2020\n\n[Epoch 26, Batch 1500/1771] Loss: 2.1030, Avg Loss: 2.2021\n\n[Epoch 26, Batch 1550/1771] Loss: 2.2516, Avg Loss: 2.2025\n\n[Epoch 26, Batch 1600/1771] Loss: 2.1711, Avg Loss: 2.2032\n\n[Epoch 26, Batch 1650/1771] Loss: 2.2361, Avg Loss: 2.2032\n\n[Epoch 26, Batch 1700/1771] Loss: 2.1920, Avg Loss: 2.2034\n\n[Epoch 26, Batch 1750/1771] Loss: 2.3808, Avg Loss: 2.2039\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 27/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 27, Batch 50/1771] Loss: 1.9956, Avg Loss: 2.1988\n\n[Epoch 27, Batch 100/1771] Loss: 2.2490, Avg Loss: 2.1987\n\n[Epoch 27, Batch 150/1771] Loss: 2.2409, Avg Loss: 2.1923\n\n[Epoch 27, Batch 200/1771] Loss: 1.8675, Avg Loss: 2.1887\n\n[Epoch 27, Batch 250/1771] Loss: 2.0841, Avg Loss: 2.1908\n\n[Epoch 27, Batch 300/1771] Loss: 2.2791, Avg Loss: 2.1920\n\n[Epoch 27, Batch 350/1771] Loss: 2.2683, Avg Loss: 2.1930\n\n[Epoch 27, Batch 400/1771] Loss: 2.2535, Avg Loss: 2.1951\n\n[Epoch 27, Batch 450/1771] Loss: 2.2106, Avg Loss: 2.1951\n\n[Epoch 27, Batch 500/1771] Loss: 2.0086, Avg Loss: 2.1954\n\n[Epoch 27, Batch 550/1771] Loss: 2.1660, Avg Loss: 2.1946\n\n[Epoch 27, Batch 600/1771] Loss: 2.2971, Avg Loss: 2.1965\n\n[Epoch 27, Batch 650/1771] Loss: 2.2148, Avg Loss: 2.1948\n\n[Epoch 27, Batch 700/1771] Loss: 2.1681, Avg Loss: 2.1955\n\n[Epoch 27, Batch 750/1771] Loss: 1.9127, Avg Loss: 2.1962\n\n[Epoch 27, Batch 800/1771] Loss: 2.1559, Avg Loss: 2.1961\n\n[Epoch 27, Batch 850/1771] Loss: 2.5184, Avg Loss: 2.1969\n\n[Epoch 27, Batch 900/1771] Loss: 2.0748, Avg Loss: 2.1967\n\n[Epoch 27, Batch 950/1771] Loss: 2.2498, Avg Loss: 2.1979\n\n[Epoch 27, Batch 1000/1771] Loss: 2.0983, Avg Loss: 2.1988\n\n[Epoch 27, Batch 1050/1771] Loss: 2.0888, Avg Loss: 2.1983\n\n[Epoch 27, Batch 1100/1771] Loss: 2.0952, Avg Loss: 2.1996\n\n[Epoch 27, Batch 1150/1771] Loss: 2.5584, Avg Loss: 2.2000\n\n[Epoch 27, Batch 1200/1771] Loss: 2.1310, Avg Loss: 2.1990\n\n[Epoch 27, Batch 1250/1771] Loss: 2.3618, Avg Loss: 2.1987\n\n[Epoch 27, Batch 1300/1771] Loss: 2.2021, Avg Loss: 2.1988\n\n[Epoch 27, Batch 1350/1771] Loss: 2.3044, Avg Loss: 2.1994\n\n[Epoch 27, Batch 1400/1771] Loss: 1.9683, Avg Loss: 2.1985\n\n[Epoch 27, Batch 1450/1771] Loss: 1.9172, Avg Loss: 2.1986\n\n[Epoch 27, Batch 1500/1771] Loss: 2.2690, Avg Loss: 2.1984\n\n[Epoch 27, Batch 1550/1771] Loss: 2.2804, Avg Loss: 2.1985\n\n[Epoch 27, Batch 1600/1771] Loss: 2.2177, Avg Loss: 2.1989\n\n[Epoch 27, Batch 1650/1771] Loss: 2.1439, Avg Loss: 2.1990\n\n[Epoch 27, Batch 1700/1771] Loss: 2.2276, Avg Loss: 2.1995\n\n[Epoch 27, Batch 1750/1771] Loss: 2.1980, Avg Loss: 2.1998\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 28/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 28, Batch 50/1771] Loss: 2.1973, Avg Loss: 2.1834\n\n[Epoch 28, Batch 100/1771] Loss: 2.1059, Avg Loss: 2.1975\n\n[Epoch 28, Batch 150/1771] Loss: 2.2427, Avg Loss: 2.1965\n\n[Epoch 28, Batch 200/1771] Loss: 2.1591, Avg Loss: 2.1995\n\n[Epoch 28, Batch 250/1771] Loss: 2.2981, Avg Loss: 2.1952\n\n[Epoch 28, Batch 300/1771] Loss: 2.2859, Avg Loss: 2.1980\n\n[Epoch 28, Batch 350/1771] Loss: 2.1952, Avg Loss: 2.1999\n\n[Epoch 28, Batch 400/1771] Loss: 2.0886, Avg Loss: 2.2037\n\n[Epoch 28, Batch 450/1771] Loss: 2.1251, Avg Loss: 2.2006\n\n[Epoch 28, Batch 500/1771] Loss: 2.4557, Avg Loss: 2.2022\n\n[Epoch 28, Batch 550/1771] Loss: 2.0456, Avg Loss: 2.2012\n\n[Epoch 28, Batch 600/1771] Loss: 2.2082, Avg Loss: 2.2030\n\n[Epoch 28, Batch 650/1771] Loss: 2.3771, Avg Loss: 2.2066\n\n[Epoch 28, Batch 700/1771] Loss: 2.0028, Avg Loss: 2.2078\n\n[Epoch 28, Batch 750/1771] Loss: 2.4223, Avg Loss: 2.2079\n\n[Epoch 28, Batch 800/1771] Loss: 2.2813, Avg Loss: 2.2058\n\n[Epoch 28, Batch 850/1771] Loss: 2.1719, Avg Loss: 2.2051\n\n[Epoch 28, Batch 900/1771] Loss: 2.1263, Avg Loss: 2.2049\n\n[Epoch 28, Batch 950/1771] Loss: 2.1993, Avg Loss: 2.2039\n\n[Epoch 28, Batch 1000/1771] Loss: 2.1939, Avg Loss: 2.2037\n\n[Epoch 28, Batch 1050/1771] Loss: 2.3462, Avg Loss: 2.2032\n\n[Epoch 28, Batch 1100/1771] Loss: 2.4198, Avg Loss: 2.2034\n\n[Epoch 28, Batch 1150/1771] Loss: 2.2104, Avg Loss: 2.2036\n\n[Epoch 28, Batch 1200/1771] Loss: 2.0646, Avg Loss: 2.2037\n\n[Epoch 28, Batch 1250/1771] Loss: 2.3498, Avg Loss: 2.2045\n\n[Epoch 28, Batch 1300/1771] Loss: 2.1150, Avg Loss: 2.2049\n\n[Epoch 28, Batch 1350/1771] Loss: 2.2663, Avg Loss: 2.2047\n\n[Epoch 28, Batch 1400/1771] Loss: 2.2223, Avg Loss: 2.2043\n\n[Epoch 28, Batch 1450/1771] Loss: 2.1859, Avg Loss: 2.2035\n\n[Epoch 28, Batch 1500/1771] Loss: 1.9699, Avg Loss: 2.2032\n\n[Epoch 28, Batch 1550/1771] Loss: 2.4877, Avg Loss: 2.2043\n\n[Epoch 28, Batch 1600/1771] Loss: 1.9765, Avg Loss: 2.2041\n\n[Epoch 28, Batch 1650/1771] Loss: 2.2315, Avg Loss: 2.2044\n\n[Epoch 28, Batch 1700/1771] Loss: 2.3000, Avg Loss: 2.2042\n\n[Epoch 28, Batch 1750/1771] Loss: 2.1060, Avg Loss: 2.2034\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/30:   0%|          | 0/1771 [00:55<?, ?it/s]\n\n\n\nEpoch 29/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 29, Batch 50/1771] Loss: 2.0787, Avg Loss: 2.2053\n\n[Epoch 29, Batch 100/1771] Loss: 2.3391, Avg Loss: 2.2078\n\n[Epoch 29, Batch 150/1771] Loss: 2.0476, Avg Loss: 2.2108\n\n[Epoch 29, Batch 200/1771] Loss: 2.0452, Avg Loss: 2.2113\n\n[Epoch 29, Batch 250/1771] Loss: 2.3037, Avg Loss: 2.2120\n\n[Epoch 29, Batch 300/1771] Loss: 2.1267, Avg Loss: 2.2054\n\n[Epoch 29, Batch 350/1771] Loss: 2.1275, Avg Loss: 2.2055\n\n[Epoch 29, Batch 400/1771] Loss: 2.2317, Avg Loss: 2.2059\n\n[Epoch 29, Batch 450/1771] Loss: 2.1878, Avg Loss: 2.2071\n\n[Epoch 29, Batch 500/1771] Loss: 2.0066, Avg Loss: 2.2062\n\n[Epoch 29, Batch 550/1771] Loss: 2.3812, Avg Loss: 2.2079\n\n[Epoch 29, Batch 600/1771] Loss: 2.1753, Avg Loss: 2.2064\n\n[Epoch 29, Batch 650/1771] Loss: 2.4037, Avg Loss: 2.2045\n\n[Epoch 29, Batch 700/1771] Loss: 1.9831, Avg Loss: 2.2047\n\n[Epoch 29, Batch 750/1771] Loss: 2.2034, Avg Loss: 2.2045\n\n[Epoch 29, Batch 800/1771] Loss: 2.3397, Avg Loss: 2.2047\n\n[Epoch 29, Batch 850/1771] Loss: 2.1914, Avg Loss: 2.2041\n\n[Epoch 29, Batch 900/1771] Loss: 2.3886, Avg Loss: 2.2046\n\n[Epoch 29, Batch 950/1771] Loss: 2.2003, Avg Loss: 2.2026\n\n[Epoch 29, Batch 1000/1771] Loss: 2.2351, Avg Loss: 2.2029\n\n[Epoch 29, Batch 1050/1771] Loss: 2.0380, Avg Loss: 2.2028\n\n[Epoch 29, Batch 1100/1771] Loss: 2.2490, Avg Loss: 2.2011\n\n[Epoch 29, Batch 1150/1771] Loss: 1.9297, Avg Loss: 2.2009\n\n[Epoch 29, Batch 1200/1771] Loss: 2.3490, Avg Loss: 2.2002\n\n[Epoch 29, Batch 1250/1771] Loss: 2.4549, Avg Loss: 2.1995\n\n[Epoch 29, Batch 1300/1771] Loss: 2.2182, Avg Loss: 2.1994\n\n[Epoch 29, Batch 1350/1771] Loss: 2.2804, Avg Loss: 2.1999\n\n[Epoch 29, Batch 1400/1771] Loss: 2.3136, Avg Loss: 2.2000\n\n[Epoch 29, Batch 1450/1771] Loss: 2.0157, Avg Loss: 2.2009\n\n[Epoch 29, Batch 1500/1771] Loss: 2.1387, Avg Loss: 2.2003\n\n[Epoch 29, Batch 1550/1771] Loss: 2.2897, Avg Loss: 2.1999\n\n[Epoch 29, Batch 1600/1771] Loss: 2.3472, Avg Loss: 2.2002\n\n[Epoch 29, Batch 1650/1771] Loss: 2.1457, Avg Loss: 2.2000\n\n[Epoch 29, Batch 1700/1771] Loss: 2.3260, Avg Loss: 2.2000\n\n[Epoch 29, Batch 1750/1771] Loss: 2.2821, Avg Loss: 2.2003\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n\n\n\nEpoch 30/30:   0%|          | 0/1771 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n[Epoch 30, Batch 50/1771] Loss: 2.3198, Avg Loss: 2.1936\n\n[Epoch 30, Batch 100/1771] Loss: 2.3063, Avg Loss: 2.2031\n\n[Epoch 30, Batch 150/1771] Loss: 2.2017, Avg Loss: 2.2058\n\n[Epoch 30, Batch 200/1771] Loss: 2.2128, Avg Loss: 2.1980\n\n[Epoch 30, Batch 250/1771] Loss: 2.2696, Avg Loss: 2.1965\n\n[Epoch 30, Batch 300/1771] Loss: 2.0632, Avg Loss: 2.1976\n\n[Epoch 30, Batch 350/1771] Loss: 2.2163, Avg Loss: 2.1983\n\n[Epoch 30, Batch 400/1771] Loss: 2.1462, Avg Loss: 2.1985\n\n[Epoch 30, Batch 450/1771] Loss: 2.2036, Avg Loss: 2.1994\n\n[Epoch 30, Batch 500/1771] Loss: 2.3156, Avg Loss: 2.2000\n\n[Epoch 30, Batch 550/1771] Loss: 2.1348, Avg Loss: 2.1974\n\n[Epoch 30, Batch 600/1771] Loss: 2.0946, Avg Loss: 2.1952\n\n[Epoch 30, Batch 650/1771] Loss: 2.3804, Avg Loss: 2.1963\n\n[Epoch 30, Batch 700/1771] Loss: 2.1737, Avg Loss: 2.1962\n\n[Epoch 30, Batch 750/1771] Loss: 2.3843, Avg Loss: 2.1984\n\n[Epoch 30, Batch 800/1771] Loss: 2.1990, Avg Loss: 2.1983\n\n[Epoch 30, Batch 850/1771] Loss: 2.3002, Avg Loss: 2.1993\n\n[Epoch 30, Batch 900/1771] Loss: 2.3253, Avg Loss: 2.1997\n\n[Epoch 30, Batch 950/1771] Loss: 2.0592, Avg Loss: 2.2017\n\n[Epoch 30, Batch 1000/1771] Loss: 2.2025, Avg Loss: 2.2015\n\n[Epoch 30, Batch 1050/1771] Loss: 2.0069, Avg Loss: 2.2005\n\n[Epoch 30, Batch 1100/1771] Loss: 2.0833, Avg Loss: 2.2000\n\n[Epoch 30, Batch 1150/1771] Loss: 2.0595, Avg Loss: 2.2006\n\n[Epoch 30, Batch 1200/1771] Loss: 2.2164, Avg Loss: 2.2012\n\n[Epoch 30, Batch 1250/1771] Loss: 2.1877, Avg Loss: 2.2005\n\n[Epoch 30, Batch 1300/1771] Loss: 2.2619, Avg Loss: 2.2003\n\n[Epoch 30, Batch 1350/1771] Loss: 2.2517, Avg Loss: 2.1997\n\n[Epoch 30, Batch 1400/1771] Loss: 2.2344, Avg Loss: 2.2008\n\n[Epoch 30, Batch 1450/1771] Loss: 2.0589, Avg Loss: 2.2001\n\n[Epoch 30, Batch 1500/1771] Loss: 2.0933, Avg Loss: 2.1996\n\n[Epoch 30, Batch 1550/1771] Loss: 2.1826, Avg Loss: 2.1993\n\n[Epoch 30, Batch 1600/1771] Loss: 2.1584, Avg Loss: 2.1998\n\n[Epoch 30, Batch 1650/1771] Loss: 2.2742, Avg Loss: 2.2000\n\n[Epoch 30, Batch 1700/1771] Loss: 2.1658, Avg Loss: 2.2004\n\n[Epoch 30, Batch 1750/1771] Loss: 2.2612, Avg Loss: 2.2002\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30:   0%|          | 0/1771 [00:54<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training history saved\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model, dataloader, config, method='greedy'):\n    \"\"\"\n    Generate captions for images using the trained model.\n    \n    Args:\n        model: Trained captioning model with generate() method\n        dataloader: DataLoader with images\n        config: Configuration dict\n        method: Generation method (only 'greedy' for now)\n        max_length: Maximum caption length\n    \n    Returns:\n        predictions: List of dict with image_id and generated caption\n    \"\"\"\n    vocab = pd.read_pickle(\"/kaggle/working/vocabulary.pkl\")\n    \n    # Get reverse vocabulary (index -> word)\n    idx2word = {idx: word for word, idx in vocab.items()}\n    \n    model.eval()  # Set model to evaluation mode\n    device = next(model.parameters()).device\n    \n    predictions = []\n    \n    pbar = tqdm(dataloader, desc=f'Generating predictions using {method} method')\n    \n    with torch.no_grad():  # No gradient computation needed\n        for batch_idx, batch in enumerate(pbar):\n            # Handle different batch formats\n            if len(batch) == 3:\n                images, captions, lengths = batch\n                image_ids = None\n            elif len(batch) == 2:\n                images, image_ids = batch\n            else:\n                images = batch[0]\n                image_ids = None\n            \n            images = images.to(device)\n            \n            # Generate predictions using model's generate() method\n            generated_indices = model.generate(images, max_length=config.max_length)\n            \n            # Convert indices to words\n            batch_predictions = indices_to_captions(\n                generated_indices, \n                idx2word,\n                remove_special_tokens=True\n            )\n            \n            # Store predictions\n            for i, caption in enumerate(batch_predictions):\n                pred_dict = {\n                    'caption': caption,\n                    'image_id': image_ids[i] if image_ids is not None else batch_idx * len(images) + i\n                }\n                predictions.append(pred_dict)\n    \n    return predictions\n\n\ndef indices_to_captions(indices_batch, idx2word, remove_special_tokens=True):\n    \"\"\"\n    Convert batch of token indices to text captions.\n    \n    Args:\n        indices_batch: Tensor of shape (batch_size, seq_len)\n        idx2word: Dictionary mapping indices to words\n        remove_special_tokens: Whether to remove <START>, <END>, <PAD>\n    \n    Returns:\n        captions: List of string captions\n    \"\"\"\n    captions = []\n    special_tokens = {'<START>', '<END>', '<PAD>', '<UNK>'}\n    \n    # Convert to numpy if needed\n    if torch.is_tensor(indices_batch):\n        indices_batch = indices_batch.cpu().numpy()\n    \n    for indices in indices_batch:\n        words = []\n        for idx in indices:\n            idx = int(idx)\n            word = idx2word.get(idx, '<UNK>')\n            \n            # Stop at <END> token\n            if word == '<END>':\n                break\n            \n            # Skip special tokens if requested\n            if remove_special_tokens and word in special_tokens:\n                continue\n            \n            words.append(word)\n        \n        caption = ' '.join(words)\n        captions.append(caption)\n    \n    return captions\n\n\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# os.makedirs(\"/kaggle/working/test\")\nprint(os.path.join(config.base_dir,\"checkpoints\"))\n\ncheckpoint_path = os.path.join(\n                config.base_dir,\n                config.save_dir,\n                f'best_model.pt'\n            )\ncheckpoint = torch.load(checkpoint_path, map_location=config.device, weights_only= False)\ntest_model = ImageCaptioningTransformer(config.vocab_size, \n                                       config.embed_dim,\n                                       config.num_heads,\n                                       config.num_layers, \n                                       config.image_feat_dim,\n                                       config.max_len,\n                                       config.dropout).to(config.device)\ntest_model.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T12:12:38.545133Z","iopub.execute_input":"2026-01-13T12:12:38.545456Z","iopub.status.idle":"2026-01-13T12:12:38.784924Z","shell.execute_reply.started":"2026-01-13T12:12:38.545429Z","shell.execute_reply":"2026-01-13T12:12:38.784147Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/checkpoints\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"train_df = pd.read_pickle('train_captions.pkl')\nval_df = pd.read_pickle('val_captions.pkl')\n\ntrain_features = np.load('coco_train_vgg16_features.npy', allow_pickle=True).item()\nval_features = np.load('coco_val_vgg16_features.npy',allow_pickle = True).item()\n\nvocab = build_vocabulary(train_df,config.vocab_size)\n\ntrain_loader, val_loader, _ = create_dataloaders(# test_features and test_df are not used during training, so i used val_df as a dummy\n    train_df,val_df,val_df,train_features,val_features,val_features,vocab,config.batch_size,shuffle_train=False\n)\nbatch = next(iter(train_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T13:26:20.160257Z","iopub.execute_input":"2026-01-13T13:26:20.160582Z","iopub.status.idle":"2026-01-13T13:26:35.813200Z","shell.execute_reply.started":"2026-01-13T13:26:20.160554Z","shell.execute_reply":"2026-01-13T13:26:35.812123Z"}},"outputs":[{"name":"stderr","text":"\n\n\nProcessing:   0%|          | 0/113287 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   1%|          | 1343/113287 [00:00<00:08, 13426.08it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   3%|▎         | 2872/113287 [00:00<00:07, 14519.61it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   4%|▍         | 4382/113287 [00:00<00:07, 14780.18it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   5%|▌         | 5866/113287 [00:00<00:07, 14802.68it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   7%|▋         | 7366/113287 [00:00<00:07, 14870.68it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   8%|▊         | 8894/113287 [00:00<00:06, 15006.63it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:   9%|▉         | 10415/113287 [00:00<00:06, 15072.46it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  11%|█         | 11923/113287 [00:00<00:06, 15053.93it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  12%|█▏        | 13434/113287 [00:00<00:06, 15069.46it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  13%|█▎        | 14953/113287 [00:01<00:06, 15104.65it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  15%|█▍        | 16465/113287 [00:01<00:06, 15109.08it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  16%|█▌        | 17976/113287 [00:01<00:06, 15035.36it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  17%|█▋        | 19480/113287 [00:01<00:06, 15022.38it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  19%|█▊        | 20983/113287 [00:01<00:06, 14978.18it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  20%|█▉        | 22481/113287 [00:01<00:06, 14963.11it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  21%|██        | 24009/113287 [00:01<00:05, 15056.60it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  23%|██▎       | 25527/113287 [00:01<00:05, 15092.12it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  24%|██▍       | 27037/113287 [00:01<00:05, 15067.17it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  25%|██▌       | 28544/113287 [00:01<00:05, 15034.08it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  27%|██▋       | 30076/113287 [00:02<00:05, 15118.34it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  28%|██▊       | 31588/113287 [00:02<00:05, 15095.51it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  29%|██▉       | 33098/113287 [00:02<00:05, 15078.15it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  31%|███       | 34606/113287 [00:02<00:05, 14931.86it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  32%|███▏      | 36100/113287 [00:02<00:05, 14711.18it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  33%|███▎      | 37625/113287 [00:02<00:05, 14869.29it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  35%|███▍      | 39113/113287 [00:02<00:05, 14416.66it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  36%|███▌      | 40612/113287 [00:02<00:04, 14581.91it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  37%|███▋      | 42110/113287 [00:02<00:04, 14696.56it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  38%|███▊      | 43582/113287 [00:02<00:04, 14185.00it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  40%|███▉      | 45069/113287 [00:03<00:04, 14380.68it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  41%|████      | 46578/113287 [00:03<00:04, 14586.22it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  42%|████▏     | 48106/113287 [00:03<00:04, 14788.01it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  44%|████▍     | 49612/113287 [00:03<00:04, 14866.04it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  45%|████▌     | 51101/113287 [00:03<00:04, 14683.66it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  46%|████▋     | 52630/113287 [00:03<00:04, 14861.65it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  48%|████▊     | 54156/113287 [00:03<00:03, 14978.04it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  49%|████▉     | 55689/113287 [00:03<00:03, 15080.15it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  51%|█████     | 57224/113287 [00:03<00:03, 15160.55it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  52%|█████▏    | 58741/113287 [00:03<00:03, 15131.95it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  53%|█████▎    | 60255/113287 [00:04<00:03, 14991.03it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  55%|█████▍    | 61771/113287 [00:04<00:03, 15039.03it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  56%|█████▌    | 63276/113287 [00:04<00:03, 15036.93it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  57%|█████▋    | 64815/113287 [00:04<00:03, 15139.89it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  59%|█████▊    | 66330/113287 [00:04<00:03, 15026.35it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  60%|█████▉    | 67840/113287 [00:04<00:03, 15045.48it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  61%|██████    | 69368/113287 [00:04<00:02, 15113.80it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  63%|██████▎   | 70885/113287 [00:04<00:02, 15128.69it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  64%|██████▍   | 72422/113287 [00:04<00:02, 15200.58it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  65%|██████▌   | 73965/113287 [00:04<00:02, 15267.00it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  67%|██████▋   | 75492/113287 [00:05<00:02, 15183.94it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  68%|██████▊   | 77034/113287 [00:05<00:02, 15251.32it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  69%|██████▉   | 78579/113287 [00:05<00:02, 15310.08it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  71%|███████   | 80111/113287 [00:05<00:02, 15290.33it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  72%|███████▏  | 81641/113287 [00:05<00:02, 15206.08it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  73%|███████▎  | 83162/113287 [00:05<00:02, 14899.25it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  75%|███████▍  | 84673/113287 [00:05<00:01, 14960.82it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  76%|███████▌  | 86171/113287 [00:05<00:01, 13632.18it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  77%|███████▋  | 87708/113287 [00:05<00:01, 14114.66it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  79%|███████▉  | 89251/113287 [00:05<00:01, 14487.43it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  80%|████████  | 90779/113287 [00:06<00:01, 14716.18it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  81%|████████▏ | 92277/113287 [00:06<00:01, 14790.76it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  83%|████████▎ | 93775/113287 [00:06<00:01, 14844.49it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  84%|████████▍ | 95266/113287 [00:06<00:01, 14619.43it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  85%|████████▌ | 96747/113287 [00:06<00:01, 14671.07it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  87%|████████▋ | 98261/113287 [00:06<00:01, 14807.96it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  88%|████████▊ | 99750/113287 [00:06<00:00, 14830.53it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  89%|████████▉ | 101236/113287 [00:06<00:00, 14838.95it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  91%|█████████ | 102738/113287 [00:06<00:00, 14892.61it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  92%|█████████▏| 104229/113287 [00:07<00:00, 14883.06it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  93%|█████████▎| 105745/113287 [00:07<00:00, 14965.34it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  95%|█████████▍| 107251/113287 [00:07<00:00, 14990.79it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  96%|█████████▌| 108752/113287 [00:07<00:00, 14993.69it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  97%|█████████▋| 110252/113287 [00:07<00:00, 14951.44it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing:  99%|█████████▊| 111748/113287 [00:07<00:00, 14872.91it/s]\u001b[A\u001b[A\u001b[A\n\n\nProcessing: 100%|██████████| 113287/113287 [00:07<00:00, 14888.45it/s]\u001b[A\u001b[A\u001b[A\n\n\n\nAdding words: 100%|██████████| 9217/9217 [00:00<00:00, 1792003.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nDataLoader Configuration:\n  Batch size: 64\n  Num workers: 4\n  Max caption length: 15\n\nDataLoader Sizes:\n  Train batches: 1771\n  Val batches: 79\n  Test batches: 79\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"print(batch)\nprint(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T13:26:59.765824Z","iopub.execute_input":"2026-01-13T13:26:59.766179Z","iopub.status.idle":"2026-01-13T13:26:59.781502Z","shell.execute_reply.started":"2026-01-13T13:26:59.766149Z","shell.execute_reply":"2026-01-13T13:26:59.780787Z"}},"outputs":[{"name":"stdout","text":"[tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7481, 0.0000]],\n\n        [[0.0000, 1.2407, 0.0000,  ..., 0.0000, 1.4061, 0.0000]],\n\n        ...,\n\n        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 2.2256]],\n\n        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4438, 0.0000]],\n\n        [[0.1779, 0.0000, 0.0000,  ..., 0.0000, 1.4963, 1.8912]]]), tensor([[   1,    4,   23,  ...,    0,    0,    0],\n        [   1,    4,   49,  ...,    0,    0,    0],\n        [   1,    4,   34,  ...,    0,    0,    0],\n        ...,\n        [   1,    4,  424,  ...,    0,    0,    0],\n        [   1,    4,  145,  ...,    0,    0,    0],\n        [   1,    4,   12,  ...,  562, 1074,    2]]), tensor([13, 11, 11, 14, 16, 11, 12, 13, 14, 14, 15, 12, 13, 10, 11, 12, 16, 11,\n        10, 10, 12, 12, 13, 17, 15, 13, 10, 12, 12, 11, 17, 16, 13, 13, 15, 11,\n        16, 10, 10, 12, 10, 13, 12, 11, 12, 11, 10, 14, 10, 13, 10, 12, 13, 12,\n        13, 15, 12, 11, 11, 11, 13, 11, 11, 17])]\n   image_id                      file_name  split  \\\n0    522418  COCO_val2014_000000522418.jpg  train   \n1    318219  COCO_val2014_000000318219.jpg  train   \n2    554625  COCO_val2014_000000554625.jpg  train   \n3    397133  COCO_val2014_000000397133.jpg  train   \n4    574769  COCO_val2014_000000574769.jpg  train   \n\n                                            captions  \n0  [A woman wearing a net on her head cutting a c...  \n1  [A young boy standing in front of a computer k...  \n2  [a boy wearing headphones using one computer i...  \n3  [A man is in a kitchen making pizzas., Man in ...  \n4  [A woman in a room with a cat., A girl smiles ...  \n","output_type":"stream"}],"execution_count":64}]}