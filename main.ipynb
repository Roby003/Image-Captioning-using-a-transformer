{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data loading and feature extraction","metadata":{}},{"cell_type":"markdown","source":"#### Load Karpathy split and organize the COCO data according to it","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nimport os\n\n\nkarpathy_file = '/kaggle/input/karpathy-splits/dataset_coco.json'\n\nif not os.path.exists(karpathy_file):\n    raise FileNotFoundError(f\"Karpathy split not found at: {karpathy_file}\")\n\nwith open(karpathy_file, 'r') as f:\n    karpathy_data = json.load(f)\n\n\ndef organize_by_split(karpathy_data):\n    splits = {'train': [], 'val': [], 'test': []}\n    \n    for img_data in karpathy_data['images']:\n        split = img_data['split']\n        \n        # handle 'restval' - we add them to the training set \n        if split == 'restval':\n            split = 'train'\n        \n        if split in ['train', 'val', 'test']:\n          \n            image_info = {\n                'image_id': img_data['cocoid'],\n                'file_name': img_data['filename'],  \n                'captions': [sent['raw'] for sent in img_data['sentences']]\n            }\n            splits[split].append(image_info)\n    \n    return splits\n\nsplits_data = organize_by_split(karpathy_data)\n\n# convert to DataFrames\ntrain_df = pd.DataFrame(splits_data['train'])\nval_df = pd.DataFrame(splits_data['val'])\ntest_df = pd.DataFrame(splits_data['test'])\nprint(train_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the pretrained feature extractor models","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"device: {device}\")\n\n# VGG16 - fc7 features (4096-dim) - matching the paper\n\nvgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nvgg16.classifier = vgg16.classifier[:-1]  # remove last layer to get fc7\nvgg16 = vgg16.to(device)\nvgg16.eval()\n\n# ResNet101\n\nresnet101 = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\nresnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])  # remove FC layer\nresnet101 = resnet101.to(device)\nresnet101.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature extraction","metadata":{}},{"cell_type":"code","source":"# image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\n\ndef extract_features(image_path, model):\n    try:\n        img = Image.open(image_path).convert('RGB')\n        img = transform(img).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            features = model(img)\n            if len(features.shape) > 2:\n                features = features.squeeze()\n        \n        return features.cpu().numpy()\n    except Exception as e:\n        return None\n\n\ndef get_image_path(filename, base_paths):   \n    # determine which folder based on filename\n    if 'train2014' in filename:\n        folder = 'train2014'\n    elif 'val2014' in filename:\n        folder = 'val2014'\n    else:\n        return None\n    \n    img_path = f\"{base_paths[folder]}/{filename}\"\n    \n    if os.path.exists(img_path):\n        return img_path\n    else:\n        return None\n\n\n\ndef extract_and_save_split_features(df, split_name, base_paths, models_dict):\n   \n    features_by_model = {model_name: {} for model_name in models_dict.keys()}\n    missing_images = []\n    processed = 0\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{split_name}\"):\n        img_id = row['image_id']\n        img_filename = row['file_name']\n        \n        img_path = get_image_path(img_filename, base_paths)\n        \n        if img_path is None:\n            missing_images.append(img_filename)\n            continue\n        \n        # extract features with each model\n        for model_name, model in models_dict.items():\n            features = extract_features(img_path, model)\n            if features is not None:\n                features_by_model[model_name][img_id] = features\n        \n        processed += 1\n    \n\nBASE_PATHS = {\n    'train2014': '/kaggle/input/coco2014/train2014/train2014',\n    'val2014': '/kaggle/input/coco2014/val2014/val2014'\n}\n\n\nmodels_dict = {\n    'vgg16': vgg16,\n    'resnet101': resnet101,\n}\n\n#start feature extraction\ntrain_features = extract_and_save_split_features(train_df, 'train', BASE_PATHS, models_dict)\nval_features = extract_and_save_split_features(val_df, 'val', BASE_PATHS, models_dict)\ntest_features = extract_and_save_split_features(test_df, 'test', BASE_PATHS, models_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Save the caption metadata","metadata":{}},{"cell_type":"code","source":"train_df.to_pickle('train_captions.pkl')\nval_df.to_pickle('val_captions.pkl')\ntest_df.to_pickle('test_captions.pkl')\n\ntrain_df.to_csv('train_captions.csv', index=False)\nval_df.to_csv('val_captions.csv', index=False)\ntest_df.to_csv('test_captions.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vocabulary\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport re\nfrom collections import Counter\nfrom tqdm import tqdm\n\n\n# vocabulary size  ~9,221 words + special tokens ( matches the paper's approach)\n\nclass Vocabulary:\n    \n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.word_counts = Counter()\n        \n        # special tokens\n        self.PAD_TOKEN = '<PAD>'\n        self.START_TOKEN = '<START>'\n        self.END_TOKEN = '<END>'\n        self.UNK_TOKEN = '<UNK>'\n        \n        # initialize with special tokens\n        self.word2idx = {\n            self.PAD_TOKEN: 0,\n            self.START_TOKEN: 1,\n            self.END_TOKEN: 2,\n            self.UNK_TOKEN: 3\n        }\n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n        self.idx = 4  # next available index\n    \n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n    \n    def __len__(self):\n        return len(self.word2idx)\n    \n    def __call__(self, word):\n        return self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])\n\n\ndef tokenize_caption(caption):\n    \"\"\"\n    - Convert to lowercase\n    - Remove punctuation (except hyphens in words)\n    - Split into words\n\n    \"\"\"\n    caption = caption.lower()\n    \n    # keeps alphanumeric, apostrophes, and hyphens\n    caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n    \n    # split and remove extra whitespace\n    tokens = caption.split()\n    \n    # remove empty strings\n    tokens = [t for t in tokens if t]\n    \n    return tokens\n\n\ndef build_vocabulary(train_captions_df, vocab_size=9221, min_word_freq=5):\n    \n    vocab = Vocabulary()\n    \n    # count all words in training captions\n    all_tokens = []\n    \n    for idx, row in tqdm(train_captions_df.iterrows(), \n                         total=len(train_captions_df),\n                         desc=\"Processing\"):\n        captions = row['captions']\n\n        #process captions\n        for caption in captions:\n            tokens = tokenize_caption(caption)\n            all_tokens.extend(tokens)\n            vocab.word_counts.update(tokens)\n    \n\n    # filter by minimum frequency\n    filtered_words = {word: count for word, count in vocab.word_counts.items() \n                      if count >= min_word_freq}\n    \n\n    most_common = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)\n    \n    # vocab_size - 4 (to account for special tokens)\n    top_words = most_common[:vocab_size - 4]\n    \n    for word, count in tqdm(top_words, desc=\"Adding words\"):\n        vocab.add_word(word)\n    \n    return vocab\n\n\ndef save_vocabulary(vocab, filepath='vocabulary.pkl'):\n    with open(filepath, 'wb') as f:\n        pickle.dump(vocab, f)\n\n\ndef load_vocabulary(filepath='vocabulary.pkl'):\n    with open(filepath, 'rb') as f:\n        vocab = pickle.load(f)\n\n    return vocab\n\n\n\n\n# load training captions\ntrain_df = pd.read_pickle('train_captions.pkl')\n\nvocab = build_vocabulary(\n    train_df, \n    vocab_size=9221,  \n    min_word_freq=5  \n)\n\nanalyze_vocabulary(vocab, train_df)\n\nsave_vocabulary(vocab, 'vocabulary.pkl')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and DataLoaders","metadata":{}},{"cell_type":"markdown","source":"Dataset Objects","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport random\nfrom typing import Tuple, List\n\n\nclass CaptionDataset(Dataset):\n    \"\"\"\n    dataset for image captioning that loads pre-extracted features and captions.\n    \n    Args:\n        captions_df: DataFrame with columns ['image_id', 'file_name', 'captions']\n        features_dict: Dictionary mapping image_id -> feature vector \n        vocabulary: Vocabulary object\n        max_length: Maximum caption length (default: 15, matching paper)\n        training: If True, randomly sample one caption per image per epoch\n    \"\"\"\n    \n    def __init__(self, \n                 captions_df: pd.DataFrame,\n                 features_dict: dict,\n                 vocabulary,\n                 max_length: int = 15,\n                 training: bool = True):\n        \n        self.captions_df = captions_df.reset_index(drop=True)\n        self.features_dict = features_dict\n        self.vocab = vocabulary\n        self.max_length = max_length\n        self.training = training\n        \n    \n    def __len__(self):\n        return len(self.valid_indices)\n    \n    def tokenize_caption(self, caption: str) -> List[str]:\n        \n        import re\n        caption = caption.lower()\n        caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n        tokens = caption.split()\n        tokens = [t for t in tokens if t]\n        return tokens\n    \n    def caption_to_sequence(self, caption: str) -> Tuple[torch.Tensor, int]:\n        \"\"\"\n        Convert caption to sequence of word indices with <START> and <END>. Pad to max length using <PAD>\n        \"\"\"\n        \n        words = self.tokenize_caption(caption)\n        \n        # leave room for START and END\n        if len(words) > self.max_length:\n            words = words[:self.max_length]\n        \n        # convert to indices and add START + END\n        tokens = [self.vocab.word2idx[self.vocab.START_TOKEN]]\n        tokens.extend([self.vocab(word) for word in words])\n        tokens.append(self.vocab.word2idx[self.vocab.END_TOKEN])\n        \n        actual_length = len(tokens)\n        \n        # pad to max_length + 2 (for START and END)\n        max_seq_len = self.max_length + 2\n        if len(tokens) < max_seq_len:\n            tokens.extend([self.vocab.word2idx[self.vocab.PAD_TOKEN]] * (max_seq_len - len(tokens)))\n        \n        return torch.tensor(tokens, dtype=torch.long), actual_length\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        returns:\n            image_features: Tensor of shape (feature_dim,) - e.g., (4096,) for VGG16\n            caption: Tensor of shape (max_length + 2,) - padded caption sequence\n            caption_length: int - actual caption length including START/END\n        \"\"\"\n        df_idx = self.valid_indices[idx]\n        row = self.captions_df.iloc[df_idx]\n        \n        # Get image features\n        image_id = row['image_id']\n        image_features = self.features_dict[image_id]\n        image_features = torch.from_numpy(image_features).float()\n        \n        # get caption\n        captions = row['captions']\n        if self.training:\n            caption = random.choice(captions)\n        else:\n            caption = captions[0]\n        \n        # Convert caption to sequence\n        caption_seq, caption_length = self.caption_to_sequence(caption)\n        \n        return image_features, caption_seq, caption_length\n\n\nclass CaptionDatasetAllCaptions(Dataset):\n    \"\"\"\n    dataset that returns all captions for each image.\n    \n    Args:\n        captions_df: DataFrame with columns ['image_id', 'file_name', 'captions']\n        features_dict: Dictionary mapping image_id -> feature vector\n        vocabulary: Vocabulary object\n        max_length: Maximum caption length\n    \"\"\"\n    \n    def __init__(self, \n                 captions_df: pd.DataFrame,\n                 features_dict: dict,\n                 vocabulary,\n                 max_length: int = 15):\n        \n        self.captions_df = captions_df.reset_index(drop=True)\n        self.features_dict = features_dict\n        self.vocab = vocabulary\n        self.max_length = max_length\n        \n        # create expanded dataset with one entry per caption\n        self.data = []\n        for idx, row in self.captions_df.iterrows():\n            if row['image_id'] in self.features_dict:\n                for caption in row['captions']:\n                    self.data.append({\n                        'image_id': row['image_id'],\n                        'caption': caption,\n                        'all_captions': row['captions']\n                    })\n        \n\n    \n    def tokenize_caption(self, caption: str) -> List[str]:\n        import re\n        caption = caption.lower()\n        caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n        tokens = caption.split()\n        tokens = [t for t in tokens if t]\n        return tokens\n    \n    def caption_to_sequence(self, caption: str) -> Tuple[torch.Tensor, int]:\n        words = self.tokenize_caption(caption)\n        if len(words) > self.max_length:\n            words = words[:self.max_length]\n        \n        tokens = [self.vocab.word2idx[self.vocab.START_TOKEN]]\n        tokens.extend([self.vocab(word) for word in words])\n        tokens.append(self.vocab.word2idx[self.vocab.END_TOKEN])\n        \n        actual_length = len(tokens)\n        max_seq_len = self.max_length + 2\n        \n        if len(tokens) < max_seq_len:\n            tokens.extend([self.vocab.word2idx[self.vocab.PAD_TOKEN]] * (max_seq_len - len(tokens)))\n        \n        return torch.tensor(tokens, dtype=torch.long), actual_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        image_features = self.features_dict[item['image_id']]\n        image_features = torch.from_numpy(image_features).float()\n        \n        # convert caption to sequence\n        caption_seq, caption_length = self.caption_to_sequence(item['caption'])\n        \n        return image_features, caption_seq, caption_length, item['all_captions']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper functions and main dataloader entrypoint","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Custom collate function for batching.\n    \n    Args:\n        batch: List of (image_features, caption, length) tuples\n    \n    Returns:\n        images: Tensor of shape (batch_size, feature_dim)\n        captions: Tensor of shape (batch_size, max_length + 2)\n        lengths: Tensor of shape (batch_size,)\n    \"\"\"\n    # Separate the components\n    images, captions, lengths = zip(*batch)\n    \n    # Stack into tensors\n    images = torch.stack(images, dim=0)\n    captions = torch.stack(captions, dim=0)\n    lengths = torch.tensor(lengths, dtype=torch.long)\n    \n    return images, captions, lengths\n\n\ndef collate_fn_eval(batch):\n    \"\"\"\n    Custom collate function for evaluation (includes all reference captions).\n    \n    Returns:\n        images: Tensor of shape (batch_size, feature_dim)\n        captions: Tensor of shape (batch_size, max_length + 2)\n        lengths: Tensor of shape (batch_size,)\n        all_captions: List of lists of reference captions\n    \"\"\"\n    images, captions, lengths, all_captions = zip(*batch)\n    \n    images = torch.stack(images, dim=0)\n    captions = torch.stack(captions, dim=0)\n    lengths = torch.tensor(lengths, dtype=torch.long)\n    \n    return images, captions, lengths, list(all_captions)\n\n\ndef create_dataloaders(\n    train_df: pd.DataFrame,\n    val_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_features: dict,\n    val_features: dict,\n    test_features: dict,\n    vocabulary,\n    batch_size: int = 64,\n    max_length: int = 15,\n    num_workers: int = 4,\n    shuffle_train: bool = True\n):\n    \"\"\"\n    Create train, validation, and test dataloaders.\n    \n    Args:\n        train_df, val_df, test_df: DataFrames with captions\n        train_features, val_features, test_features: Feature dictionaries\n        vocabulary: Vocabulary object\n        batch_size: Batch size for training\n        max_length: Maximum caption length\n        num_workers: Number of worker processes for data loading\n        shuffle_train: Whether to shuffle training data\n    \n    Returns:\n        train_loader, val_loader, test_loader\n    \"\"\"\n  \n    # create datasets\n    train_dataset = CaptionDataset(\n        train_df, \n        train_features, \n        vocabulary, \n        max_length=max_length,\n        training=True\n    )\n    \n    val_dataset = CaptionDataset(\n        val_df, \n        val_features, \n        vocabulary, \n        max_length=max_length,\n        training=False\n    )\n    \n    test_dataset = CaptionDataset(\n        test_df, \n        test_features, \n        vocabulary, \n        max_length=max_length,\n        training=False\n    )\n    \n    # create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=shuffle_train,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    print(f\"\\nDataLoader Configuration:\")\n    print(f\"  Batch size: {batch_size}\")\n    print(f\"  Num workers: {num_workers}\")\n    print(f\"  Max caption length: {max_length}\")\n    \n    print(f\"\\nDataLoader Sizes:\")\n    print(f\"  Train batches: {len(train_loader)}\")\n    print(f\"  Val batches: {len(val_loader)}\")\n    print(f\"  Test batches: {len(test_loader)}\")\n    \n    return train_loader, val_loader, test_loader\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ","metadata":{}},{"cell_type":"markdown","source":"### ","metadata":{}}]}