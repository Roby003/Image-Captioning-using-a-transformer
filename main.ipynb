{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data loading and feature extraction","metadata":{}},{"cell_type":"markdown","source":"#### Load Karpathy split and organize the COCO data according to it","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nimport os\n\n\nkarpathy_file = '/kaggle/input/karpathy-splits/dataset_coco.json'\n\nif not os.path.exists(karpathy_file):\n    raise FileNotFoundError(f\"Karpathy split not found at: {karpathy_file}\")\n\nwith open(karpathy_file, 'r') as f:\n    karpathy_data = json.load(f)\n\n\ndef organize_by_split(karpathy_data):\n    splits = {'train': [], 'val': [], 'test': []}\n    \n    for img_data in karpathy_data['images']:\n        split = img_data['split']\n        \n        # handle 'restval' - we add them to the training set \n        if split == 'restval':\n            split = 'train'\n        \n        if split in ['train', 'val', 'test']:\n          \n            image_info = {\n                'image_id': img_data['cocoid'],\n                'file_name': img_data['filename'],  \n                'captions': [sent['raw'] for sent in img_data['sentences']]\n            }\n            splits[split].append(image_info)\n    \n    return splits\n\nsplits_data = organize_by_split(karpathy_data)\n\n# convert to DataFrames\ntrain_df = pd.DataFrame(splits_data['train'])\nval_df = pd.DataFrame(splits_data['val'])\ntest_df = pd.DataFrame(splits_data['test'])\nprint(train_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the pretrained feature extractor models","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"device: {device}\")\n\n# VGG16 - fc7 features (4096-dim) - matching the paper\n\nvgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nvgg16.classifier = vgg16.classifier[:-1]  # remove last layer to get fc7\nvgg16 = vgg16.to(device)\nvgg16.eval()\n\n# ResNet101\n\nresnet101 = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\nresnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])  # remove FC layer\nresnet101 = resnet101.to(device)\nresnet101.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature extraction","metadata":{}},{"cell_type":"code","source":"# image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\n\ndef extract_features(image_path, model):\n    try:\n        img = Image.open(image_path).convert('RGB')\n        img = transform(img).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            features = model(img)\n            if len(features.shape) > 2:\n                features = features.squeeze()\n        \n        return features.cpu().numpy()\n    except Exception as e:\n        return None\n\n\ndef get_image_path(filename, base_paths):   \n    # determine which folder based on filename\n    if 'train2014' in filename:\n        folder = 'train2014'\n    elif 'val2014' in filename:\n        folder = 'val2014'\n    else:\n        return None\n    \n    img_path = f\"{base_paths[folder]}/{filename}\"\n    \n    if os.path.exists(img_path):\n        return img_path\n    else:\n        return None\n\n\n\ndef extract_and_save_split_features(df, split_name, base_paths, models_dict):\n   \n    features_by_model = {model_name: {} for model_name in models_dict.keys()}\n    missing_images = []\n    processed = 0\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{split_name}\"):\n        img_id = row['image_id']\n        img_filename = row['file_name']\n        \n        img_path = get_image_path(img_filename, base_paths)\n        \n        if img_path is None:\n            missing_images.append(img_filename)\n            continue\n        \n        # extract features with each model\n        for model_name, model in models_dict.items():\n            features = extract_features(img_path, model)\n            if features is not None:\n                features_by_model[model_name][img_id] = features\n        \n        processed += 1\n    \n\nBASE_PATHS = {\n    'train2014': '/kaggle/input/coco2014/train2014/train2014',\n    'val2014': '/kaggle/input/coco2014/val2014/val2014'\n}\n\n\nmodels_dict = {\n    'vgg16': vgg16,\n    'resnet101': resnet101,\n}\n\n#start feature extraction\ntrain_features = extract_and_save_split_features(train_df, 'train', BASE_PATHS, models_dict)\nval_features = extract_and_save_split_features(val_df, 'val', BASE_PATHS, models_dict)\ntest_features = extract_and_save_split_features(test_df, 'test', BASE_PATHS, models_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Save the caption metadata","metadata":{}},{"cell_type":"code","source":"train_df.to_pickle('train_captions.pkl')\nval_df.to_pickle('val_captions.pkl')\ntest_df.to_pickle('test_captions.pkl')\n\ntrain_df.to_csv('train_captions.csv', index=False)\nval_df.to_csv('val_captions.csv', index=False)\ntest_df.to_csv('test_captions.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vocabulary\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport re\nfrom collections import Counter\nfrom tqdm import tqdm\n\n\n# vocabulary size  ~9,221 words + special tokens ( matches the paper's approach)\n\nclass Vocabulary:\n    \n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.word_counts = Counter()\n        \n        # special tokens\n        self.PAD_TOKEN = '<PAD>'\n        self.START_TOKEN = '<START>'\n        self.END_TOKEN = '<END>'\n        self.UNK_TOKEN = '<UNK>'\n        \n        # initialize with special tokens\n        self.word2idx = {\n            self.PAD_TOKEN: 0,\n            self.START_TOKEN: 1,\n            self.END_TOKEN: 2,\n            self.UNK_TOKEN: 3\n        }\n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n        self.idx = 4  # next available index\n    \n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n    \n    def __len__(self):\n        return len(self.word2idx)\n    \n    def __call__(self, word):\n        return self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])\n\n\ndef tokenize_caption(caption):\n    \"\"\"\n    - Convert to lowercase\n    - Remove punctuation (except hyphens in words)\n    - Split into words\n\n    \"\"\"\n    caption = caption.lower()\n    \n    # keeps alphanumeric, apostrophes, and hyphens\n    caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n    \n    # split and remove extra whitespace\n    tokens = caption.split()\n    \n    # remove empty strings\n    tokens = [t for t in tokens if t]\n    \n    return tokens\n\n\ndef build_vocabulary(train_captions_df, vocab_size=9221, min_word_freq=5):\n    \n    vocab = Vocabulary()\n    \n    # count all words in training captions\n    all_tokens = []\n    \n    for idx, row in tqdm(train_captions_df.iterrows(), \n                         total=len(train_captions_df),\n                         desc=\"Processing\"):\n        captions = row['captions']\n\n        #process captions\n        for caption in captions:\n            tokens = tokenize_caption(caption)\n            all_tokens.extend(tokens)\n            vocab.word_counts.update(tokens)\n    \n\n    # filter by minimum frequency\n    filtered_words = {word: count for word, count in vocab.word_counts.items() \n                      if count >= min_word_freq}\n    \n\n    most_common = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)\n    \n    # vocab_size - 4 (to account for special tokens)\n    top_words = most_common[:vocab_size - 4]\n    \n    for word, count in tqdm(top_words, desc=\"Adding words\"):\n        vocab.add_word(word)\n    \n    return vocab\n\n\ndef save_vocabulary(vocab, filepath='vocabulary.pkl'):\n    with open(filepath, 'wb') as f:\n        pickle.dump(vocab, f)\n\n\ndef load_vocabulary(filepath='vocabulary.pkl'):\n    with open(filepath, 'rb') as f:\n        vocab = pickle.load(f)\n\n    return vocab\n\n\n\n\n# load training captions\ntrain_df = pd.read_pickle('train_captions.pkl')\n\nvocab = build_vocabulary(\n    train_df, \n    vocab_size=9221,  \n    min_word_freq=5  \n)\n\nanalyze_vocabulary(vocab, train_df)\n\nsave_vocabulary(vocab, 'vocabulary.pkl')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and DataLoaders","metadata":{}},{"cell_type":"markdown","source":"Dataset Objects","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport random\nfrom typing import Tuple, List\n\n\nclass CaptionDataset(Dataset):\n    \"\"\"\n    dataset for image captioning that loads pre-extracted features and captions.\n    \n    Args:\n        captions_df: DataFrame with columns ['image_id', 'file_name', 'captions']\n        features_dict: Dictionary mapping image_id -> feature vector \n        vocabulary: Vocabulary object\n        max_length: Maximum caption length (default: 15, matching paper)\n        training: If True, randomly sample one caption per image per epoch\n    \"\"\"\n    \n    def __init__(self, \n                 captions_df: pd.DataFrame,\n                 features_dict: dict,\n                 vocabulary,\n                 max_length: int = 15,\n                 training: bool = True):\n        \n        self.captions_df = captions_df.reset_index(drop=True)\n        self.features_dict = features_dict\n        self.vocab = vocabulary\n        self.max_length = max_length\n        self.training = training\n        \n    \n    def __len__(self):\n        return len(self.valid_indices)\n    \n    def tokenize_caption(self, caption: str) -> List[str]:\n        \n        import re\n        caption = caption.lower()\n        caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n        tokens = caption.split()\n        tokens = [t for t in tokens if t]\n        return tokens\n    \n    def caption_to_sequence(self, caption: str) -> Tuple[torch.Tensor, int]:\n        \"\"\"\n        Convert caption to sequence of word indices with <START> and <END>. Pad to max length using <PAD>\n        \"\"\"\n        \n        words = self.tokenize_caption(caption)\n        \n        # leave room for START and END\n        if len(words) > self.max_length:\n            words = words[:self.max_length]\n        \n        # convert to indices and add START + END\n        tokens = [self.vocab.word2idx[self.vocab.START_TOKEN]]\n        tokens.extend([self.vocab(word) for word in words])\n        tokens.append(self.vocab.word2idx[self.vocab.END_TOKEN])\n        \n        actual_length = len(tokens)\n        \n        # pad to max_length + 2 (for START and END)\n        max_seq_len = self.max_length + 2\n        if len(tokens) < max_seq_len:\n            tokens.extend([self.vocab.word2idx[self.vocab.PAD_TOKEN]] * (max_seq_len - len(tokens)))\n        \n        return torch.tensor(tokens, dtype=torch.long), actual_length\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        returns:\n            image_features: Tensor of shape (feature_dim,) - e.g., (4096,) for VGG16\n            caption: Tensor of shape (max_length + 2,) - padded caption sequence\n            caption_length: int - actual caption length including START/END\n        \"\"\"\n        df_idx = self.valid_indices[idx]\n        row = self.captions_df.iloc[df_idx]\n        \n        # Get image features\n        image_id = row['image_id']\n        image_features = self.features_dict[image_id]\n        image_features = torch.from_numpy(image_features).float()\n        \n        # get caption\n        captions = row['captions']\n        if self.training:\n            caption = random.choice(captions)\n        else:\n            caption = captions[0]\n        \n        # Convert caption to sequence\n        caption_seq, caption_length = self.caption_to_sequence(caption)\n        \n        return image_features, caption_seq, caption_length\n\n\nclass CaptionDatasetAllCaptions(Dataset):\n    \"\"\"\n    dataset that returns all captions for each image.\n    \n    Args:\n        captions_df: DataFrame with columns ['image_id', 'file_name', 'captions']\n        features_dict: Dictionary mapping image_id -> feature vector\n        vocabulary: Vocabulary object\n        max_length: Maximum caption length\n    \"\"\"\n    \n    def __init__(self, \n                 captions_df: pd.DataFrame,\n                 features_dict: dict,\n                 vocabulary,\n                 max_length: int = 15):\n        \n        self.captions_df = captions_df.reset_index(drop=True)\n        self.features_dict = features_dict\n        self.vocab = vocabulary\n        self.max_length = max_length\n        \n        # create expanded dataset with one entry per caption\n        self.data = []\n        for idx, row in self.captions_df.iterrows():\n            if row['image_id'] in self.features_dict:\n                for caption in row['captions']:\n                    self.data.append({\n                        'image_id': row['image_id'],\n                        'caption': caption,\n                        'all_captions': row['captions']\n                    })\n        \n\n    \n    def tokenize_caption(self, caption: str) -> List[str]:\n        import re\n        caption = caption.lower()\n        caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n        tokens = caption.split()\n        tokens = [t for t in tokens if t]\n        return tokens\n    \n    def caption_to_sequence(self, caption: str) -> Tuple[torch.Tensor, int]:\n        words = self.tokenize_caption(caption)\n        if len(words) > self.max_length:\n            words = words[:self.max_length]\n        \n        tokens = [self.vocab.word2idx[self.vocab.START_TOKEN]]\n        tokens.extend([self.vocab(word) for word in words])\n        tokens.append(self.vocab.word2idx[self.vocab.END_TOKEN])\n        \n        actual_length = len(tokens)\n        max_seq_len = self.max_length + 2\n        \n        if len(tokens) < max_seq_len:\n            tokens.extend([self.vocab.word2idx[self.vocab.PAD_TOKEN]] * (max_seq_len - len(tokens)))\n        \n        return torch.tensor(tokens, dtype=torch.long), actual_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        image_features = self.features_dict[item['image_id']]\n        image_features = torch.from_numpy(image_features).float()\n        \n        # convert caption to sequence\n        caption_seq, caption_length = self.caption_to_sequence(item['caption'])\n        \n        return image_features, caption_seq, caption_length, item['all_captions']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper functions and main dataloader entrypoint","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Custom collate function for batching.\n    \n    Args:\n        batch: List of (image_features, caption, length) tuples\n    \n    Returns:\n        images: Tensor of shape (batch_size, feature_dim)\n        captions: Tensor of shape (batch_size, max_length + 2)\n        lengths: Tensor of shape (batch_size,)\n    \"\"\"\n    # Separate the components\n    images, captions, lengths = zip(*batch)\n    \n    # Stack into tensors\n    images = torch.stack(images, dim=0)\n    captions = torch.stack(captions, dim=0)\n    lengths = torch.tensor(lengths, dtype=torch.long)\n    \n    return images, captions, lengths\n\n\ndef collate_fn_eval(batch):\n    \"\"\"\n    Custom collate function for evaluation (includes all reference captions).\n    \n    Returns:\n        images: Tensor of shape (batch_size, feature_dim)\n        captions: Tensor of shape (batch_size, max_length + 2)\n        lengths: Tensor of shape (batch_size,)\n        all_captions: List of lists of reference captions\n    \"\"\"\n    images, captions, lengths, all_captions = zip(*batch)\n    \n    images = torch.stack(images, dim=0)\n    captions = torch.stack(captions, dim=0)\n    lengths = torch.tensor(lengths, dtype=torch.long)\n    \n    return images, captions, lengths, list(all_captions)\n\n\ndef create_dataloaders(\n    train_df: pd.DataFrame,\n    val_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_features: dict,\n    val_features: dict,\n    test_features: dict,\n    vocabulary,\n    batch_size: int = 64,\n    max_length: int = 15,\n    num_workers: int = 4,\n    shuffle_train: bool = True\n):\n    \"\"\"\n    Create train, validation, and test dataloaders.\n    \n    Args:\n        train_df, val_df, test_df: DataFrames with captions\n        train_features, val_features, test_features: Feature dictionaries\n        vocabulary: Vocabulary object\n        batch_size: Batch size for training\n        max_length: Maximum caption length\n        num_workers: Number of worker processes for data loading\n        shuffle_train: Whether to shuffle training data\n    \n    Returns:\n        train_loader, val_loader, test_loader\n    \"\"\"\n  \n    # create datasets\n    train_dataset = CaptionDataset(\n        train_df, \n        train_features, \n        vocabulary, \n        max_length=max_length,\n        training=True\n    )\n    \n    val_dataset = CaptionDataset(\n        val_df, \n        val_features, \n        vocabulary, \n        max_length=\n        \\,\n        training=False\n    )\n    \n    test_dataset = CaptionDataset(\n        test_df, \n        test_features, \n        vocabulary, \n        max_length=max_length,\n        training=False\n    )\n    \n    # create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle= ,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    print(f\"\\nDataLoader Configuration:\")\n    print(f\"  Batch size: {batch_size}\")\n    print(f\"  Num workers: {num_workers}\")\n    print(f\"  Max caption length: {max_length}\")\n    \n    print(f\"\\nDataLoader Sizes:\")\n    print(f\"  Train batches: {len(train_loader)}\")\n    print(f\"  Val batches: {len(val_loader)}\")\n    print(f\"  Test batches: {len(test_loader)}\")\n    \n    return train_loader, val_loader, test_loader\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Decoder-Transformer Implementation","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\n\nclass ImageCaptioningTransformer(nn.Module):\n \n    def __init__(self, vocab_size, embed_dim=512, num_heads=8, \n                 num_layers=4, image_feat_dim=4096, max_len=17,dropout=0.1):\n        super().__init__()\n        \n        self.embed_dim = embed_dim\n        self.max_len = max_len\n        self.vocab_size = vocab_size\n        \n        self.image_embed = nn.Sequential(\n            nn.Linear(image_feat_dim, embed_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        self.word_embed = nn.Embedding(vocab_size, embed_dim)\n        self.positional_embed = nn.Embedding(max_len+1, embed_dim) # +1 for the image\n\n        self.embed_dropout = nn.Dropout(dropout)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=2048,\n            dropout=dropout,\n            activation='relu',\n            batch_first=True, \n            norm_first=False  \n        )\n\n        self.transformer = nn.TransformerEncoder(\n            encoder_layer=encoder_layer,\n            num_layers = num_layers\n        )\n        \n        self.output_project= nn.Linear(embed_dim, vocab_size)\n        \n\n    def generate_causal_mask(self, seq_len, device):\n        \"\"\"\n        Generate causal mask: upper triangular matrix of -inf.\n        \n        \"\"\"\n        mask = torch.triu(\n            torch.ones(seq_len, seq_len, device=device) * float('-inf'),\n            diagonal=1\n        )\n        return mask\n    \n    def generate_padding_mask(self,captions,pad_idx=0):\n        return (captions == pad_idx)\n        \n    def forward(self,images,captions):\n        \"\"\"\n        forward pass\n        \"\"\"\n        device = captions.device\n        batch_size, seq_len = captions.shape\n\n        img_embed = self.image_embed(images).unsqueeze(1) # -> (batch, 1,512)\n\n        caption_embed = self.word_embed(captions)\n\n        sequence = torch.cat([img_embed,caption_embed], dim = 1)\n\n        positional_encoding = torch.arange(seq_len + 1,device=device).unsqueeze(0)\n        positional_encoding= self.positional_embed(positional_encoding)\n\n        sequence = sequence + postional_encoding\n        sequence = self.embed_dropout(sequence)\n        \n        causal_mask = self.generate_causal_mask(seq_len +1, device)\n        padding_mask = self.generate_padding_mask(captions)\n\n        img_padding_mask = torch.zeros(batch_size,1, dtype=bool, device=device)\n\n        padding_mask = torch.cat([img_padding_mask, padding_mask], dim = 1)\n\n        output = self.transformer(\n            sequence,\n            mask=causal_mask,          \n            src_key_padding_mask=padding_mask\n        )\n\n        logits = self.output_project(output)\n        \n        return logits\n\n\n    def generate(self,images, start_token_idx=1, end_token_idx =2):\n\n         \"\"\"\n        Generate captions autoregressively\n        \n        Start with [START]\n        Loop:\n          1. Get predictions for current sequence\n          2. Take last prediction\n          3. Sample/argmax next token\n          4. Append to sequence\n          5. Stop if END token or max_len reached\n        \"\"\"\n        self.eval()\n        batch_size = images.shape[0]\n        device = images.device\n        generated = torch.full((batch_size,1), start_token_idx,dtype= torch.long, device=device)\n\n        # track what sequences have finished\n        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n        with torch.no_grad():\n            for _ in range(self.max_len):\n                logits = self.forward(images, generated)\n\n                next_logits = logits[:,-1,:]\n                next_token = next_logits.argmax(dim=1,keepdim = True)\n\n                generated = torch.cat([generated, next_token], dim = 1)\n\n                finished = finished | (next_token.squeeze(-1) == end_token_idx)\n\n                # stop if all sequences have finished\n                if finished.all():\n                    break\n\n        return generated\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss function and padding mask","metadata":{}},{"cell_type":"code","source":"class MaskedCrossEntropyLoss(nn.Module):\n    def __init__(self,pad_idx=0):\n        self.pad_idx = pad_idx\n        self.criterion = nn.CrossEntropy(reduction='none')\n\n    def forward(self, logits, targets):\n        \"\"\"\n        logits (batch,seq_len,vocab_size)\n        targets (batch, seq_len, 1)\n        \"\"\"\n        batch_size,seq_len, vocab_size = logits.shape\n        device = logits.device\n        \n        logits_flat = logits.reshape(-1,vocab_size) # ( batch * seq_len, vocab_size)\n        targets_flat = targets.reshape(-1)  # (batch * seq_len)\n\n        \n        loss = self.criterion(logits_flat, targets_flat)\n\n        mask = (targets_flat != self.pad_idx).float()\n\n        loss_masked = loss * mask\n\n        return torch.sum(loss_masked) / torch.sum(mask)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T14:12:31.112995Z","iopub.execute_input":"2026-01-12T14:12:31.113256Z","iopub.status.idle":"2026-01-12T14:12:31.119800Z","shell.execute_reply.started":"2026-01-12T14:12:31.113232Z","shell.execute_reply":"2026-01-12T14:12:31.118818Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_54/3557761403.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class MaskedCrossEntropyLoss(nn.Module):\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"],"ename":"SyntaxError","evalue":"incomplete input (3557761403.py, line 1)","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"## Train step and validation","metadata":{}},{"cell_type":"code","source":"\ndef train_step(model, images, captions, lengths, criterion, optimizer, device):\n    \"\"\"\n    One training step with teacher forcing\n    \n    Remember:\n      - Input to model: captions\n      - Model output: logits at each position\n      - Predictions: logits[:, :-1, :] (drop last)\n      - Targets: captions (original)\n    \"\"\"\n\n    model.train()\n    # Move to gpu\n    images = images.to(device)\n    captions = captions.to(device)\n    lengths = lengths.to(device)\n    logits = model(images,captions) # we pass ground truth for \"teacher forcing\"\n\n    logits = logits[:,:-1,:]\n\n    loss = criterion(logits, captions,lengths)\n\n    updates = \n    \n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    \n    # Gradient clipping (prevent exploding gradients)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    \n    optimizer.step()\n    \n    return loss.item()\n\n\ndef validate(model, val_loader, criterion, device):\n    \"\"\"\n    Validation loop.\n    \n    Args:\n        model: ImageCaptioningTransformer model\n        val_loader: validation DataLoader\n        criterion: MaskedCrossEntropyLoss\n        device: device to run on\n    \n    Returns:\n        avg_loss: average validation loss\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for images, captions, lengths in val_loader:\n            images = images.to(device)\n            captions = captions.to(device)\n            lengths = lengths.to(device)\n            \n            # forward pass\n            logits = model(images, captions)\n            \n            # compute loss \n            predictions = logits[:, :-1, :]\n            targets = captions\n            \n            loss = criterion(predictions, targets, lengths)\n            \n            total_loss += loss.item()\n            num_batches += 1\n    \n    return total_loss / num_batches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, criterion, train_loader,optimizer,config,epoch):\n    model.train()\n\n    total_loss = 0\n    num_batches =0\n\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.num_epochs}')\n    for batch_idx, (images,captions,lengths) in enumerate(train_loader):\n       \n        loss = train_step(model, images, captions, lengths, \n                         criterion, optimizer, config.device)\n        \n        total_loss+=loss\n        num_batches+=1\n\n        if (batch_idx + 1) % config.print_every == 0:\n            print(f'\\n[Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}] '\n                  f'Loss: {loss:.4f}, Avg Loss: {total_loss/num_batches:.4f}')\n    return total_loss/ num_batches\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    \"\"\"\n    training configuration\n    \"\"\"\n   # model\n    vocab_size = 9221\n    embed_dim = 512\n    num_heads = 8\n    num_layers = 4\n    image_feat_dim = 4096\n    max_len = 17\n    dropout = 0.1\n    \n    # training\n    batch_size = 64\n    num_epochs = 30\n    learning_rate = 5e-5\n    weight_decay = 1e-4\n    base_dir =\"/kaggle/working\"\n    save_dir=\"/checkpoints\"\n    best_dir = \"/best_model\"\n    save_every= 10 \n    \n    \n    # device\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n\n\n\ndef train(config):\n    \"\"\"\n    -. we need to call the train_step function for n epochs for all batches\n    - \n    \n    \"\"\"\n    train_df = pd.read_pickle('train_captions.pkl')\n    val_df = pd.read_pickle('val_captions.pkl')\n\n    train_features = np.load('coco_train_vgg16_features.npy', allow_pickle=True).item()\n    val_features = np.load('coco_val_vgg16_features.npy',allow_pickle = True).item()\n    \n    vocab = build_vocabulary(train_df,config.vocab_size)\n    \n    train_loader, val_loader, _ = create_dataloaders(# test_features and test_df are not used during training, so i used val_df as a dummy\n        train_df,val_df,val_df,train_features,val_features,val_features,vocab,config.batch_size\n    )\n\n    model = ImageCaptioningTransformer(config.vocab_size, \n                                       config.embed_dim,\n                                       config.num_heads,\n                                       config.num_layers, \n                                       config.image_feat_dim,\n                                       config.max_len,\n                                       config.dropout\n                                      ).to(config.device)\n    \n\n    optimizer = torch.optim.RMSprop(model.parameters(), \n                                    lr=config.learning_rate)\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=config.num_epochs\n    )\n\n    criterion = MaskedCrossEntropyLoss()\n    best_val_loss = float('inf')\n    train_losses = []\n    val_losses = []\n\n    os.makedirs(os.path.join(config.base_dir, config.save_dir),exist_ok=True)\n    \n    for epoch in range(config.num_epochs):\n        epoch_start= time.time()\n        \n        train_loss = train_epoch(model, criterion, train_loader,optimizer,config,epoch)\n        train_losses.append(train_loss)\n\n        val_loss = validate(model,val_loader,criterion,config.device)\n        val_losses.append(val_loss)\n        \n        scheduler.step()\n\n        epoch_time = time.time() - epoch_start\n\n        if (epoch +1) % config.save_every ==0:\n             checkpoint_path = os.path.join(\n                config.base_dir,\n                config.save_dir,\n                f'checkpoint_epoch_{epoch+1}.pt'\n            )\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'config': config\n            }, checkpoint_path)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            checkpoint_path = os.path.join(\n                config.base_dir,\n                config.save_dir,\n                f'best_model.pt'\n            )\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'config': config\n            }, checkpoint_path)\n        \n    history = {\n        'train_losses': train_losses,\n        'val_losses': val_losses\n    }\n    \n    history_path = os.path.join(config.base_dir,config.save_dir, 'training_history.pkl')\n    with open(history_path, 'wb') as f:\n        pickle.dump(history, f)\n        \n    print(\"Training history saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}