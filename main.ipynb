{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Data loading and feature extraction","metadata":{}},{"cell_type":"markdown","source":"Creating DataFrame for training","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nimport torchvision.models as models\n\n# Load the JSON file\nwith open('/kaggle/input/coco-image-caption/annotations_trainval2014/annotations/captions_train2014.json', 'r') as f:\n    coco_data = json.load(f)\n\n# Convert to DataFrames\nannotations_df = pd.DataFrame(coco_data['annotations'])\nimages_df = pd.DataFrame(coco_data['images'])\n\n\ndf = annotations_df.merge(images_df, left_on='image_id', right_on='id', \n                          suffixes=('_ann', '_img'))\n\n# Select relevant columns\ndf = df[['image_id', 'file_name', 'caption', 'height', 'width']]\n\n\n\ncaptions_grouped = annotations_df.groupby('image_id')['caption'].apply(list).reset_index()\ncaptions_grouped.columns = ['image_id', 'captions']\n\ngrouped_df = captions_grouped.merge(images_df, left_on='image_id', right_on='id')\ngrouped_df = final_df[['image_id', 'file_name', 'captions', 'height', 'width']]\nprint(grouped_df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Pretrained VGG-16 and ResNet loading","metadata":{}},{"cell_type":"code","source":"vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nvgg16.classifier = vgg16.classifier[:-1]  # Removes layer [6]\nresnet101 = models.resnet101(pretrained=True)\nresnet152 = models.resnet152(pretrained=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nvgg16 =vgg16.to(device)\nresnet101 = resnet101.to(device)\nresnet152 = resnet152.to(device)\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\ndef extract_features(image_path,model):\n    img = Image.open(image_path).convert('RGB')\n    img = transform(img).unsqueeze(0).cuda()\n    \n    with torch.no_grad():\n        features = model(img)\n    \n    return features.cpu().numpy()\n\n# Extract and save features for all COCO images\nfeatures_dict = {vgg16:{},resnet101:{},resnet152:{}}\n\nfor row in tqdm(grouped_df[['file_name','image_id']].itertuples(index=False)):\n    img_filename = row.file_name\n    img_id = row.image_id\n    img_path = f'/kaggle/input/coco-image-caption/train2014/train2014/{img_filename}'\n    for model in features_dict.keys():\n        features = extract_features(img_path, model)\n        features_dict[model][img_id] = features\n# Save extracted features\nnp.save('coco_vgg16_features.npy', features_dict[vgg16])\nnp.save('coco_resnet101_features.npy', features_dict[resnet101])\nnp.save('coco_resnet152_features.npy', features_dict[resnet152])\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feature extraction and saving","metadata":{}},{"cell_type":"markdown","source":"Build complete dataset","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nimport os\n\n# ============================================================================\n# 1. LOAD KARPATHY SPLIT\n# ============================================================================\n\nprint(\"Loading Karpathy split...\")\nkarpathy_file = '/kaggle/input/karpathy-splits/dataset_coco.json'\n\nif not os.path.exists(karpathy_file):\n    raise FileNotFoundError(f\"Karpathy split not found at: {karpathy_file}\")\n\nwith open(karpathy_file, 'r') as f:\n    karpathy_data = json.load(f)\n\nprint(f\"✓ Loaded Karpathy split: {len(karpathy_data['images'])} total images\")\n\n# ============================================================================\n# 2. ORGANIZE DATA BY SPLIT\n# ============================================================================\n\ndef organize_by_split(karpathy_data):\n    \"\"\"Organize images and captions by train/val/test split\"\"\"\n    splits = {'train': [], 'val': [], 'test': []}\n    \n    for img_data in karpathy_data['images']:\n        split = img_data['split']\n        \n        # Handle 'restval' - add to training (standard practice)\n        if split == 'restval':\n            split = 'train'\n        \n        if split in ['train', 'val', 'test']:\n            # Extract relevant info\n            # Note: Karpathy uses images from BOTH train2014 and val2014 folders\n            image_info = {\n                'image_id': img_data['cocoid'],\n                'file_name': img_data['filename'],  # e.g., COCO_train2014_xxx.jpg or COCO_val2014_xxx.jpg\n                'split': split,\n                'captions': [sent['raw'] for sent in img_data['sentences']]\n            }\n            splits[split].append(image_info)\n    \n    return splits\n\nprint(\"\\nOrganizing data by Karpathy split...\")\nsplits_data = organize_by_split(karpathy_data)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"KARPATHY SPLIT DISTRIBUTION\")\nprint(f\"{'='*70}\")\nprint(f\"Train images: {len(splits_data['train']):,}\")\nprint(f\"Val images:   {len(splits_data['val']):,}\")\nprint(f\"Test images:  {len(splits_data['test']):,}\")\nprint(f\"{'='*70}\")\n\n# Convert to DataFrames\ntrain_df = pd.DataFrame(splits_data['train'])\nval_df = pd.DataFrame(splits_data['val'])\ntest_df = pd.DataFrame(splits_data['test'])\n\n# Check which COCO folders the images come from\nprint(\"\\nAnalyzing image sources...\")\ntrain_sources = train_df['file_name'].str.contains('train2014').sum()\ntrain_val_sources = train_df['file_name'].str.contains('val2014').sum()\nprint(f\"Train split:\")\nprint(f\"  From train2014/: {train_sources:,} images\")\nprint(f\"  From val2014/:   {train_val_sources:,} images\")\n\nval_sources = val_df['file_name'].str.contains('train2014').sum()\nval_val_sources = val_df['file_name'].str.contains('val2014').sum()\nprint(f\"Val split:\")\nprint(f\"  From train2014/: {val_sources:,} images\")\nprint(f\"  From val2014/:   {val_val_sources:,} images\")\n\ntest_sources = test_df['file_name'].str.contains('train2014').sum()\ntest_val_sources = test_df['file_name'].str.contains('val2014').sum()\nprint(f\"Test split:\")\nprint(f\"  From train2014/: {test_sources:,} images\")\nprint(f\"  From val2014/:   {test_val_sources:,} images\")\n\nprint(\"\\nSample filenames:\")\nprint(f\"Train: {train_df.iloc[0]['file_name']}\")\nprint(f\"Val:   {val_df.iloc[0]['file_name']}\")\nprint(f\"Test:  {test_df.iloc[0]['file_name']}\")\n\n# ============================================================================\n# 3. LOAD PRETRAINED MODELS\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"LOADING PRETRAINED MODELS\")\nprint(f\"{'='*70}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\n# VGG16 - fc7 features (4096-dim) - matching the paper\nprint(\"\\n[1/2] Loading VGG16...\")\nvgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nvgg16.classifier = vgg16.classifier[:-1]  # Remove last layer to get fc7\nvgg16 = vgg16.to(device)\nvgg16.eval()\nprint(\"✓ VGG16 loaded (fc7: 4096-dim)\")\n\n# ResNet101 (optional - for better performance)\nprint(\"\\n[2/2] Loading ResNet101...\")\nresnet101 = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\nresnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])  # Remove FC layer\nresnet101 = resnet101.to(device)\nresnet101.eval()\nprint(\"✓ ResNet101 loaded (avgpool: 2048-dim)\")\n\nprint(f\"\\n{'='*70}\")\n\n# ============================================================================\n# 4. IMAGE PREPROCESSING\n# ============================================================================\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\n# ============================================================================\n# 5. FEATURE EXTRACTION FUNCTION\n# ============================================================================\n\ndef extract_features(image_path, model):\n    \"\"\"Extract features from a single image\"\"\"\n    try:\n        img = Image.open(image_path).convert('RGB')\n        img = transform(img).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            features = model(img)\n            # Flatten if needed (for ResNet)\n            if len(features.shape) > 2:\n                features = features.squeeze()\n        \n        return features.cpu().numpy()\n    except Exception as e:\n        return None\n\n# ============================================================================\n# 6. HELPER FUNCTION TO FIND IMAGE PATH\n# ============================================================================\n\ndef get_image_path(filename, base_paths):\n    \"\"\"\n    Find the correct path for an image.\n    Karpathy split uses images from both train2014 and val2014 folders.\n    \n    Args:\n        filename: e.g., 'COCO_train2014_000000123456.jpg' or 'COCO_val2014_000000123456.jpg'\n        base_paths: dict with 'train2014' and 'val2014' keys\n    \"\"\"\n    # Determine which folder based on filename\n    if 'train2014' in filename:\n        folder = 'train2014'\n    elif 'val2014' in filename:\n        folder = 'val2014'\n    else:\n        return None\n    \n    img_path = f\"{base_paths[folder]}/{filename}\"\n    \n    if os.path.exists(img_path):\n        return img_path\n    else:\n        return None\n\n# ============================================================================\n# 7. EXTRACT AND SAVE FEATURES FOR EACH SPLIT\n# ============================================================================\n\ndef extract_and_save_split_features(df, split_name, base_paths, models_dict):\n    \"\"\"Extract features for all images in a split and save\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"EXTRACTING {split_name.upper()} FEATURES ({len(df):,} images)\")\n    print(f\"{'='*70}\")\n    \n    features_by_model = {model_name: {} for model_name in models_dict.keys()}\n    missing_images = []\n    processed = 0\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{split_name}\"):\n        img_id = row['image_id']\n        img_filename = row['file_name']\n        \n        # Find the correct image path (from train2014 or val2014)\n        img_path = get_image_path(img_filename, base_paths)\n        \n        if img_path is None:\n            missing_images.append(img_filename)\n            continue\n        \n        # Extract features with each model\n        for model_name, model in models_dict.items():\n            features = extract_features(img_path, model)\n            if features is not None:\n                features_by_model[model_name][img_id] = features\n        \n        processed += 1\n    \n    # Report statistics\n    print(f\"\\n✓ Processed: {processed:,} images\")\n    if missing_images:\n        print(f\"⚠ Missing: {len(missing_images):,} images\")\n        if len(missing_images) <= 10:\n            print(f\"  Missing files: {missing_images}\")\n        else:\n            print(f\"  First 5 missing: {missing_images[:5]}\")\n    \n    # Save features for each model\n    print(f\"\\nSaving features...\")\n    for model_name, features_dict in features_by_model.items():\n        save_path = f'coco_{split_name}_{model_name}_features.npy'\n        np.save(save_path, features_dict)\n        feature_shape = list(features_dict.values())[0].shape if features_dict else 'N/A'\n        print(f\"  ✓ {save_path}: {len(features_dict):,} images, shape={feature_shape}\")\n    \n    return features_by_model\n\n# ============================================================================\n# 8. SETUP IMAGE PATHS (BOTH train2014 AND val2014)\n# ============================================================================\n\nBASE_PATHS = {\n    'train2014': '/kaggle/input/coco2014/train2014/train2014',\n    'val2014': '/kaggle/input/coco2014/val2014/val2014'\n}\n\n# Verify paths exist\nprint(f\"\\n{'='*70}\")\nprint(\"VERIFYING IMAGE DIRECTORIES\")\nprint(f\"{'='*70}\")\n\nfor folder, path in BASE_PATHS.items():\n    if not os.path.exists(path):\n        print(f\"⚠ Warning: Path not found: {path}\")\n    else:\n        num_files = len([f for f in os.listdir(path) if f.endswith('.jpg')])\n        print(f\"✓ {folder}: {path}\")\n        print(f\"  Files: {num_files:,} .jpg images\")\n\n# ============================================================================\n# 9. RUN FEATURE EXTRACTION\n# ============================================================================\n\n# Define models to use\nmodels_dict = {\n    'vgg16': vgg16,\n    'resnet101': resnet101,\n}\n\n# Extract features for each split\nprint(f\"\\n{'='*70}\")\nprint(\"STARTING FEATURE EXTRACTION\")\nprint(f\"{'='*70}\")\n\ntrain_features = extract_and_save_split_features(import json\nimport pandas as pd\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nimport os\n\n# ============================================================================\n# 1. LOAD KARPATHY SPLIT\n# ============================================================================\n\nprint(\"Loading Karpathy split...\")\nkarpathy_file = '/kaggle/input/karpathy-splits/dataset_coco.json'\n\nif not os.path.exists(karpathy_file):\n    raise FileNotFoundError(f\"Karpathy split not found at: {karpathy_file}\")\n\nwith open(karpathy_file, 'r') as f:\n    karpathy_data = json.load(f)\n\nprint(f\"✓ Loaded Karpathy split: {len(karpathy_data['images'])} total images\")\n\n# ============================================================================\n# 2. ORGANIZE DATA BY SPLIT\n# ============================================================================\n\ndef organize_by_split(karpathy_data):\n    \"\"\"Organize images and captions by train/val/test split\"\"\"\n    splits = {'train': [], 'val': [], 'test': []}\n    \n    for img_data in karpathy_data['images']:\n        split = img_data['split']\n        \n        # Handle 'restval' - add to training (standard practice)\n        if split == 'restval':\n            split = 'train'\n        \n        if split in ['train', 'val', 'test']:\n            # Extract relevant info\n            # Note: Karpathy uses images from BOTH train2014 and val2014 folders\n            image_info = {\n                'image_id': img_data['cocoid'],\n                'file_name': img_data['filename'],  # e.g., COCO_train2014_xxx.jpg or COCO_val2014_xxx.jpg\n                'split': split,\n                'captions': [sent['raw'] for sent in img_data['sentences']]\n            }\n            splits[split].append(image_info)\n    \n    return splits\n\nprint(\"\\nOrganizing data by Karpathy split...\")\nsplits_data = organize_by_split(karpathy_data)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"KARPATHY SPLIT DISTRIBUTION\")\nprint(f\"{'='*70}\")\nprint(f\"Train images: {len(splits_data['train']):,}\")\nprint(f\"Val images:   {len(splits_data['val']):,}\")\nprint(f\"Test images:  {len(splits_data['test']):,}\")\nprint(f\"{'='*70}\")\n\n# Convert to DataFrames\ntrain_df = pd.DataFrame(splits_data['train'])\nval_df = pd.DataFrame(splits_data['val'])\ntest_df = pd.DataFrame(splits_data['test'])\n\n# Check which COCO folders the images come from\nprint(\"\\nAnalyzing image sources...\")\ntrain_sources = train_df['file_name'].str.contains('train2014').sum()\ntrain_val_sources = train_df['file_name'].str.contains('val2014').sum()\nprint(f\"Train split:\")\nprint(f\"  From train2014/: {train_sources:,} images\")\nprint(f\"  From val2014/:   {train_val_sources:,} images\")\n\nval_sources = val_df['file_name'].str.contains('train2014').sum()\nval_val_sources = val_df['file_name'].str.contains('val2014').sum()\nprint(f\"Val split:\")\nprint(f\"  From train2014/: {val_sources:,} images\")\nprint(f\"  From val2014/:   {val_val_sources:,} images\")\n\ntest_sources = test_df['file_name'].str.contains('train2014').sum()\ntest_val_sources = test_df['file_name'].str.contains('val2014').sum()\nprint(f\"Test split:\")\nprint(f\"  From train2014/: {test_sources:,} images\")\nprint(f\"  From val2014/:   {test_val_sources:,} images\")\n\nprint(\"\\nSample filenames:\")\nprint(f\"Train: {train_df.iloc[0]['file_name']}\")\nprint(f\"Val:   {val_df.iloc[0]['file_name']}\")\nprint(f\"Test:  {test_df.iloc[0]['file_name']}\")\n\n# ============================================================================\n# 3. LOAD PRETRAINED MODELS\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"LOADING PRETRAINED MODELS\")\nprint(f\"{'='*70}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\n# VGG16 - fc7 features (4096-dim) - matching the paper\nprint(\"\\n[1/2] Loading VGG16...\")\nvgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nvgg16.classifier = vgg16.classifier[:-1]  # Remove last layer to get fc7\nvgg16 = vgg16.to(device)\nvgg16.eval()\nprint(\"✓ VGG16 loaded (fc7: 4096-dim)\")\n\n# ResNet101 (optional - for better performance)\nprint(\"\\n[2/2] Loading ResNet101...\")\nresnet101 = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\nresnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])  # Remove FC layer\nresnet101 = resnet101.to(device)\nresnet101.eval()\nprint(\"✓ ResNet101 loaded (avgpool: 2048-dim)\")\n\nprint(f\"\\n{'='*70}\")\n\n# ============================================================================\n# 4. IMAGE PREPROCESSING\n# ============================================================================\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\n# ============================================================================\n# 5. FEATURE EXTRACTION FUNCTION\n# ============================================================================\n\ndef extract_features(image_path, model):\n    \"\"\"Extract features from a single image\"\"\"\n    try:\n        img = Image.open(image_path).convert('RGB')\n        img = transform(img).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            features = model(img)\n            # Flatten if needed (for ResNet)\n            if len(features.shape) > 2:\n                features = features.squeeze()\n        \n        return features.cpu().numpy()\n    except Exception as e:\n        return None\n\n# ============================================================================\n# 6. HELPER FUNCTION TO FIND IMAGE PATH\n# ============================================================================\n\ndef get_image_path(filename, base_paths):\n    \"\"\"\n    Find the correct path for an image.\n    Karpathy split uses images from both train2014 and val2014 folders.\n    \n    Args:\n        filename: e.g., 'COCO_train2014_000000123456.jpg' or 'COCO_val2014_000000123456.jpg'\n        base_paths: dict with 'train2014' and 'val2014' keys\n    \"\"\"\n    # Determine which folder based on filename\n    if 'train2014' in filename:\n        folder = 'train2014'\n    elif 'val2014' in filename:\n        folder = 'val2014'\n    else:\n        return None\n    \n    img_path = f\"{base_paths[folder]}/{filename}\"\n    \n    if os.path.exists(img_path):\n        return img_path\n    else:\n        return None\n\n# ============================================================================\n# 7. EXTRACT AND SAVE FEATURES FOR EACH SPLIT\n# ============================================================================\n\ndef extract_and_save_split_features(df, split_name, base_paths, models_dict):\n    \"\"\"Extract features for all images in a split and save\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"EXTRACTING {split_name.upper()} FEATURES ({len(df):,} images)\")\n    print(f\"{'='*70}\")\n    \n    features_by_model = {model_name: {} for model_name in models_dict.keys()}\n    missing_images = []\n    processed = 0\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{split_name}\"):\n        img_id = row['image_id']\n        img_filename = row['file_name']\n        \n        # Find the correct image path (from train2014 or val2014)\n        img_path = get_image_path(img_filename, base_paths)\n        \n        if img_path is None:\n            missing_images.append(img_filename)\n            continue\n        \n        # Extract features with each model\n        for model_name, model in models_dict.items():\n            features = extract_features(img_path, model)\n            if features is not None:\n                features_by_model[model_name][img_id] = features\n        \n        processed += 1\n    \n    # Report statistics\n    print(f\"\\n✓ Processed: {processed:,} images\")\n    if missing_images:\n        print(f\"⚠ Missing: {len(missing_images):,} images\")\n        if len(missing_images) <= 10:\n            print(f\"  Missing files: {missing_images}\")\n        else:\n            print(f\"  First 5 missing: {missing_images[:5]}\")\n    \n    # Save features for each model\n    print(f\"\\nSaving features...\")\n    for model_name, features_dict in features_by_model.items():\n        save_path = f'coco_{split_name}_{model_name}_features.npy'\n        np.save(save_path, features_dict)\n        feature_shape = list(features_dict.values())[0].shape if features_dict else 'N/A'\n        print(f\"  ✓ {save_path}: {len(features_dict):,} images, shape={feature_shape}\")\n    \n    return features_by_model\n\n# ============================================================================\n# 8. SETUP IMAGE PATHS (BOTH train2014 AND val2014)\n# ============================================================================\n\nBASE_PATHS = {\n    'train2014': '/kaggle/input/coco2014/train2014/train2014',\n    'val2014': '/kaggle/input/coco2014/val2014/val2014'\n}\n\n# Verify paths exist\nprint(f\"\\n{'='*70}\")\nprint(\"VERIFYING IMAGE DIRECTORIES\")\nprint(f\"{'='*70}\")\n\nfor folder, path in BASE_PATHS.items():\n    if not os.path.exists(path):\n        print(f\"⚠ Warning: Path not found: {path}\")\n    else:\n        num_files = len([f for f in os.listdir(path) if f.endswith('.jpg')])\n        print(f\"✓ {folder}: {path}\")\n        print(f\"  Files: {num_files:,} .jpg images\")\n\n# ============================================================================\n# 9. RUN FEATURE EXTRACTION\n# ============================================================================\n\n# Define models to use\nmodels_dict = {\n    'vgg16': vgg16,\n    'resnet101': resnet101,\n}\n\n# Extract features for each split\nprint(f\"\\n{'='*70}\")\nprint(\"STARTING FEATURE EXTRACTION\")\nprint(f\"{'='*70}\")\n\ntrain_features = extract_and_save_split_features(train_df, 'train', BASE_PATHS, models_dict)\nval_features = extract_and_save_split_features(val_df, 'val', BASE_PATHS, models_dict)\ntest_features = extract_and_save_split_features(test_df, 'test', BASE_PATHS, models_dict)\n\n# ============================================================================\n# 10. SAVE DATAFRAMES WITH METADATA\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"SAVING CAPTION METADATA\")\nprint(f\"{'='*70}\")\n\ntrain_df.to_pickle('train_captions.pkl')\nval_df.to_pickle('val_captions.pkl')\ntest_df.to_pickle('test_captions.pkl')\nprint(\"✓ Saved .pkl files\")\n\ntrain_df.to_csv('train_captions.csv', index=False)\nval_df.to_csv('val_captions.csv', index=False)\ntest_df.to_csv('test_captions.csv', index=False)\nprint(\"✓ Saved .csv files\")\n\n# ============================================================================\n# 11. FINAL SUMMARY\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"FEATURE EXTRACTION COMPLETE! ✓\")\nprint(f\"{'='*70}\")\n\nprint(f\"\\nDataset sizes:\")\nprint(f\"  Train: {len(train_df):,} images\")\nprint(f\"  Val:   {len(val_df):,} images\")\nprint(f\"  Test:  {len(test_df):,} images\")\nprint(f\"  TOTAL: {len(train_df) + len(val_df) + len(test_df):,} images\")\n\nprint(f\"\\nSaved files:\")\nprint(f\"  Captions:\")\nprint(f\"    - train_captions.pkl / .csv\")\nprint(f\"    - val_captions.pkl / .csv\")\nprint(f\"    - test_captions.pkl / .csv\")\nprint(f\"  Features:\")\nprint(f\"    - coco_train_vgg16_features.npy\")\nprint(f\"    - coco_train_resnet101_features.npy\")\nprint(f\"    - coco_val_vgg16_features.npy\")\nprint(f\"    - coco_val_resnet101_features.npy\")\nprint(f\"    - coco_test_vgg16_features.npy\")\nprint(f\"    - coco_test_resnet101_features.npy\")\n\n# ============================================================================\n# 12. VERIFICATION\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"VERIFICATION\")\nprint(f\"{'='*70}\")\n\n# Load and verify\nloaded_vgg_features = np.load('coco_train_vgg16_features.npy', allow_pickle=True).item()\nloaded_resnet_features = np.load('coco_train_resnet101_features.npy', allow_pickle=True).item()\nloaded_df = pd.read_pickle('train_captions.pkl')\n\nprint(f\"\\nTrain set verification:\")\nprint(f\"  VGG16 features: {len(loaded_vgg_features):,} images\")\nprint(f\"    Shape: {list(loaded_vgg_features.values())[0].shape}\")\nprint(f\"  ResNet101 features: {len(loaded_resnet_features):,} images\")\nprint(f\"    Shape: {list(loaded_resnet_features.values())[0].shape}\")\nprint(f\"  Captions DataFrame: {len(loaded_df):,} rows\")\n\nprint(f\"\\nSample image (ID={loaded_df.iloc[0]['image_id']}):\")\nprint(f\"  File: {loaded_df.iloc[0]['file_name']}\")\nprint(f\"  Captions:\")\nfor i, cap in enumerate(loaded_df.iloc[0]['captions']):\n    print(f\"    {i+1}. {cap}\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"NEXT STEP: Build vocabulary from training captions\")\nprint(f\"{'='*70}\"), 'train', BASE_PATHS, models_dict)\nval_features = extract_and_save_split_features(val_df, 'val', BASE_PATHS, models_dict)\ntest_features = extract_and_save_split_features(test_df, 'test', BASE_PATHS, models_dict)\n\n# ============================================================================\n# 10. SAVE DATAFRAMES WITH METADATA\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"SAVING CAPTION METADATA\")\nprint(f\"{'='*70}\")\n\ntrain_df.to_pickle('train_captions.pkl')\nval_df.to_pickle('val_captions.pkl')\ntest_df.to_pickle('test_captions.pkl')\nprint(\"✓ Saved .pkl files\")\n\ntrain_df.to_csv('train_captions.csv', index=False)\nval_df.to_csv('val_captions.csv', index=False)\ntest_df.to_csv('test_captions.csv', index=False)\nprint(\"✓ Saved .csv files\")\n\n# ============================================================================\n# 11. FINAL SUMMARY\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"FEATURE EXTRACTION COMPLETE! ✓\")\nprint(f\"{'='*70}\")\n\nprint(f\"\\nDataset sizes:\")\nprint(f\"  Train: {len(train_df):,} images\")\nprint(f\"  Val:   {len(val_df):,} images\")\nprint(f\"  Test:  {len(test_df):,} images\")\nprint(f\"  TOTAL: {len(train_df) + len(val_df) + len(test_df):,} images\")\n\nprint(f\"\\nSaved files:\")\nprint(f\"  Captions:\")\nprint(f\"    - train_captions.pkl / .csv\")\nprint(f\"    - val_captions.pkl / .csv\")\nprint(f\"    - test_captions.pkl / .csv\")\nprint(f\"  Features:\")\nprint(f\"    - coco_train_vgg16_features.npy\")\nprint(f\"    - coco_train_resnet101_features.npy\")\nprint(f\"    - coco_val_vgg16_features.npy\")\nprint(f\"    - coco_val_resnet101_features.npy\")\nprint(f\"    - coco_test_vgg16_features.npy\")\nprint(f\"    - coco_test_resnet101_features.npy\")\n\n# ============================================================================\n# 12. VERIFICATION\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"VERIFICATION\")\nprint(f\"{'='*70}\")\n\n# Load and verify\nloaded_vgg_features = np.load('coco_train_vgg16_features.npy', allow_pickle=True).item()\nloaded_resnet_features = np.load('coco_train_resnet101_features.npy', allow_pickle=True).item()\nloaded_df = pd.read_pickle('train_captions.pkl')\n\nprint(f\"\\nTrain set verification:\")\nprint(f\"  VGG16 features: {len(loaded_vgg_features):,} images\")\nprint(f\"    Shape: {list(loaded_vgg_features.values())[0].shape}\")\nprint(f\"  ResNet101 features: {len(loaded_resnet_features):,} images\")\nprint(f\"    Shape: {list(loaded_resnet_features.values())[0].shape}\")\nprint(f\"  Captions DataFrame: {len(loaded_df):,} rows\")\n\nprint(f\"\\nSample image (ID={loaded_df.iloc[0]['image_id']}):\")\nprint(f\"  File: {loaded_df.iloc[0]['file_name']}\")\nprint(f\"  Captions:\")\nfor i, cap in enumerate(loaded_df.iloc[0]['captions']):\n    print(f\"    {i+1}. {cap}\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"NEXT STEP: Build vocabulary from training captions\")\nprint(f\"{'='*70}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T13:40:46.746835Z","iopub.execute_input":"2026-01-11T13:40:46.747744Z","iopub.status.idle":"2026-01-11T15:12:26.099724Z","shell.execute_reply.started":"2026-01-11T13:40:46.747709Z","shell.execute_reply":"2026-01-11T15:12:26.098968Z"}},"outputs":[{"name":"stdout","text":"Loading Karpathy split...\n✓ Loaded Karpathy split: 123287 total images\n\nOrganizing data by Karpathy split...\n\n======================================================================\nKARPATHY SPLIT DISTRIBUTION\n======================================================================\nTrain images: 113,287\nVal images:   5,000\nTest images:  5,000\n======================================================================\n\nAnalyzing image sources...\nTrain split:\n  From train2014/: 82,783 images\n  From val2014/:   30,504 images\nVal split:\n  From train2014/: 0 images\n  From val2014/:   5,000 images\nTest split:\n  From train2014/: 0 images\n  From val2014/:   5,000 images\n\nSample filenames:\nTrain: COCO_val2014_000000522418.jpg\nVal:   COCO_val2014_000000184613.jpg\nTest:  COCO_val2014_000000391895.jpg\n\n======================================================================\nLOADING PRETRAINED MODELS\n======================================================================\nDevice: cuda\n\n[1/2] Loading VGG16...\n✓ VGG16 loaded (fc7: 4096-dim)\n\n[2/2] Loading ResNet101...\n✓ ResNet101 loaded (avgpool: 2048-dim)\n\n======================================================================\n\n======================================================================\nVERIFYING IMAGE DIRECTORIES\n======================================================================\n✓ train2014: /kaggle/input/coco2014/train2014/train2014\n  Files: 82,783 .jpg images\n✓ val2014: /kaggle/input/coco2014/val2014/val2014\n  Files: 40,504 .jpg images\n\n======================================================================\nSTARTING FEATURE EXTRACTION\n======================================================================\n\n======================================================================\nEXTRACTING TRAIN FEATURES (113,287 images)\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 113287/113287 [1:23:46<00:00, 22.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✓ Processed: 113,287 images\n\nSaving features...\n  ✓ coco_train_vgg16_features.npy: 113,287 images, shape=(1, 4096)\n  ✓ coco_train_resnet101_features.npy: 113,287 images, shape=(2048,)\n\n======================================================================\nEXTRACTING VAL FEATURES (5,000 images)\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"val: 100%|██████████| 5000/5000 [03:44<00:00, 22.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✓ Processed: 5,000 images\n\nSaving features...\n  ✓ coco_val_vgg16_features.npy: 5,000 images, shape=(1, 4096)\n  ✓ coco_val_resnet101_features.npy: 5,000 images, shape=(2048,)\n\n======================================================================\nEXTRACTING TEST FEATURES (5,000 images)\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"test: 100%|██████████| 5000/5000 [03:43<00:00, 22.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✓ Processed: 5,000 images\n\nSaving features...\n  ✓ coco_test_vgg16_features.npy: 5,000 images, shape=(1, 4096)\n  ✓ coco_test_resnet101_features.npy: 5,000 images, shape=(2048,)\n\n======================================================================\nSAVING CAPTION METADATA\n======================================================================\n✓ Saved .pkl files\n✓ Saved .csv files\n\n======================================================================\nFEATURE EXTRACTION COMPLETE! ✓\n======================================================================\n\nDataset sizes:\n  Train: 113,287 images\n  Val:   5,000 images\n  Test:  5,000 images\n  TOTAL: 123,287 images\n\nSaved files:\n  Captions:\n    - train_captions.pkl / .csv\n    - val_captions.pkl / .csv\n    - test_captions.pkl / .csv\n  Features:\n    - coco_train_vgg16_features.npy\n    - coco_train_resnet101_features.npy\n    - coco_val_vgg16_features.npy\n    - coco_val_resnet101_features.npy\n    - coco_test_vgg16_features.npy\n    - coco_test_resnet101_features.npy\n\n======================================================================\nVERIFICATION\n======================================================================\n\nTrain set verification:\n  VGG16 features: 113,287 images\n    Shape: (1, 4096)\n  ResNet101 features: 113,287 images\n    Shape: (2048,)\n  Captions DataFrame: 113,287 rows\n\nSample image (ID=522418):\n  File: COCO_val2014_000000522418.jpg\n  Captions:\n    1. A woman wearing a net on her head cutting a cake. \n    2. A woman cutting a large white sheet cake.\n    3. A woman wearing a hair net cutting a large sheet cake.\n    4. there is a woman that is cutting a white cake\n    5. A woman marking a cake with the back of a chef's knife. \n\n======================================================================\nNEXT STEP: Build vocabulary from training captions\n======================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Vocabulary creation (9,221 words)\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport re\nfrom collections import Counter\nfrom tqdm import tqdm\n\n# ============================================================================\n# VOCABULARY BUILDER FOR IMAGE CAPTIONING\n# ============================================================================\n# This script builds a vocabulary from training captions, matching the paper's\n# approach: ~9,221 words + special tokens\n# ============================================================================\n\nclass Vocabulary:\n    \"\"\"Vocabulary class for image captioning\"\"\"\n    \n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.word_counts = Counter()\n        \n        # Special tokens (must be added first to get correct indices)\n        self.PAD_TOKEN = '<PAD>'\n        self.START_TOKEN = '<START>'\n        self.END_TOKEN = '<END>'\n        self.UNK_TOKEN = '<UNK>'\n        \n        # Initialize with special tokens\n        self.word2idx = {\n            self.PAD_TOKEN: 0,\n            self.START_TOKEN: 1,\n            self.END_TOKEN: 2,\n            self.UNK_TOKEN: 3\n        }\n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n        self.idx = 4  # Next available index\n    \n    def add_word(self, word):\n        \"\"\"Add a word to the vocabulary\"\"\"\n        if word not in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n    \n    def __len__(self):\n        return len(self.word2idx)\n    \n    def __call__(self, word):\n        \"\"\"Convert word to index\"\"\"\n        return self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])\n\n\ndef tokenize_caption(caption):\n    \"\"\"\n    Tokenize a caption:\n    - Convert to lowercase\n    - Remove punctuation (except hyphens in words)\n    - Split into words\n    \n    Args:\n        caption: str, raw caption text\n    \n    Returns:\n        list of tokens\n    \"\"\"\n    # Lowercase\n    caption = caption.lower()\n    \n    # Remove punctuation at end of words but keep apostrophes and hyphens\n    # This regex keeps alphanumeric, apostrophes, and hyphens\n    caption = re.sub(r'[^\\w\\s\\'-]', ' ', caption)\n    \n    # Split and remove extra whitespace\n    tokens = caption.split()\n    \n    # Remove empty strings\n    tokens = [t for t in tokens if t]\n    \n    return tokens\n\n\ndef build_vocabulary(train_captions_df, vocab_size=9221, min_word_freq=5):\n    \"\"\"\n    Build vocabulary from training captions.\n    \n    Args:\n        train_captions_df: pandas DataFrame with 'captions' column\n        vocab_size: target vocabulary size (excluding special tokens)\n        min_word_freq: minimum frequency for a word to be included\n    \n    Returns:\n        Vocabulary object\n    \"\"\"\n    print(f\"{'='*70}\")\n    print(f\"BUILDING VOCABULARY\")\n    print(f\"{'='*70}\")\n    \n    vocab = Vocabulary()\n    \n    # Count all words in training captions\n    print(f\"\\n[1/3] Tokenizing and counting words...\")\n    all_tokens = []\n    \n    for idx, row in tqdm(train_captions_df.iterrows(), \n                         total=len(train_captions_df),\n                         desc=\"Processing\"):\n        captions = row['captions']\n        \n        # Process each caption for this image (usually 5 captions per image)\n        for caption in captions:\n            tokens = tokenize_caption(caption)\n            all_tokens.extend(tokens)\n            vocab.word_counts.update(tokens)\n    \n    print(f\"✓ Total tokens processed: {len(all_tokens):,}\")\n    print(f\"✓ Unique words found: {len(vocab.word_counts):,}\")\n    \n    # Filter by minimum frequency\n    print(f\"\\n[2/3] Filtering words by frequency (min_freq={min_word_freq})...\")\n    filtered_words = {word: count for word, count in vocab.word_counts.items() \n                      if count >= min_word_freq}\n    print(f\"✓ Words with freq >= {min_word_freq}: {len(filtered_words):,}\")\n    \n    # Sort by frequency and take top vocab_size words\n    print(f\"\\n[3/3] Building vocabulary (size={vocab_size})...\")\n    most_common = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)\n    \n    # Take top vocab_size - 4 (to account for special tokens)\n    top_words = most_common[:vocab_size - 4]\n    \n    # Add to vocabulary\n    for word, count in tqdm(top_words, desc=\"Adding words\"):\n        vocab.add_word(word)\n    \n    print(f\"\\n✓ Vocabulary built successfully!\")\n    print(f\"  Total vocabulary size: {len(vocab)} words\")\n    print(f\"  Special tokens: {[vocab.PAD_TOKEN, vocab.START_TOKEN, vocab.END_TOKEN, vocab.UNK_TOKEN]}\")\n    print(f\"  Regular words: {len(vocab) - 4}\")\n    \n    return vocab\n\n\ndef analyze_vocabulary(vocab, train_df):\n    \"\"\"\n    Analyze vocabulary statistics\n    \n    Args:\n        vocab: Vocabulary object\n        train_df: training DataFrame\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"VOCABULARY STATISTICS\")\n    print(f\"{'='*70}\")\n    \n    # Most common words\n    print(f\"\\nTop 20 most common words:\")\n    most_common = vocab.word_counts.most_common(20)\n    for i, (word, count) in enumerate(most_common, 1):\n        print(f\"  {i:2d}. '{word}': {count:,} occurrences\")\n    \n    # Calculate coverage\n    total_tokens = sum(vocab.word_counts.values())\n    vocab_coverage = sum(count for word, count in vocab.word_counts.items() \n                         if word in vocab.word2idx)\n    coverage_pct = (vocab_coverage / total_tokens) * 100\n    \n    print(f\"\\nVocabulary Coverage:\")\n    print(f\"  Total tokens in training: {total_tokens:,}\")\n    print(f\"  Tokens covered by vocab: {vocab_coverage:,}\")\n    print(f\"  Coverage: {coverage_pct:.2f}%\")\n    \n    # Caption length statistics\n    print(f\"\\nCaption Length Statistics:\")\n    caption_lengths = []\n    for idx, row in train_df.iterrows():\n        for caption in row['captions']:\n            tokens = tokenize_caption(caption)\n            caption_lengths.append(len(tokens))\n    \n    print(f\"  Min length: {min(caption_lengths)} words\")\n    print(f\"  Max length: {max(caption_lengths)} words\")\n    print(f\"  Mean length: {np.mean(caption_lengths):.2f} words\")\n    print(f\"  Median length: {np.median(caption_lengths):.0f} words\")\n    print(f\"  Std dev: {np.std(caption_lengths):.2f} words\")\n    \n    # Distribution of caption lengths\n    print(f\"\\n  Length distribution:\")\n    length_bins = [0, 5, 10, 15, 20, 25, 100]\n    for i in range(len(length_bins) - 1):\n        count = sum(1 for l in caption_lengths if length_bins[i] < l <= length_bins[i+1])\n        pct = (count / len(caption_lengths)) * 100\n        print(f\"    {length_bins[i]+1}-{length_bins[i+1]} words: {count:,} ({pct:.1f}%)\")\n\n\ndef save_vocabulary(vocab, filepath='vocabulary.pkl'):\n    \"\"\"Save vocabulary to file\"\"\"\n    with open(filepath, 'wb') as f:\n        pickle.dump(vocab, f)\n    print(f\"\\n✓ Vocabulary saved to: {filepath}\")\n\n\ndef load_vocabulary(filepath='vocabulary.pkl'):\n    \"\"\"Load vocabulary from file\"\"\"\n    with open(filepath, 'rb') as f:\n        vocab = pickle.load(f)\n    print(f\"✓ Vocabulary loaded from: {filepath}\")\n    print(f\"  Size: {len(vocab)} words\")\n    return vocab\n\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\"*70)\n    print(\"IMAGE CAPTIONING - VOCABULARY BUILDER\")\n    print(\"=\"*70)\n    \n    # Load training captions\n    print(\"\\nLoading training captions...\")\n    train_df = pd.read_pickle('train_captions.pkl')\n    print(f\"✓ Loaded {len(train_df):,} training images\")\n    \n    # Build vocabulary\n    # Target size: 9,221 words (matching the paper)\n    # This is 9,217 regular words + 4 special tokens = 9,221 total\n    vocab = build_vocabulary(\n        train_df, \n        vocab_size=9221,  # Total vocabulary size (including special tokens)\n        min_word_freq=5   # Minimum word frequency threshold\n    )\n    \n    # Analyze vocabulary\n    analyze_vocabulary(vocab, train_df)\n    \n    # Save vocabulary\n    save_vocabulary(vocab, 'vocabulary.pkl')\n    \n    # Verification - test tokenization\n    print(f\"\\n{'='*70}\")\n    print(\"VERIFICATION - Sample Tokenization\")\n    print(f\"{'='*70}\")\n    \n    sample_caption = train_df.iloc[0]['captions'][0]\n    print(f\"\\nOriginal caption:\")\n    print(f\"  '{sample_caption}'\")\n    \n    tokens = tokenize_caption(sample_caption)\n    print(f\"\\nTokenized:\")\n    print(f\"  {tokens}\")\n    \n    indices = [vocab(token) for token in tokens]\n    print(f\"\\nWord indices:\")\n    print(f\"  {indices}\")\n    \n    reconstructed = [vocab.idx2word[idx] for idx in indices]\n    print(f\"\\nReconstructed:\")\n    print(f\"  {reconstructed}\")\n    \n    # Show special tokens\n    print(f\"\\n{'='*70}\")\n    print(\"SPECIAL TOKENS\")\n    print(f\"{'='*70}\")\n    print(f\"  {vocab.PAD_TOKEN}: {vocab(vocab.PAD_TOKEN)}\")\n    print(f\"  {vocab.START_TOKEN}: {vocab(vocab.START_TOKEN)}\")\n    print(f\"  {vocab.END_TOKEN}: {vocab(vocab.END_TOKEN)}\")\n    print(f\"  {vocab.UNK_TOKEN}: {vocab(vocab.UNK_TOKEN)}\")\n    \n    # Test unknown word\n    print(f\"\\nTest unknown word:\")\n    unknown_word = \"xyzabc123notinvocab\"\n    print(f\"  '{unknown_word}' -> index {vocab(unknown_word)} ({vocab.UNK_TOKEN})\")\n    \n    print(f\"\\n{'='*70}\")\n    print(\"VOCABULARY BUILDING COMPLETE! ✓\")\n    print(f\"{'='*70}\")\n    print(f\"\\nNext steps:\")\n    print(f\"  1. Use vocabulary.pkl in your training script\")\n    print(f\"  2. Convert captions to sequences using vocab(word)\")\n    print(f\"  3. Pad sequences to max length (recommend 15-20 based on paper)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ","metadata":{}},{"cell_type":"markdown","source":"### ","metadata":{}}]}