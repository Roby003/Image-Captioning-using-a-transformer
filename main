{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6019472,"sourceType":"datasetVersion","datasetId":3445072}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Data loading and feature extraction","metadata":{}},{"cell_type":"markdown","source":"Creating DataFrame for training","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nimport torchvision.models as models\n\n# Load the JSON file\nwith open('/kaggle/input/coco-image-caption/annotations_trainval2014/annotations/captions_train2014.json', 'r') as f:\n    coco_data = json.load(f)\n\n# Convert to DataFrames\nannotations_df = pd.DataFrame(coco_data['annotations'])\nimages_df = pd.DataFrame(coco_data['images'])\n\n\ndf = annotations_df.merge(images_df, left_on='image_id', right_on='id', \n                          suffixes=('_ann', '_img'))\n\n# Select relevant columns\ndf = df[['image_id', 'file_name', 'caption', 'height', 'width']]\n\n\n\ncaptions_grouped = annotations_df.groupby('image_id')['caption'].apply(list).reset_index()\ncaptions_grouped.columns = ['image_id', 'captions']\n\ngrouped_df = captions_grouped.merge(images_df, left_on='image_id', right_on='id')\ngrouped_df = final_df[['image_id', 'file_name', 'captions', 'height', 'width']]\nprint(grouped_df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-11T12:41:01.397603Z","iopub.execute_input":"2026-01-11T12:41:01.397954Z","iopub.status.idle":"2026-01-11T12:41:04.301607Z","shell.execute_reply.started":"2026-01-11T12:41:01.397927Z","shell.execute_reply":"2026-01-11T12:41:04.300790Z"}},"outputs":[{"name":"stdout","text":"   image_id                        file_name  \\\n0         9  COCO_train2014_000000000009.jpg   \n1        25  COCO_train2014_000000000025.jpg   \n2        30  COCO_train2014_000000000030.jpg   \n3        34  COCO_train2014_000000000034.jpg   \n4        36  COCO_train2014_000000000036.jpg   \n\n                                            captions  height  width  \n0  [Closeup of bins of food that include broccoli...     480    640  \n1  [A giraffe eating food from the top of the tre...     426    640  \n2  [A flower vase is sitting on a porch stand., W...     428    640  \n3  [A zebra grazing on lush green grass in a fiel...     425    640  \n4  [Woman in swim suit holding parasol on sunny d...     640    481  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Pretrained VGG-16 and ResNet loading","metadata":{}},{"cell_type":"code","source":"vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nvgg16.classifier = vgg16.classifier[:-1]  # Removes layer [6]\nresnet101 = models.resnet101(pretrained=True)\nresnet152 = models.resnet152(pretrained=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T12:41:04.303101Z","iopub.execute_input":"2026-01-11T12:41:04.303358Z","iopub.status.idle":"2026-01-11T12:41:07.419319Z","shell.execute_reply.started":"2026-01-11T12:41:04.303335Z","shell.execute_reply":"2026-01-11T12:41:07.418473Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Feature extraction and saving","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nvgg16 =vgg16.to(device)\nresnet101 = resnet101.to(device)\nresnet152 = resnet152.to(device)\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\ndef extract_features(image_path,model):\n    img = Image.open(image_path).convert('RGB')\n    img = transform(img).unsqueeze(0).cuda()\n    \n    with torch.no_grad():\n        features = model(img)\n    \n    return features.cpu().numpy()\n\n# Extract and save features for all COCO images\nfeatures_dict = {vgg16:{},resnet101:{},resnet152:{}}\n\nfor row in tqdm(grouped_df[['file_name','image_id']].itertuples(index=False)):\n    img_filename = row.file_name\n    img_id = row.image_id\n    img_path = f'/kaggle/input/coco-image-caption/train2014/train2014/{img_filename}'\n    for model in features_dict.keys():\n        features = extract_features(img_path, model)\n        features_dict[model][img_id] = features\n# Save extracted features\nnp.save('coco_vgg16_features.npy', features_dict[vgg16])\nnp.save('coco_resnet101_features.npy', features_dict[resnet101])\nnp.save('coco_resnet152_features.npy', features_dict[resnet152])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T12:41:07.420343Z","iopub.execute_input":"2026-01-11T12:41:07.420608Z"}},"outputs":[{"name":"stderr","text":"19390it [24:44, 13.54it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Build complete dataset","metadata":{}},{"cell_type":"markdown","source":"### ","metadata":{}},{"cell_type":"markdown","source":"### ","metadata":{}}]}